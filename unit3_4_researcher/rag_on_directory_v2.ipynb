{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a55f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Any, Union\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import tool\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from tqdm import tqdm\n",
    "from langchain_core.messages import HumanMessage\n",
    "from FlagEmbedding import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acbf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALIZED_LOADERS = {\n",
    "    \".pdf\": PyPDFLoader,\n",
    "}\n",
    "\n",
    "# Configuration for chunking and embeddings\n",
    "CHUNK_SIZE = 4000  # Characters per chunk\n",
    "CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "MAX_INPUT_SIZE = 4000  # Max tokens for embedding model (adjusted for long-input model)\n",
    "\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "\n",
    "FOLDERS_TO_EXCLUDE = [\".claude/\", \".conda\", \".gradio/\", \"__pycache__\", \".git\", \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d87e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama initialized: Hello! How can I assist you today?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/6mkw37fs20q07p9x11q7d7k80000gq/T/ipykernel_33823/2138498567.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n",
      "Fetching 4 files:   0%|          | 0/4 [04:34<?, ?it/s]\n",
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOllama initialized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_response.text[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Initialize HuggingFace embeddings model designed for longer inputs\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# embeddings_model = HuggingFaceEmbeddings(\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     model_name=\"sentence-transformers/all-mpnet-base-v2\",  # Better for longer texts\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#     normalize_embeddings=True\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ) # Setting use_fp16 to True speeds up computation with a slight performance degradation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m embeddings_model = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen3-Embedding-8B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalize_embeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Initialize text splitter for chunking\u001b[39;00m\n\u001b[32m     33\u001b[39m text_splitter = RecursiveCharacterTextSplitter(\n\u001b[32m     34\u001b[39m     chunk_size=CHUNK_SIZE,\n\u001b[32m     35\u001b[39m     chunk_overlap=CHUNK_OVERLAP,\n\u001b[32m     36\u001b[39m     length_function=\u001b[38;5;28mlen\u001b[39m,\n\u001b[32m     37\u001b[39m     separators=[\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/langchain_community/embeddings/huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:2254\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2249\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2271\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:541\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m     **kwargs,\n\u001b[32m    527\u001b[39m ) -> Self:\n\u001b[32m    528\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    529\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    530\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         backend=backend,\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     91\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:186\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_onnx_model(model_name_or_path, config, cache_dir, **model_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/modeling_utils.py:1152\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1150\u001b[39m sharded_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     checkpoint_files, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     checkpoint_files = [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/utils/hub.py:1115\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[32m   1113\u001b[39m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m cached_filenames = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/transformers/utils/hub.py:439\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    424\u001b[39m         hf_hub_download(\n\u001b[32m    425\u001b[39m             path_or_repo_id,\n\u001b[32m    426\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    436\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    437\u001b[39m         )\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py:297\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    295\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFetching \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m files\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM for decision making and direct responses\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test Ollama connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"Ollama initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Initialize HuggingFace embeddings model designed for longer inputs\n",
    "# embeddings_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\",  # Better for longer texts\n",
    "#     model_kwargs={'device': 'cpu'},\n",
    "#     encode_kwargs={'normalize_embeddings': True}\n",
    "# )\n",
    "# embeddings_model = BGEM3FlagModel(\n",
    "#     'BAAI/bge-m3',  \n",
    "#     use_fp16=True,  \n",
    "#     normalize_embeddings=True\n",
    "# ) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-8B\", \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Initialize text splitter for chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"Embeddings model and text splitter initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02462345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_from_directory(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scans a directory and returns a list of all file paths.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not any(folder_to_exclude in file_path for folder_to_exclude in FOLDERS_TO_EXCLUDE):\n",
    "                all_files.append(file_path)\n",
    "    return all_files\n",
    "\n",
    "def load_documents(file_paths: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads documents from a list of file paths using the appropriate loader.\n",
    "    Defaults to TextLoader for any unrecognized file type.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "        loader_class = SPECIALIZED_LOADERS.get(extension)\n",
    "\n",
    "        try:\n",
    "            if loader_class:\n",
    "                loader = loader_class(file_path)\n",
    "            else:\n",
    "                # Default to TextLoader for all other files \n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            # This will catch errors for true binary files that can't be decoded\n",
    "            print(f\"Skipping file {file_path}, could not be read as text. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunk_and_embed_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Chunks large documents and creates averaged embeddings for each document.\n",
    "    If a document is small enough, uses it directly. If too large, chunks it\n",
    "    and averages the embeddings.\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    print(f\"Processing {len(documents)} documents with chunking strategy...\")\n",
    "    \n",
    "    # Create progress bar for document processing\n",
    "    doc_progress = tqdm(documents, desc=\"Processing documents\", unit=\"doc\")\n",
    "    \n",
    "    for doc in doc_progress:\n",
    "        if not doc.page_content.strip():\n",
    "            doc_progress.set_postfix(status=\"Skipping empty\")\n",
    "            print(f\"Skipping empty document: {doc.metadata.get('source', 'N/A')}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            source_file = doc.metadata.get('source', 'N/A')\n",
    "            doc_progress.set_postfix(status=f\"Processing: {os.path.basename(source_file)}\")\n",
    "            \n",
    "            # Check if document needs chunking\n",
    "            if len(doc.page_content) <= CHUNK_SIZE:\n",
    "                # Document is small enough, use as-is\n",
    "                processed_docs.append(doc)\n",
    "                print(f\"Document {source_file} is small enough, using directly\")\n",
    "            else:\n",
    "                # Document is too large, chunk it and average embeddings\n",
    "                chunks = text_splitter.split_text(doc.page_content)\n",
    "                print(f\"Document {source_file} chunked into {len(chunks)} pieces\")\n",
    "                \n",
    "                # Get embeddings for each chunk with progress bar\n",
    "                chunk_embeddings = []\n",
    "                chunk_progress = tqdm(enumerate(chunks), \n",
    "                                    total=len(chunks), \n",
    "                                    desc=f\"Embedding chunks for {os.path.basename(source_file)}\", \n",
    "                                    unit=\"chunk\", \n",
    "                                    leave=False)\n",
    "                \n",
    "                for i, chunk in chunk_progress:\n",
    "                    if chunk.strip():  # Skip empty chunks\n",
    "                        try:\n",
    "                            chunk_progress.set_postfix(status=f\"Chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "                            # embedding = embeddings_model.embed_query(chunk) # if you are using hugging face\n",
    "                            embedding = embeddings_model.encode(chunk, max_length=CHUNK_SIZE)['dense_vecs']\n",
    "\n",
    "                            chunk_embeddings.append(np.array(embedding))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error embedding chunk {i} of {source_file}: {e}\")\n",
    "                \n",
    "                chunk_progress.close()\n",
    "                \n",
    "                if chunk_embeddings:\n",
    "                    # Average the embeddings\n",
    "                    avg_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "                    \n",
    "                    # Create a document with the original content but store the averaged embedding\n",
    "                    # We'll store the embedding in metadata for later use\n",
    "                    processed_doc = Document(\n",
    "                        page_content=doc.page_content,  # Keep original content\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            \"avg_embedding\": avg_embedding.tolist(),  # Store as list for JSON serialization\n",
    "                            \"num_chunks\": len(chunks)\n",
    "                        }\n",
    "                    )\n",
    "                    processed_docs.append(processed_doc)\n",
    "                    doc_progress.set_postfix(status=f\"✓ Completed: {len(chunks)} chunks averaged\")\n",
    "                else:\n",
    "                    print(f\"Failed to create embeddings for {source_file}\")\n",
    "                    doc_progress.set_postfix(status=\"✗ Failed\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {doc.metadata.get('source', 'N/A')}: {e}\")\n",
    "            doc_progress.set_postfix(status=\"✗ Error\")\n",
    "\n",
    "    doc_progress.close()\n",
    "    print(f\"Finished processing documents. {len(processed_docs)} documents ready for indexing.\")\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_logic(directory_path: str) -> Union[Qdrant, str]:\n",
    "    \"\"\"\n",
    "    Contains the core logic for the indexing process.\n",
    "    This function is called by the LangGraph tool.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory_path):\n",
    "        return f\"Error: The provided path '{directory_path}' is not a valid directory.\"\n",
    "\n",
    "    print(f\"Starting to process directory: {directory_path}\")\n",
    "\n",
    "    # 1. Get all files from the directory, regardless of extension\n",
    "    file_paths = get_all_files_from_directory(directory_path)\n",
    "    if not file_paths:\n",
    "        return \"No files found in the directory.\"\n",
    "    print(f\"Found {len(file_paths)} files to process.\")\n",
    "\n",
    "    # 2. Load the content of all readable documents\n",
    "    documents = load_documents(file_paths)\n",
    "    if not documents:\n",
    "        return \"Could not load any readable text content from the files found.\"\n",
    "    print(f\"Successfully loaded content from {len(documents)} readable files.\")\n",
    "\n",
    "    # 3. Process documents with chunking strategy (no LLM summarization)\n",
    "    processed_docs = chunk_and_embed_documents(documents)\n",
    "    if not processed_docs:\n",
    "        return \"Failed to process documents with chunking strategy. Aborting.\"\n",
    "\n",
    "    # 4. Create an in-memory Qdrant vector store\n",
    "    collection_name = \"directory_documents\"\n",
    "    print(f\"Creating in-memory Qdrant collection: '{collection_name}'\")\n",
    "    try:\n",
    "        qdrant = Qdrant.from_documents(\n",
    "            processed_docs,\n",
    "            embeddings_model,\n",
    "            location=\":memory:\",  # Specifies an in-memory database\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(\"Successfully created Qdrant vector store in memory.\")\n",
    "        return qdrant\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while creating the Qdrant vector store: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the vector store for reuse\n",
    "global_vector_store = None\n",
    "\n",
    "@tool\n",
    "def index_directory(directory_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes all readable files in a directory using chunking strategy for large files\n",
    "    and averaged embeddings. No LLM calls during indexing - only HuggingFace embeddings.\n",
    "    Args:\n",
    "        directory_path: The absolute path to the directory to be indexed.\n",
    "    Returns:\n",
    "        A success or error message.\n",
    "    \"\"\"\n",
    "    global global_vector_store\n",
    "    result = _index_logic(directory_path)\n",
    "    if isinstance(result, str):\n",
    "        return result  # Return error message\n",
    "    else:\n",
    "        # Store the vector store globally for the RAG tool to use\n",
    "        global_vector_store = result\n",
    "        return f\"Successfully processed directory '{directory_path}'. Documents are now indexed with chunked embeddings.\"\n",
    "\n",
    "@tool\n",
    "def search_documents(query: str, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search through the indexed documents using semantic similarity.\n",
    "    Args:\n",
    "        query: The search query\n",
    "        k: Number of results to return (default 5)\n",
    "    Returns:\n",
    "        Search results with file paths and relevant content\n",
    "    \"\"\"\n",
    "    global global_vector_store\n",
    "    \n",
    "    if global_vector_store is None:\n",
    "        return \"No documents have been indexed yet. Please run index_directory first.\"\n",
    "    \n",
    "    try:\n",
    "        search_results = global_vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        if not search_results:\n",
    "            return f\"No relevant documents found for query: '{query}'\"\n",
    "        \n",
    "        results_text = f\"Found {len(search_results)} relevant documents for query: '{query}'\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(search_results, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            content_preview = doc.page_content[:500] + \"...\"\n",
    "            num_chunks = doc.metadata.get('num_chunks', 'N/A')\n",
    "            \n",
    "            results_text += f\"Result {i}:\\n\"\n",
    "            results_text += f\"File: {source}\\n\"\n",
    "            if num_chunks != 'N/A':\n",
    "                results_text += f\"Chunks: {num_chunks}\\n\"\n",
    "            results_text += f\"Content: {content_preview}\\n\"\n",
    "            results_text += \"-\" * 80 + \"\\n\\n\"\n",
    "        \n",
    "        return results_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching documents: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720108ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our agent\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query: str\n",
    "    search_results: str\n",
    "    needs_local_knowledge: bool\n",
    "\n",
    "# Decision function using Ollama LLM\n",
    "def decide_knowledge_source(state: AgentState) -> str:\n",
    "    \"\"\"Use Ollama LLM to decide if the question requires local knowledge from vector store\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    decision_prompt = f\"\"\"You are an intelligent assistant that determines whether a user's question requires local knowledge from a vector database containing files and documents from a directory, or if it can be answered with general knowledge.\n",
    "\n",
    "User Question: \"{query}\"\n",
    "\n",
    "Consider the following:\n",
    "- Questions about specific files, code, implementations, or content within a directory require LOCAL KNOWLEDGE\n",
    "- Questions about general concepts, explanations, tutorials, or broad topics can be answered with GENERAL KNOWLEDGE\n",
    "- Questions asking \"what is in this directory\", \"show me files\", \"explain this code\" require LOCAL KNOWLEDGE\n",
    "- Questions like \"what is Python?\", \"how does machine learning work?\", \"explain REST APIs\" need GENERAL KNOWLEDGE\n",
    "\n",
    "Respond with EXACTLY one word: either \"LOCAL\" or \"GENERAL\"\n",
    "\n",
    "Decision:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.complete(decision_prompt)\n",
    "        decision = response.text.strip().upper()\n",
    "        \n",
    "        if \"LOCAL\" in decision:\n",
    "            return \"use_vector_store\"\n",
    "        else:\n",
    "            return \"direct_response\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in decision making: {e}\")\n",
    "        return \"use_vector_store\"\n",
    "\n",
    "def search_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Search for relevant documents in vector store\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_results = search_documents(query)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"query\": query,\n",
    "        \"search_results\": search_results,\n",
    "        \"needs_local_knowledge\": True\n",
    "    }\n",
    "\n",
    "def vector_response_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate response using vector store search results\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_results = state.get(\"search_results\", \"\")\n",
    "    \n",
    "    if search_results and \"No relevant documents found\" not in search_results:\n",
    "        # Use Ollama to generate a response based on the search results\n",
    "        response_prompt = f\"\"\"Based on the following search results from local files and documents, answer the user's question comprehensively.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Please provide a helpful answer based on the information found in the local documents. If the search results don't fully answer the question, say so and provide what information is available.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            ollama_response = llm.complete(response_prompt)\n",
    "            response_content = f\"Based on the local documents:\\n\\n{ollama_response.text}\"\n",
    "        except Exception as e:\n",
    "            response_content = f\"Found local documents but error generating response: {e}\\n\\nRaw search results:\\n{search_results}\"\n",
    "    else:\n",
    "        response_content = \"No relevant documents found in the local directory for your question.\"\n",
    "    \n",
    "    from langchain_core.messages import AIMessage\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response_content)],\n",
    "        \"query\": query,\n",
    "        \"search_results\": search_results,\n",
    "        \"needs_local_knowledge\": True\n",
    "    }\n",
    "\n",
    "def direct_response_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate direct response using Ollama without vector store\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    try:\n",
    "        # Direct response from Ollama for general knowledge questions\n",
    "        response = llm.complete(query)\n",
    "        response_content = response.text\n",
    "    except Exception as e:\n",
    "        response_content = f\"Error generating response: {e}\"\n",
    "    \n",
    "    from langchain_core.messages import AIMessage\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response_content)],\n",
    "        \"query\": query,\n",
    "        \"search_results\": \"\",\n",
    "        \"needs_local_knowledge\": False\n",
    "    }\n",
    "\n",
    "def route_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Initial routing step\"\"\"\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"query\": state[\"messages\"][-1].content,\n",
    "        \"search_results\": \"\",\n",
    "        \"needs_local_knowledge\": False\n",
    "    }\n",
    "\n",
    "# Create the LangGraph with improved decision making\n",
    "def create_rag_agent():\n",
    "    \"\"\"Create a LangGraph agent with LLM-based decision making\"\"\"\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"route\", route_query)\n",
    "    workflow.add_node(\"search\", search_step)\n",
    "    workflow.add_node(\"vector_response\", vector_response_step)\n",
    "    workflow.add_node(\"direct_response\", direct_response_step)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"route\")\n",
    "    \n",
    "    # Add conditional edges from route using LLM decision\n",
    "    workflow.add_conditional_edges(\n",
    "        \"route\",\n",
    "        decide_knowledge_source,  # LLM-based decision function\n",
    "        {\n",
    "            \"use_vector_store\": \"search\",\n",
    "            \"direct_response\": \"direct_response\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After search, go to vector response\n",
    "    workflow.add_edge(\"search\", \"vector_response\")\n",
    "    \n",
    "    # Both response types end the workflow\n",
    "    workflow.add_edge(\"vector_response\", END)\n",
    "    workflow.add_edge(\"direct_response\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the agent\n",
    "rag_agent = create_rag_agent()\n",
    "\n",
    "print(\"RAG Agent with LLM-based decision making created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eu3oz0ub",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RAG Agent Workflow Diagram ===\")\n",
    "\n",
    "try:\n",
    "    # Generate and display the workflow diagram\n",
    "    from IPython.display import Image, display\n",
    "    \n",
    "    # Get the diagram as PNG bytes\n",
    "    diagram_png = rag_agent.get_graph().draw_mermaid_png()\n",
    "    \n",
    "    # Display the diagram\n",
    "    display(Image(diagram_png))\n",
    "    print(\"Workflow diagram displayed above.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate diagram: {e}\")\n",
    "    \n",
    "    # Fallback: print text representation of the workflow\n",
    "    print(\"\\nWorkflow Structure (Text):\")\n",
    "    print(\"1. route (entry point)\")\n",
    "    print(\"2. decide_knowledge_source() -> LOCAL or GENERAL\")\n",
    "    print(\"3a. If LOCAL: search -> vector_response -> END\")\n",
    "    print(\"3b. If GENERAL: direct_response -> END\")\n",
    "    print(\"\\nNodes:\")\n",
    "    print(\"- route: Initial query processing\")\n",
    "    print(\"- search: Vector database search\")\n",
    "    print(\"- vector_response: LLM response with search results\")\n",
    "    print(\"- direct_response: Direct LLM response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861739a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, index a directory \n",
    "print(\"Indexing directory...\")\n",
    "result = index_directory(\"../.\")\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_payloads = []\n",
    "while True:\n",
    "    points, offset = global_vector_store.client.scroll(\n",
    "        collection_name=global_vector_store.collection_name,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    all_payloads.extend([p.payload for p in points])\n",
    "    if offset is None:\n",
    "        break\n",
    "    print([p.payload for p in points])\n",
    "\n",
    "all_payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311840b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Local knowledge question (should use vector store)\n",
    "print(\"\\n--- Test 1: Local Knowledge Question ---\")\n",
    "local_query = \"What file should I look at when working with SmolAgents?\"\n",
    "local_state = {\n",
    "    \"messages\": [HumanMessage(content=local_query)],\n",
    "    \"query\": \"\",\n",
    "    \"search_results\": \"\",\n",
    "    \"needs_local_knowledge\": False\n",
    "}\n",
    "\n",
    "result = rag_agent.invoke(local_state)\n",
    "print(f\"Query: {local_query}\")\n",
    "print(f\"Response: {result['messages'][-1]}...\")\n",
    "print(f\"Used local knowledge: {result['needs_local_knowledge']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: General knowledge question (should use direct Ollama)\n",
    "print(\"\\n--- Test 2: General Knowledge Question ---\")\n",
    "general_query = \"Explain history of Prussia briefly.\"\n",
    "general_state = {\n",
    "    \"messages\": [HumanMessage(content=general_query)],\n",
    "    \"query\": \"\",\n",
    "    \"search_results\": \"\",\n",
    "    \"needs_local_knowledge\": False\n",
    "}\n",
    "\n",
    "result = rag_agent.invoke(general_state)\n",
    "print(f\"Query: {general_query}\")\n",
    "print(f\"Response: {result['messages'][-1]}...\")\n",
    "print(f\"Used local knowledge: {result['needs_local_knowledge']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test 3: Ambiguous question to test decision making\n",
    "print(\"\\n--- Test 3: Ambiguous Question ---\")\n",
    "ambiguous_query = \"How do I implement a function?\"\n",
    "ambiguous_state = {\n",
    "    \"messages\": [HumanMessage(content=ambiguous_query)],\n",
    "    \"query\": \"\",\n",
    "    \"search_results\": \"\",\n",
    "    \"needs_local_knowledge\": False\n",
    "}\n",
    "\n",
    "result = rag_agent.invoke(ambiguous_state)\n",
    "print(f\"Query: {ambiguous_query}\")\n",
    "print(f\"Response: {result['messages'][-1]}...\")\n",
    "print(f\"Used local knowledge: {result['needs_local_knowledge']}\")\n",
    "\n",
    "print(\"\\n=== Enhanced RAG System Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271fe2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
