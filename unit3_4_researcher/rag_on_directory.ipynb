{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a55f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Any, Union\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.tools import tool\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.chat_engine import SimpleChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALIZED_LOADERS = {\n",
    "    \".pdf\": PyPDFLoader,\n",
    "}\n",
    "\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d87e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: Hello! How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"Model initialized: {test_response.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02462345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_files_from_directory(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scans a directory and returns a list of all file paths.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "def load_documents(file_paths: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads documents from a list of file paths using the appropriate loader.\n",
    "    Defaults to TextLoader for any unrecognized file type.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "        loader_class = SPECIALIZED_LOADERS.get(extension)\n",
    "\n",
    "        try:\n",
    "            if loader_class:\n",
    "                loader = loader_class(file_path)\n",
    "            else:\n",
    "                # Default to TextLoader for all other files (code, .sh, .o, etc.)\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            # This will catch errors for true binary files that can't be decoded\n",
    "            print(f\"Skipping file {file_path}, could not be read as text. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "def generate_summaries(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Generates extensive summaries for a list of documents using an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    summary_docs = []\n",
    "    print(f\"Generating summaries for {len(documents)} documents...\")\n",
    "    for doc in documents:\n",
    "        if not doc.page_content.strip():\n",
    "            print(f\"Skipping empty document: {doc.metadata.get('source', 'N/A')}\")\n",
    "            continue\n",
    "        try:\n",
    "            # Create a summarization chain\n",
    "            prompt_template =f\"\"\"Write an extensive and detailed summary of the following text.\n",
    "            Capture the key topics, arguments, important entities, and the main purpose of the document.\n",
    "            The summary should be comprehensive enough to replace the original text for the purpose of a semantic search.\n",
    "\n",
    "            TEXT:\n",
    "            \"{doc.page_content}\"\n",
    "\n",
    "            EXTENSIVE SUMMARY:\"\"\"\n",
    "            resp = llm.complete(prompt_template)\n",
    "            # Create a new document with the summary and original metadata\n",
    "            summary_doc = Document(\n",
    "                page_content=resp.text,\n",
    "                metadata={\"source\": doc.metadata.get(\"source\", \"N/A\")}\n",
    "            )\n",
    "            summary_docs.append(summary_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing document {doc.metadata.get('source', 'N/A')}: {e}\")\n",
    "\n",
    "    print(\"Finished generating summaries.\")\n",
    "    return summary_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_logic(directory_path: str) -> Union[Qdrant, str]:\n",
    "    \"\"\"\n",
    "    Contains the core logic for the indexing process.\n",
    "    This function is called by the LangGraph tool.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory_path):\n",
    "        return f\"Error: The provided path '{directory_path}' is not a valid directory.\"\n",
    "\n",
    "    print(f\"Starting to process directory: {directory_path}\")\n",
    "\n",
    "    # 1. Get all files from the directory, regardless of extension\n",
    "    file_paths = get_all_files_from_directory(directory_path)\n",
    "    if not file_paths:\n",
    "        return \"No files found in the directory.\"\n",
    "    print(f\"Found {len(file_paths)} files to process.\")\n",
    "\n",
    "    # 2. Load the content of all readable documents\n",
    "    documents = load_documents(file_paths)\n",
    "    if not documents:\n",
    "        return \"Could not load any readable text content from the files found.\"\n",
    "    print(f\"Successfully loaded content from {len(documents)} readable files.\")\n",
    "\n",
    "    # 3. Generate summaries for each document\n",
    "    summary_docs = generate_summaries(documents)\n",
    "    if not summary_docs:\n",
    "        return \"Failed to generate summaries for the documents. Aborting.\"\n",
    "\n",
    "    # 4. Initialize free embeddings from Hugging Face\n",
    "    print(\"Initializing Hugging Face embeddings...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 5. Create an in-memory Qdrant vector store\n",
    "    collection_name = \"directory_summaries\"\n",
    "    print(f\"Creating in-memory Qdrant collection: '{collection_name}'\")\n",
    "    try:\n",
    "        qdrant = Qdrant.from_documents(\n",
    "            summary_docs,\n",
    "            embeddings,\n",
    "            location=\":memory:\",  # Specifies an in-memory database\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(\"Successfully created Qdrant vector store in memory.\")\n",
    "        return qdrant\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while creating the Qdrant vector store: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def index_directory_with_summaries(directory_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes all readable files in a directory (code, text, pdf, etc.), creates\n",
    "    extensive summaries of their content, and indexes these summaries in an\n",
    "    in-memory Qdrant vector store using Hugging Face embeddings. The original\n",
    "    file path is stored as metadata.\n",
    "    Args:\n",
    "        directory_path: The absolute path to the directory to be indexed.\n",
    "    Returns:\n",
    "        A success or error message.\n",
    "    \"\"\"\n",
    "    result = _index_logic(directory_path)\n",
    "    if isinstance(result, str):\n",
    "        return result  # Return error message\n",
    "    else:\n",
    "        # The actual vector store object is held in memory.\n",
    "        # For a tool, we return a success message.\n",
    "        return f\"Successfully processed directory '{directory_path}'. Summaries are now indexed in an in-memory Qdrant collection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720108ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy directory and files for testing\n",
    "test_dir = \"test_documents_all_types\"\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "with open(os.path.join(test_dir, \"ml_paper.txt\"), \"w\") as f:\n",
    "    f.write(\"Artificial Intelligence (AI) and Machine Learning (ML) are fields of computer science.\")\n",
    "\n",
    "with open(os.path.join(test_dir, \"script.py\"), \"w\") as f:\n",
    "    f.write(\"import os\\n\\ndef main():\\n    print('Hello, World!')\")\n",
    "\n",
    "with open(os.path.join(test_dir, \"run.sh\"), \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\necho 'Starting process...'\")\n",
    "\n",
    "# Create a dummy binary-like file that should be skipped\n",
    "with open(os.path.join(test_dir, \"some_data.o\"), \"wb\") as f:\n",
    "    f.write(b'\\xDE\\xAD\\xBE\\xEF')\n",
    "\n",
    "\n",
    "# --- Run the core logic directly to get the vector store object for testing ---\n",
    "print(\"--- Running Indexing Logic ---\")\n",
    "vector_store = _index_logic(test_dir)\n",
    "\n",
    "# --- Test the in-memory index ---\n",
    "if isinstance(vector_store, Qdrant):\n",
    "    print(\"\\n--- Testing the In-Memory Index ---\")\n",
    "    query = \"What is the python script about?\"\n",
    "    search_results = vector_store.similarity_search(query)\n",
    "\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"Search Results (from summaries):\")\n",
    "    for doc in search_results:\n",
    "        print(f\"\\nSummary: {doc.page_content}\")\n",
    "        print(f\"Original File: {doc.metadata['source']}\")\n",
    "else:\n",
    "    # Print the error message if something went wrong\n",
    "    print(f\"\\n--- Tool Execution Failed ---\")\n",
    "    print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311840b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
