{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a55f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Any, Union\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import tool\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acbf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALIZED_LOADERS = {\n",
    "    \".pdf\": PyPDFLoader,\n",
    "}\n",
    "\n",
    "# Configuration for chunking and embeddings\n",
    "CHUNK_SIZE = 1000  # Characters per chunk\n",
    "CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "MAX_INPUT_SIZE = 512  # Max tokens for embedding model (adjusted for long-input model)\n",
    "\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "\n",
    "FOLDERS_TO_EXCLUDE = [\".claude/\", \".conda\", \".gradio/\", \"__pycache__\", \".git\", \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9982dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: Hello! How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"Model initialized: {test_response.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d87e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace embeddings model designed for longer inputs\n",
    "# Using a model optimized for longer sequences\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",  # Better for longer texts\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Initialize text splitter for chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"Embeddings model and text splitter initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02462345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_from_directory(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scans a directory and returns a list of all file paths.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not any(folder_to_exclude in file_path for folder_to_exclude in FOLDERS_TO_EXCLUDE):\n",
    "                all_files.append(file_path)\n",
    "    return all_files\n",
    "\n",
    "def load_documents(file_paths: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads documents from a list of file paths using the appropriate loader.\n",
    "    Defaults to TextLoader for any unrecognized file type.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "        loader_class = SPECIALIZED_LOADERS.get(extension)\n",
    "\n",
    "        try:\n",
    "            if loader_class:\n",
    "                loader = loader_class(file_path)\n",
    "            else:\n",
    "                # Default to TextLoader for all other files \n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            # This will catch errors for true binary files that can't be decoded\n",
    "            print(f\"Skipping file {file_path}, could not be read as text. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunk_and_embed_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Chunks large documents and creates averaged embeddings for each document.\n",
    "    If a document is small enough, uses it directly. If too large, chunks it\n",
    "    and averages the embeddings.\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    print(f\"Processing {len(documents)} documents with chunking strategy...\")\n",
    "    \n",
    "    for doc in documents:\n",
    "        if not doc.page_content.strip():\n",
    "            print(f\"Skipping empty document: {doc.metadata.get('source', 'N/A')}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Check if document needs chunking\n",
    "            if len(doc.page_content) <= CHUNK_SIZE:\n",
    "                # Document is small enough, use as-is\n",
    "                processed_docs.append(doc)\n",
    "                print(f\"Document {doc.metadata.get('source', 'N/A')} is small enough, using directly\")\n",
    "            else:\n",
    "                # Document is too large, chunk it and average embeddings\n",
    "                chunks = text_splitter.split_text(doc.page_content)\n",
    "                print(f\"Document {doc.metadata.get('source', 'N/A')} chunked into {len(chunks)} pieces\")\n",
    "                \n",
    "                # Get embeddings for each chunk\n",
    "                chunk_embeddings = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if chunk.strip():  # Skip empty chunks\n",
    "                        try:\n",
    "                            # Get embedding for this chunk\n",
    "                            embedding = embeddings_model.embed_query(chunk)\n",
    "                            chunk_embeddings.append(np.array(embedding))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error embedding chunk {i} of {doc.metadata.get('source', 'N/A')}: {e}\")\n",
    "                \n",
    "                if chunk_embeddings:\n",
    "                    # Average the embeddings\n",
    "                    avg_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "                    \n",
    "                    # Create a document with the original content but store the averaged embedding\n",
    "                    # We'll store the embedding in metadata for later use\n",
    "                    processed_doc = Document(\n",
    "                        page_content=doc.page_content,  # Keep original content\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            \"avg_embedding\": avg_embedding.tolist(),  # Store as list for JSON serialization\n",
    "                            \"num_chunks\": len(chunks)\n",
    "                        }\n",
    "                    )\n",
    "                    processed_docs.append(processed_doc)\n",
    "                else:\n",
    "                    print(f\"Failed to create embeddings for {doc.metadata.get('source', 'N/A')}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {doc.metadata.get('source', 'N/A')}: {e}\")\n",
    "\n",
    "    print(f\"Finished processing documents. {len(processed_docs)} documents ready for indexing.\")\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_logic(directory_path: str) -> Union[Qdrant, str]:\n",
    "    \"\"\"\n",
    "    Contains the core logic for the indexing process.\n",
    "    This function is called by the LangGraph tool.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory_path):\n",
    "        return f\"Error: The provided path '{directory_path}' is not a valid directory.\"\n",
    "\n",
    "    print(f\"Starting to process directory: {directory_path}\")\n",
    "\n",
    "    # 1. Get all files from the directory, regardless of extension\n",
    "    file_paths = get_all_files_from_directory(directory_path)\n",
    "    if not file_paths:\n",
    "        return \"No files found in the directory.\"\n",
    "    print(f\"Found {len(file_paths)} files to process.\")\n",
    "\n",
    "    # 2. Load the content of all readable documents\n",
    "    documents = load_documents(file_paths)\n",
    "    if not documents:\n",
    "        return \"Could not load any readable text content from the files found.\"\n",
    "    print(f\"Successfully loaded content from {len(documents)} readable files.\")\n",
    "\n",
    "    # 3. Process documents with chunking strategy (no LLM summarization)\n",
    "    processed_docs = chunk_and_embed_documents(documents)\n",
    "    if not processed_docs:\n",
    "        return \"Failed to process documents with chunking strategy. Aborting.\"\n",
    "\n",
    "    # 4. Create an in-memory Qdrant vector store\n",
    "    collection_name = \"directory_documents\"\n",
    "    print(f\"Creating in-memory Qdrant collection: '{collection_name}'\")\n",
    "    try:\n",
    "        qdrant = Qdrant.from_documents(\n",
    "            processed_docs,\n",
    "            embeddings_model,\n",
    "            location=\":memory:\",  # Specifies an in-memory database\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "        print(\"Successfully created Qdrant vector store in memory.\")\n",
    "        return qdrant\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while creating the Qdrant vector store: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the vector store for reuse\n",
    "global_vector_store = None\n",
    "\n",
    "@tool\n",
    "def index_directory(directory_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes all readable files in a directory using chunking strategy for large files\n",
    "    and averaged embeddings. No LLM calls during indexing - only HuggingFace embeddings.\n",
    "    Args:\n",
    "        directory_path: The absolute path to the directory to be indexed.\n",
    "    Returns:\n",
    "        A success or error message.\n",
    "    \"\"\"\n",
    "    global global_vector_store\n",
    "    result = _index_logic(directory_path)\n",
    "    if isinstance(result, str):\n",
    "        return result  # Return error message\n",
    "    else:\n",
    "        # Store the vector store globally for the RAG tool to use\n",
    "        global_vector_store = result\n",
    "        return f\"Successfully processed directory '{directory_path}'. Documents are now indexed with chunked embeddings.\"\n",
    "\n",
    "@tool\n",
    "def search_documents(query: str, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search through the indexed documents using semantic similarity.\n",
    "    Args:\n",
    "        query: The search query\n",
    "        k: Number of results to return (default 5)\n",
    "    Returns:\n",
    "        Search results with file paths and relevant content\n",
    "    \"\"\"\n",
    "    global global_vector_store\n",
    "    \n",
    "    if global_vector_store is None:\n",
    "        return \"No documents have been indexed yet. Please run index_directory first.\"\n",
    "    \n",
    "    try:\n",
    "        search_results = global_vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        if not search_results:\n",
    "            return f\"No relevant documents found for query: '{query}'\"\n",
    "        \n",
    "        results_text = f\"Found {len(search_results)} relevant documents for query: '{query}'\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(search_results, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            content_preview = doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content\n",
    "            num_chunks = doc.metadata.get('num_chunks', 'N/A')\n",
    "            \n",
    "            results_text += f\"Result {i}:\\n\"\n",
    "            results_text += f\"File: {source}\\n\"\n",
    "            if num_chunks != 'N/A':\n",
    "                results_text += f\"Chunks: {num_chunks}\\n\"\n",
    "            results_text += f\"Content: {content_preview}\\n\"\n",
    "            results_text += \"-\" * 80 + \"\\n\\n\"\n",
    "        \n",
    "        return results_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching documents: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720108ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our agent\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query: str\n",
    "    search_results: str\n",
    "\n",
    "# Agent functions\n",
    "def should_search(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we need to search for documents\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # Simple logic: if the message contains questions about files or code, search\n",
    "    search_keywords = [\"what\", \"how\", \"where\", \"find\", \"show\", \"explain\", \"code\", \"file\", \"script\", \"function\"]\n",
    "    if any(keyword in last_message.content.lower() for keyword in search_keywords):\n",
    "        return \"search\"\n",
    "    return \"respond\"\n",
    "\n",
    "def search_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Search for relevant documents\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    search_results = search_documents(query)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"query\": query,\n",
    "        \"search_results\": search_results\n",
    "    }\n",
    "\n",
    "def respond_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate response with or without search results\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    if state.get(\"search_results\"):\n",
    "        # We have search results, use them in the response\n",
    "        response_content = f\"Based on the documents in the indexed directory:\\n\\n{state['search_results']}\"\n",
    "    else:\n",
    "        # No search results, general response\n",
    "        response_content = \"I can help you search through indexed documents. Please index a directory first using the index_directory function, then ask questions about the contents.\"\n",
    "    \n",
    "    from langchain_core.messages import AIMessage\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response_content)],\n",
    "        \"query\": query,\n",
    "        \"search_results\": state.get(\"search_results\", \"\")\n",
    "    }\n",
    "\n",
    "# Create the LangGraph\n",
    "def create_rag_agent():\n",
    "    \"\"\"Create a LangGraph agent with RAG capabilities\"\"\"\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"search_step\", search_step)\n",
    "    workflow.add_node(\"respond_step\", respond_step)\n",
    "\n",
    "    workflow.set_entry_point(\"search_step\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"search_step\",\n",
    "        lambda x: \"respond_step\",  \n",
    "        {\"respond\": \"respond_step\"}\n",
    "    )\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.add_edge(\"respond_step\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the agent\n",
    "rag_agent = create_rag_agent()\n",
    "\n",
    "\n",
    "print(\"RAG Agent with LangGraph created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311840b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new RAG system\n",
    "print(\"=== Testing the New RAG System ===\")\n",
    "\n",
    "# First, index a directory \n",
    "print(\"\\n1. Indexing directory...\")\n",
    "result = index_directory(\"../.\")\n",
    "print(result)\n",
    "\n",
    "# Test search functionality\n",
    "print(\"\\n2. Testing search...\")\n",
    "search_result = search_documents(\"python script functionality\", k=3)\n",
    "print(search_result)\n",
    "\n",
    "# Test the LangGraph agent\n",
    "print(\"\\n3. Testing LangGraph Agent...\")\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do i install this directory as python project?\"\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=test_query)],\n",
    "    \"query\": \"\",\n",
    "    \"search_results\": \"\"\n",
    "}\n",
    "\n",
    "# Run the agent\n",
    "result = rag_agent.invoke(initial_state)\n",
    "print(f\"Agent Response: {result['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n=== RAG System Test Complete ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
