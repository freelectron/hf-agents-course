{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17dffb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfa015f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='44b3cb82-03e2-4a40-93a2-f98ff9ea3086', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<center>\\n    <h1> Bias-Variance Decomposition </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? ü§î\\n$~$\\n\\nThe point of this exercise is to try to create an understanding of what\\noverfitting and underfitting mean in the context of machine learning.\\n\\n$~$\\n\\n$~$\\n## The Model Error ‚öôÔ∏è\\n$~$\\n\\nLet\\'s start by saying that it does really not matter what kind of machine learning model \\nwe want to evaluate a simple regression model, Variation Autoencoded,  transfformer model, Large Language model,\\nclimate model of the Earth, or a model of the Universe. \\nThe principle stays the same - you want to predict an output given some input.\\ninput. Imagine a scenario of the regression task just because the math for it is the easiest to do.\\n**Given** a dataset represented by i.i.d. pairs $x, y$,\\ni.e., observations of independent random variables that follow some (joint) statistical distribution, \\nwe can write the dataset as\\n\\n$$ \\nD=\\\\left\\\\{\\\\left(x_1 ; y_1\\\\right), \\\\ldots, \\\\left(x_n ; y_n\\\\right)\\\\right\\\\}   .\\n$$\\n\\n**We then want to** predict the expected target $\\\\bar{y}(x)$ given an\\nobservation (input) $x$\\n\\n$$\\n\\\\bar{y}(x) = \\\\mathit{E}_{y|x}(y) = \\\\int_{y} y p(y | x) d y  .\\n$$\\n\\n**What we do** is use an algorithm $\\\\mathrm{A}$ to arrive to a function\\n$h$ based on the data sample $D$\\n\\n$$\\n{h}_{D} = \\\\mathrm{A}(D)\\n$$\\n\\nIn our example, $\\\\mathcal{A}$ could be any modelling technique, e.g.,\\nANN, SVM, OLS regression, with its hyperparameter values. Then,\\n$h_D$ is a resulting (trained) model, i.e., model hypothesis.\\nIn order to judge the quality of our trained model we want to compare it\\nto the \"true\\\\\" observed values from the population described by the\\nfunction $\\\\rho$. Thus, **our evaluation function** becomes the\\nexpected/mean squared error\\n\\n$$\\n\\\\mathit{E}_{x, y \\\\sim \\\\rho}\\\\left[\\\\left(h_D(x)-y\\\\right)^2\\\\right]=\\\\iint_{x y} p_{x,y} \\\\cdot \\\\left(h_D(x)-y\\\\right)^2 d x d y\\n$$\\n\\nwhere $\\\\left\\\\{p_{x, y}=Pr(x, y)\\\\right\\\\}$. The caveat is that we usually do not have access to the generative function $\\\\rho$.\\nTherefore, we have to evaluate our models based on the already sampled\\ndataset $D$, i.e., subdivide it into the training and test sets.  \\n\\n$~$\\n\\nIf we could sample infinitely many datasets $D$, we would have a model $\\\\bar{h}$ such that\\n$$\\n\\\\bar{h}=\\\\mathit{E}_{D \\\\sim \\\\rho^{(n)}}[\\\\mathit{A}(D)]=\\\\int h_D \\\\cdot \\\\operatorname{Pr}(D) d D  . \\n$$\\n\\nIn other words, if we had infinite time to sample and observe infinitely\\nmany data samples $D$ from the population, and at the same time run/train the algorithm $\\\\mathit{A}$ with them,\\nwe would have arrived to the best possible model $\\\\bar{h}$. We cannot do better than $\\\\bar{h}$ \\nwhich is our \"expected function\\\\\" given $\\\\mathit{A}$.\\n\\n$~$\\n\\nThe variable $D$ becomes just another random variable in our analysis\\nif we can condition on it. We can thus apply the expectation operator\\nover $D$ and do all sort of crazy things. Speaking of crazy things,\\nlet\\'s rewrite our expected error as if we trained and evaluated our\\nmodel on infinitely many dataset $D$ of size $n$ sampled from the\\nunderlying $\\\\rho$\\n\\n$$\\n\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[ \\\\left(h_D(x)-y\\\\right)^2\\\\right]\\n=\\\\int_D^{} \\\\int_x^{} \\\\int_y^{} \\\\left(h_D(x)-y\\\\right)^2 \\\\cdot p(x, y) p(D) d x d y d D \\\\ .\\n$$\\n\\nWon\\'t you agree that if we were to integrate over all possible\\ncombinations of $x, y, D$, we would know how good our model selection\\nalgorithm $\\\\mathit{A}$ really is? Let\\'s see if we can decompose this\\nerror into several components. We can do a few cheeky-mathy tricks, for example, \\nrepresent the error using some \\\\\"stable\\\\\" or, better said, training-set-independent\\nfunctions $\\\\bar{h}$ and later $\\\\bar{y}$. Begin by\\n\\n$$\\n\\\\begin{align}\\n\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(h_D(x)-y\\\\right)^2\\\\right]\\n&= \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(\\\\left(h_D(x)-\\\\bar{h}(x)\\\\right)+\\\\left(\\\\bar{h}(x) -y\\\\right)\\\\right)^2\\\\right]  \\\\\\\\\\n&= \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}} \\\\left[ \\\\left(h_D(x)-\\\\bar{h}(x)\\\\right)^2 \\\\right] + \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}} \\\\left[ \\\\left(\\\\bar{h}(x) - y\\\\right)^2 \\\\right] + \\\\\\\\\\n& 2 \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(\\\\left(h_D(x)-\\\\bar{h}(x)\\\\right) \\\\cdot \\\\left(\\\\bar{h}(x) -y\\\\right)\\\\right)\\\\right]\\n\\\\end{align}\\n$$\\n\\nwhere,\\n\\n$$\\n\\\\begin{align}\\n \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(\\\\left(h_D(x)-\\\\bar{h}(x)\\\\right) \\\\cdot \\\\left(\\\\bar{h}(x) -y\\\\right)\\\\right)\\\\right] &= \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho}} \\n    \\\\left[ \\\\mathit{E}_{\\\\substack{D \\\\sim \\\\rho^{(n)}}}  \\n        \\\\left(\\\\left(h_D(x)-\\\\bar{h}(x)\\\\right) \\\\cdot \\\\left(\\\\bar{h}(x) -y\\\\right)\\\\right)\\n    \\\\right] \\\\\\\\\\n  &=  \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho}} \\\\left[ c \\\\cdot \\\\mathit{E}_{\\\\substack{D \\\\sim \\\\rho^{(n)}}}  \\n    [h_D(x)] - \\\\bar{h}(x)\\n  \\\\right]\\\\end{align}\\n$$\\n\\nbecause $\\\\left(\\\\bar{h}(x) - y\\\\right) = c$ as both $\\\\bar{h}(x)$ and $y$ are\\nindependent of the training dataset $D$. Reminder, $x, y$ belong to the\\n\\\\\"evaluation\\\\\"/test here.\\n\\nLook further and see that\\n\\n$$\\n\\\\begin{align}\\n \\\\mathit{E}_{\\\\substack{D \\\\sim \\\\rho^{(n)}}}  \\n    [h_D(x)] = \\\\bar{h}(x)  \\\\text{ \\\\{ per definition \\\\} }. \\n\\\\end{align}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\begin{align}\\n  \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho}} \\\\left[ \\\\bar{h}(x) - \\\\bar{h}(x) \\n  \\\\right] = 0\\n\\\\end{align}\\n$$\\n\\nand the semi-final expected error becomes,\\n\\n$$\\n\\\\begin{align}\\n  \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(h_D(x)-y\\\\right)^2\\\\right]  \\n  &= \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}} \\\\left[ \\\\left(h_D(x)-\\\\bar{h}(x)\\\\right)^2 \\\\right] + \\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}} \\\\left[ \\\\left(\\\\bar{h}(x) - y\\\\right)^2 \\\\right] + 0 \\n\\\\end{align}\\n$$\\n\\nLet\\'s look at the most right term. Note, it is independent of $D$ again. What we can do is \\n\\n$$\\n\\\\begin{align}\\n\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho}}\\\\left[(\\\\bar{h}(x)-y)^2\\\\right] &= \\\\mathit{E}_{x, y}\\\\left[(\\\\bar{h}(x)-\\\\bar{y}(x)+\\\\bar{y}(x)-y)^2\\\\right] \\\\\\\\\\n&= \\\\mathit{E}_{x, y}\\\\left[(\\\\bar{h}(x)-\\\\bar{y}(x))^2\\\\right] + \\\\mathit{E}_{x, y}\\\\left[(\\\\bar{y}(x) - y)^2\\\\right]  + \\\\\\\\ & + 2 \\\\mathit{E}_{x, y}[(\\\\bar{h}(x)-\\\\bar{y}(x)) \\\\cdot (\\\\bar{y}(x)-y)] \\n\\\\end{align}\\n$$\\n\\nBut also\\n\\n$$\\n\\\\begin{align}\\n2 \\\\mathit{E}_{x, y}[(\\\\bar{h}(x)-\\\\bar{y}(x)) \\\\cdot (\\\\bar{y}(x)-y(x))] &= 2 \\\\mathit{E}_{x, y}[(\\\\bar{y}(x)-y) \\\\cdot c] \\n\\\\\\\\ & \\\\text{ \\\\{ since $(\\\\bar{h}(x)-\\\\bar{y}(x))$ is independent of $y$ \\\\} } \\\\\\\\\\n &= 2 \\\\mathit{E}_x\\\\left[\\\\mathit{E}_{y | x}[(\\\\bar{y}(x)-y) \\\\cdot c]\\\\right] \\\\\\\\\\n &= 2  \\\\mathit{E}_x\\\\left[c \\\\cdot\\\\left(\\\\mathit{E}_{y | x}[\\\\bar{y}(x)]-\\\\mathit{E}_{y | x}[y]\\\\right)\\\\right] \\\\\\\\\\n & \\\\text{ \\\\{ } \\\\mathit{E}_{y | x}[\\\\bar{y}(x)] \\\\text{is independent of $y | x$ and } \\\\\\\\ & \\\\mathit{E}_{y | x}[y] \\\\text{ is just } \\\\bar{y}(x) \\\\text{ \\\\} }\\n\\\\end{align}\\n$$\\n\\nThus,\\n\\n$$\\n\\\\begin{align}\\n\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho}}\\\\left[(\\\\bar{h}(x)-y)^2\\\\right]\\n&= \\\\mathit{E}_{x, y}\\\\left[(\\\\bar{h}(x)-\\\\bar{y}(x))^2\\\\right] + \\\\mathit{E}_{x, y}\\\\left[(\\\\bar{y}(x) - y)^2\\\\right] \\n\\\\end{align}\\n$$\\n\\nCombining everything together\\n\\n$$\\n\\\\begin{align}\\n\\\\mathit{E}_{\\\\substack{(x, y) \\\\sim \\\\rho \\\\\\\\ D  \\\\sim \\\\rho^{(n)}}}\\\\left[\\\\left(h_D(x)-y\\\\right)^2\\\\right] &= \\\\underbrace{{\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}} \\\\left[ \\\\left(h_D(x)-\\\\bar{h}(x)\\\\right)^2 \\\\right]}}_\\\\text{variance} + \\\\underbrace{\\\\mathit{E}_{x, y}\\\\left[(\\\\bar{h}(x)-\\\\bar{y}(x))^2\\\\right]}_\\\\text{bias} + \\\\\\\\ & + \\\\underbrace{\\\\mathit{E}_{x, y}\\\\left[(\\\\bar{y}(x) - y)^2\\\\right]}_\\\\text{noise} , \\n\\\\end{align}\\n$$\\n\\nand oh my, oh [my](https://youtu.be/ikllVKppHRE), what do we have here.. We see that variance component is the only one dependent on the dataset, i.e., \\nwhat dataset we sampled and how large it is ($n$). Variance is\\nsaying \\\\\"given a training/testing dataset $D$ and a point $x$, how much\\non average will our predictions vary from the predictions of the best\\npossible model allowed by $\\\\mathit{A}$\\\\\". We want our algorithm $\\\\mathit{A}$\\nto be able to generalise well across all $D$\\'s that can be drawn, thus a high\\nvariance is [no\\nbueno](https://www.memecreator.org/static/images/memes/4801156.jpg). Next, the bias term includes the true target function $\\\\bar{y}$ and\\nessentially shows what our average error between the best estimator\\n$\\\\bar{h}$ and the true values $\\\\bar{y}$ is. Said differently: how much our\\nalgo $\\\\mathit{A}$ is biased towards some other explanation that is not\\nin the data. Well, as for the noise term. Bad news, we will never know what it\\nis...\\n\\n$~$\\n\\n$~$\\n## The Trade-off ‚öñÔ∏è\\n$~$\\n\\nWhy does bias increase when you reduce variance and vice versa? \\nThat is not clearly shown by the last formula we drew out. \\nAn intuitive explanation would be: the larger your hypothesis pool (more\\noptions of choosing $h_D$), the more likely you are to be close to the\\ntrue function (global minima of the error) but you are also more likely to\\narrive to a different variation of the model hypothesis given a single\\nsampled dataset (since you have more parameters to tune, more degrees of\\nfreedom). Figure 1 tries to illustrate this point. An\\ninflexible model can only have a \\\\\"simple\\\\\" form that is likely to miss\\nthe true underlying target function, but it will always stay within a\\nsmall area of its hypothesis set. While a flexible, highly non-linear\\nmodel is more likely to capture the true, possibly, very complex\\nfunction, but because the algo $\\\\mathit{A}$ will be \\\\\"choosing\\\\\" a model\\nfrom such a variety, it will be very dependent on the $D$ that it gets.\\n\\n<p align=\"center\" id=\"fig:trade_off_intuitive\">\\n    <br>\\n    <strong> Figure 1: Trade-off between Bias and Variance </strong>\\n    <br>\\n    <img width=\"500\" height=\"250\" src=\"/posts/bias_variance_decomposition/pics/trade_off2.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html\\n\\nFor a more practical example of bias-variance inverse relationship with\\na $sin$ function approximation, you can watch\\n[this](https://www.youtube.com/watch?v=zrEyxfl2-a8) video.\\nUnfortunately, I could not find or come up with a derivation or proof of this trade-off\\nrelationship. I would very much appreciate if someone can point me to\\none.\\n\\n$~$\\n\\n$~$\\n## The Regimes ‚ÜóÔ∏è ‚ÜòÔ∏è\\n$~$\\n\\nHopefully, the seed of intuition behind bias and variance, and their connection, is\\nplanted. As a result of this relationship, we have to two common cases, or \"regimes\" when we train a model.\\nFigure 2  attempts to show what regularly happens with the train and test error in practice.\\n\\n<p align=\"center\" id=\"fig:test_train_error\">\\n    <br>\\n    <strong> Figure 2: Two common Bias-Variance regimes </strong>\\n    <br>\\n  <img width=\"500\" height=\"250\" src=\"/posts/bias_variance_decomposition/pics/test_train_error.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nCase #1 is when we trained a model and end up with a low training error\\nwhile our test error is high. This means that $\\\\mathit{A}$ was able to\\nchoose an approximation that is flexible and fitted a sample well. But\\nthe evaluation on another (test set) sample showed that the model was\\nnot general enough.\\n\\nIn Case #2 our training error was higher than an acceptable threshold\\nand we did not capture the complexity of the training dataset that well.\\nNonetheless, our test error is now close to the acceptable level meaning\\nthat implicit assumptions that were made during training might result in\\nbetter generalisation.\\n\\n$~$\\n\\nWhen modeling it is important to strike a balance between bias and variance to achieve a model that is most likely to generalise well.\\nHere are some tips:\\n\\n<p align=\"center\" id=\"fig:test_train_error\">\\n    <br>\\n    <strong> Figure 3: Dealing with The Trade </strong>\\n    <br>\\n  <img width=\"500\" height=\"450\" src=\"/posts/bias_variance_decomposition/pics/regimes3.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nIt is often said that the Regime #2 is when we do not have enough\\npower in the model. It lacks \\\\\"range\\\\\" and cannot capture the\\nrelationships between $x$ and $y$ well so it ends up making\\nsimplifying assumptions. Our error is then high due to high bias.\\n\\nThe Regime #1 is when error due to the variance component prevails. Then, the data is\\nfitted well on the training dataset but when evaluated on a different\\nsample, a high error is returned. That is a trait of high variance since our\\nmodel was \\\\\"too niche\\\\\" to generalise well.\\n\\n$~$\\n\\n$~$\\n## References üìú\\n$~$\\n-   [Analogous derivation](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html8)\\n    \\n-   [Trade-off explained](https://www.youtube.com/watch?v=zrEyxfl2-a8)\\n\\n-   [Nice blog with illustrations](https://mlu-explain.github.io/bias-variance/)\\n\\n-   Nice math read: Elements of Statistical Learning (Chapter 7.3 in 2nd\\n    edition, Jan 2017)\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7e154ecd-2737-4b3a-849a-7a8f3687320d', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<center>\\n    <h1> Kernel Methods  </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? ü§î\\n$~$\\n\\nA previous blog post on Linear Models explained a\\nbeautiful connection between linear models and kernels. It has also set\\nthe stage for us to progress towards what more commonly referred to\\nas Kernel Methods. Probabilistic techniques that utilise kernel\\nfunctions to model complex behaviours in the data.\\n\\n$~$\\n## Gaussian Processes for Regression ÔÆ©Ÿ®ŸÄÔÆ©ÔÆ©Ÿ®ŸÄ‚ô°ÔÆ©Ÿ®ŸÄÔÆ©ÔÆ©Ÿ®ŸÄ\\n$~$\\n\\nIn Linear models, we saw a bunch of linear models that\\nassumed some functional form. Most of them, if not all, can be\\nrepresented by some (linear) combination of the targets. Now, consider\\nthat instead of finding optimal function parameters, we would try to\\nfind functions (function mappings). This way, we can directly sample a\\nfunction from a governing (multivariate) distribution. However, a question arirse: \\nhow could one parameterise a distribution over functions? Well, we do not parameterise\\nfunctions directly, but model the joint marginal probabilities of their\\nvalues. We then reason about the entire function space by evaluating the\\njoint probabilities at any set of input points.\\n\\n$~$\\n\\nLet $X = \\\\{\\\\vec{x}_1, \\\\vec{x}_2 ... \\\\vec{x}_N\\\\}$ be a finite set of\\nelements and consider a set $\\\\mathcal{H}$ of all possible functions\\n$f$ mapping from $\\\\mathcal{X}$ to $\\\\mathbb{R}$. If we\\nassume that the domain of any $f(\\\\cdot) \\\\in \\\\mathcal{H}$ has a limited\\n($N$) number of elements, we can say that $f$ is \"represented\" by its\\nvector $\\\\vec{f} = [f(\\\\vec{x}_1), f(\\\\vec{x}_2)... f(\\\\vec{x}_N)]$. Note\\nthat $f(\\\\vec{x}_i)$\\'s by themselves can be seen as random\\nvariables/functions if the underlying function $f(\\\\cdot)$ is also\\nrandom. Thus, there is a one-to-one correspondence between a function\\n$f(\\\\cdot) \\\\in \\\\mathcal{H}$ and its \"vector form\\\\\" representation.\\nConsequently, we can specify a governing probability distribution where\\n$\\\\vec{f} \\\\sim N(\\\\vec{\\\\mu}, \\\\Lambda)$. This implies a (joint) probability\\ndistribution over random variables/functions that could have produced\\nthe values we are trying to predict, i.e., $p(\\\\vec{f}|X) = N(\\\\vec{\\\\mu}, K))$. \\n\\n$~$\\n\\nIn most of the cases, we need to assume that our stream of data is\\ninfinite and we can always make new predictions based on the new\\nobservations $\\\\vec{x}^{*}$ , i.e., a continuing process. Gaussian\\nprocess (GP) is a stochastic process - collection of random variables -\\nsuch that any finite sub-collection of random variables from the process\\nhas a joint multivariate distribution. By assuming a Gaussian process,\\nwe are pretty much saying: there were these values $\\\\vec{f}$ from\\nunderlying $f$, now any new collection $\\\\vec{f}^{*}$ must come from the\\nsame $f$. \\n\\n$~$\\n\\nAssume a GP is defined as $\\\\vec{f} \\\\sim N(\\\\vec{0}, K(\\\\cdot, \\\\cdot))$.\\nThe function that produces $\\\\vec{f}$ or $\\\\vec{f}^{*}$ is zero-meaned and\\ngenerates values according to the kernel function $K(\\\\cdot, \\\\cdot)$.\\nJust like before the kernel identifies how we are going to take\\nneighbours of a new observation and mix/combine/weigh their target\\nvalues to arrive to the prediction for the new observation. Generally,\\nif two points are similar in the kernel space, the function values at\\nthese points will also be similar.\\n\\n$~$\\n\\nFinally, let\\'s look at how to make predictions with GP. Imagine, we have\\ncollected a training set $S=\\\\{\\\\vec{x_i}, t_i \\\\}_{i=1}^N$ of i.i.d.\\nexamples. If we zero-mean the sample\\'s observations - GP was assumed\\nwith the mean 0 - we can apply a GP to the data and say\\n$t_i=f(\\\\vec{x_i}) + \\\\epsilon_{i}$. This means the observed targets $t_i$\\nare generated by a noisy process where the underlying function values\\nfollow a Gaussian process. Now, let\\n$S^{*}=\\\\{\\\\vec{x_i}^{*}, t_{i}^{*} \\\\}_{i=1}^{N^{*}}$ be a test set of\\npoints that come from the same process, i.e.,\\n$t_{i}^{*}=f(\\\\vec{x_i}^{*}) + \\\\epsilon_{i}^{*}$. Since, we said said\\nthat any sub-collection of random variables drawn from a GP has a joint\\nmultivariate distribution, then we can define the multivariate Gaussian\\nas \\n\\n$$\\n\\\\begin{align}\\n\\\\begin{bmatrix}\\n\\\\vec{f} \\\\\\\\ \\n\\\\vec{f}^*\\n\\\\end{bmatrix} \\n\\\\bigg\\\\rvert\\\\, X, X^{*} \\n&\\\\sim \\\\mathcal{N}\\\\left(\\\\vec{0},\\n\\\\begin{bmatrix}\\nK(X, X) & K\\\\left(X, X^{*}\\\\right) \\\\\\\\ \\nK\\\\left(X^*, X\\\\right) & K\\\\left(X^*, X^*\\\\right)\\n\\\\end{bmatrix}\\\\right) \\n\\\\end{align}\\n$$\\n\\nAdditionally, there is white noise i.i.d. noise that is natural to all\\nthe real-world data \\n\\n$$\\n\\\\begin{align} \\n\\\\left[\\\\begin{array}{l}\\\\vec{\\\\epsilon} \\\\\\\\ \\\\vec{\\\\epsilon}^*\\\\end{array}\\\\right] &\\\\sim \\\\mathcal{N}\\\\left(\\\\vec{0},\\\\left[\\\\begin{array}{cc} \\n\\\\sigma^2 \\\\mathbb{1} & \\\\vec{0} \\\\\\\\ \\n\\\\vec{0} & \\\\sigma^2 \\\\mathbb{1}\\n\\\\end{array}\\\\right]\\\\right)\\n\\\\end{align}\\n$$\\n\\nThis gives us function values with some white noise around them. Since\\nthe white noise is i.i.d. we can still use GP frame, but instead of\\nfiguring out estimates for $\\\\vec{f}^{*}$, we are searching for\\n$\\\\vec{t}^{*}$. \\n\\n$$\\n\\\\begin{align} \\n\\\\left[\\\\begin{array}{l}\\\\vec{t}_i \\\\\\\\ \\\\vec{t}_i^*\\\\end{array}\\\\right]_{|X,X^{*}} = \\n\\\\begin{bmatrix}\\n\\\\vec{f} \\\\\\\\ \\n\\\\vec{f}^*\\n\\\\end{bmatrix}  +\\n\\\\left[\\\\begin{array}{l}\\\\vec{\\\\varepsilon} \\\\\\\\ \\\\vec{\\\\varepsilon}\\\\end{array}\\\\right] \\\\sim \\\\mathcal{N}\\\\left(\\\\vec{0},\\\\left[\\\\begin{array}{cc}K(X, X)+\\\\sigma^2\\\\mathbb{1} & K(X, X^{*}) \\\\\\\\ K\\\\left(X^*, X\\\\right) & \\\\left.K\\\\left(X, X^*\\\\right)+\\\\sigma^2\\\\mathbb{1}\\\\right]\\\\end{array}\\\\right]\\\\right. \\\\ . \\n\\\\end{align}\\n$$\\n\\nIf we were to take the rules of [conditional distributions of the\\nmultivariate\\nGaussian\\'s](https://statproofbook.github.io/P/mvn-cond.html), we can\\nfind mean and covariance for the conditional distribution of\\n$p(\\\\vec{t}^* \\\\mid \\\\vec{t}, X, X^*) = N\\\\left(\\\\vec{\\\\mu}^*, \\\\Sigma^*\\\\right)$\\n\\n$$\\n\\\\begin{align} \\n\\\\vec{\\\\mu}^{*} &= K\\\\left(X^*, X\\\\right)\\\\left(K(X, X)+\\\\sigma^2 \\\\mathbb{1}\\\\right)^{-1} \\\\vec{t} \\\\\\\\ \\n\\\\Sigma^* &= K\\\\left(X^* , X^*\\\\right) + \\\\sigma^2 \\\\mathbb{1} - K\\\\left(X^* , X\\\\right) - \\\\left(K(X, X) + \\\\sigma^2 \\\\mathbb{1} \\\\right)^{-1} K\\\\left(X, X^*\\\\right)\\n\\\\end{align}\\n$$\\n\\nThere are two things to keep in mind when using GP in the wild. First,\\nlook how compute and memory expensive it is to make a new prediction..\\nIf we have a lot of past data, computing the kernels and inverting those\\neach time a new observation comes can be extremely inefficient. That is\\nwhy the real-world implementations of GP\\'s usually do not look at the\\nwhole covariance matrix of observations, i.e., evaluating the kernel\\nfunction for all data points, instead they use a sample of the data for\\neach new set of observations. Those optimised methods are often called\\nsparse kernel methods or sparse kernel machines. Second, the choice of\\nkernels is another aspect to consider. Many kernel function comes with\\nits own behaviour and hyperparameters. As a result, a practitioner can\\nspend a considerable amount of time in choosing the right kernel and\\noptimising its hyperparameters.\\n\\n$~$\\n\\n$~$\\n## Gaussian Processes for Classification üî°\\n$~$\\n\\nThe solution for GP\\'s in the case of a regression task, though\\ncomputationally heavy, is analytical. If we put GP\\'s into the framework\\nof classification tasks, we end up with a less behaved solution.\\n\\n$~$\\n\\nNow, we are predicting a variable that can only take either of two value\\n$t_i \\\\in\\\\{0 ; 1\\\\}$. Use a linear separator function $a(\\\\cdot)$ and put\\nit into a sigmoid to map a value to the probability range of $[0, 1]$\\n\\n$$\\n\\\\begin{align} \\na(\\\\vec{x}) : \\\\vec{x} \\\\in \\\\mathbb{R}^M \\\\mapsto a \\\\in \\\\mathbb{R}^1 \\\\\\\\ \\n\\\\sigma(a) = \\\\frac{1}{1 + e^{-a}} \\\\\\\\ \\n\\\\sigma(a): a \\\\in \\\\mathbb{R}^1 \\\\mapsto [0,1] \\\\ .\\n\\\\end{align}\\n$$\\n\\nThen, define the probability for the target to follow a Bernoulli\\ndistribution \\n\\n$$\\n\\\\begin{align} \\np(t \\\\mid a)=\\\\sigma(a)^t (1-\\\\sigma(a))^{1-t} . \\\\ \\n\\\\end{align}\\n$$\\n\\nUnlike the regression case, variable $t$ is defined with a non-Gaussian\\ndistribution. This prevents us from entering the GP framework right away\\nto model the target. However, the values produced by $a(\\\\cdot)$ are\\ncontinuous and can be modeled with a Gaussian process just like in the\\nregression case. Consequently, we can define a zero-meaned prior with\\nthe covariance function produced by a chosen kernel function\\n\\n$$\\n\\\\begin{align} \\np(\\\\vec{a}) = N(\\\\vec{a} | \\\\vec{0}, C_N) \\\\ .\\n\\\\end{align}\\n$$\\n\\nThe final predictive probability distribution, the probability of\\n$\\\\vec{t}^*$ given $\\\\vec{t}, X, \\\\vec{x}^{*}$, is predictably\\n\\n$$\\n\\\\begin{align} \\np(t^* \\\\mid \\\\vec{t}, X, \\\\vec{x}^{*}) \\\\text{\\\\ or, simply, \\\\ \\\\ } p(t^* \\\\mid \\\\vec{t}) = ?\\n\\\\end{align}\\n$$\\n\\nWe can rewrite it as \\n\\n$$\\n\\\\begin{align} \\np\\\\left(t^* \\\\mid \\\\vec{t}\\\\right) = \\\\int p\\\\left(t^* \\\\mid a^{*}\\\\right) p\\\\left(a^{*} | \\\\vec{t}\\\\right) d a^{*}\\n\\\\end{align}\\n$$\\n\\nwhere\\n$p\\\\left(t^* \\\\mid a^{*}\\\\right) = p\\\\left(t^* = 1 | a^{*}\\\\right) = \\\\sigma\\\\left(a^{*}\\\\right)$.\\nWe see that the first term $p\\\\left(t^* \\\\mid a^{*}\\\\right)$ is a sigmoid\\nand the posterior $p\\\\left(a^{*} | \\\\vec{t}\\\\right)$ can be rewritten in\\nthe following form \\n\\n$$\\n\\\\begin{align} \\np\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right) &= \\\\int p\\\\left(a^{*}, \\\\vec{a} \\\\mid \\\\vec{t} \\\\right) d \\\\vec{a} = \\\\\\\\\\n& = \\\\text{\\\\{ following Bayes formula \\\\}} = \\\\\\\\\\n& = \\\\int \\\\frac{ p\\\\left(\\\\vec{t} | a^{*}, \\\\vec{a} ) \\\\cdot p\\\\left(a^{*}, \\\\vec{a} \\\\right)\\\\right.} {p\\\\left(\\\\vec{t} \\\\right)} d \\\\vec{a} = \\\\\\\\\\n& = \\\\frac{1}{p (\\\\vec{t} )}  \\\\int p\\\\left(a^{*}, \\\\vec{a} \\\\right) p\\\\left(\\\\vec{t} \\\\mid a^{*}, \\\\vec{a} \\\\right) d \\\\vec{a} = \\\\\\\\ \\n& = \\\\left\\\\{P\\\\left(\\\\vec{t} \\\\mid a^{*}, \\\\vec{a} \\\\right) = p\\\\left(\\\\vec{t} \\\\left(\\\\vec{a} \\\\right)\\\\right) \\\\text{\\\\ since $\\\\vec{t} $ is independent of } a^{*} \\\\right\\\\} = \\\\\\\\ \\n& = \\\\frac{1}{p (\\\\vec{t} )} \\\\int p\\\\left(a^{*} \\\\mid \\\\vec{a}\\\\right) p\\\\left(\\\\vec{a} \\\\right) p\\\\left(\\\\vec{t} | \\\\vec{a} \\\\right) d \\\\vec{a} \\\\\\\\\\n& = \\\\text{\\\\{ using Bayes formula again \\\\}} = \\\\\\\\\\n& = \\\\int p\\\\left(a^{*} | \\\\vec{a} \\\\right) p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right) d \\\\vec{a} \\\\ .\\n\\\\end{align}\\n$$\\n\\nThe integral $p\\\\left(t^* \\\\mid \\\\vec{t}\\\\right)$ can be approximate using multiple methods, see [Bishop\\'s Section\\n6.4.5](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\\nto get an idea of those methods. He himself - maybe seeking for an\\neasier to write solution - goes for the Laplace approximation method\\nsince $p\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right)$ can be so neatly reconstructed. The term\\n$p\\\\left(a^{*} | \\\\vec{a} \\\\right)$ is the prior and\\ncan be found by using the GP for regressions. What is left is to notice\\nthat $p\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right)$ gives us a convolution of two Gaussian\\'s if\\nwe were two approximate $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ as such.\\nThen, one can the solution for the posterior and predictive distribution\\nby using [the\\nformula](https://math.stackexchange.com/questions/1745174/convolution-of-two-gaussian-functions)\\nfor convolving two Gaussian\\'s.\\n\\n\\n<br>\\n<br>\\n\\n<style>\\n  .intermezzo1 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 2px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo1::before {\\n    content: \"‚òÖ Intuition: Convolutions ‚òÖ\"; \\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    padding: 10px 5px;\\n    color: #4e3737;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo1\">\\n\\nThe general formula for a convolution is:\\n\\n$$\\n(f * g)(x) = \\\\int_{-\\\\infty}^{\\\\infty} f(t) g(x - t) \\\\, dt\\n\\\\tag{I.1}\\n$$\\n\\nWe can see $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ as $ f(t) $ because a zero mean was assumed for the $ a(\\\\cdot) $ \\'s GP. Thus, the formula in the exponent of $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ is something like:\\n\\n$$\\n-\\\\frac{1}{(2\\\\pi \\\\sigma^2)^{1/2}} a({\\\\vec{x}})^2 \\\\tag{I.2}\\n$$\\n\\nSimilarly, $ p\\\\left(a^{*} \\\\mid \\\\vec{a} \\\\right) $ corresponds to $ g(x - t) $, since conditioning on $ \\\\vec{a} $ will lead to something along the lines of:\\n\\n$$\\n-\\\\frac{1}{(2\\\\pi \\\\sigma^2)^{1/2}} \\\\left( a(\\\\vec{x}^{*}) - a(\\\\vec{x}) \\\\right)^2  \\\\tag{I.3}\\n$$\\n\\nVery roughly speaking, you can interpret this convolution as marginalizing out $\\\\vec{a}$ and determining the probability of $ a^{*} $.\\n\\n<style>\\n  .intermezzo12 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 40px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo12::before {\\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    background: white;\\n    padding: 10px 10px;\\n    color: #666;\\n    font-size: 1.2em;\\n  }\\n</style>\\n\\n<br>\\n\\n[//]: # (<hr class=\"intermezzo12\">)\\n\\n\\n<style>\\n  .intermezzo21 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 2px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo21::before {\\n    content: \"‚òÖ Intuition: Laplace approximation ‚òÖ\"; \\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    padding: 10px 5px;\\n    color: #4e3737;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo21\">\\n\\nLaplace\\'s approximation takes a uni-modal function where most of the probability mass is concentrated around its mode and approximates it with a Normal density function.\\n\\nHow do we know that $p\\\\left(\\\\vec{a}_N \\\\mid \\\\vec{t}_N\\\\right)$ is a function that can be approximated with the Laplace method? We can see that\\n\\n$$\\np(\\\\vec{a}|\\\\vec{t}) \\\\propto p(\\\\vec{a}) \\\\cdot p(\\\\vec{t} | \\\\vec{a}) \\\\text{ where}\\n$$\\n\\n$$\\np(\\\\vec{a}) = N(\\\\vec{a} | \\\\vec{0}, {C}_N) \\\\text{ , and }\\n$$\\n\\n$$\\np\\\\left(\\\\vec{t} \\\\mid \\\\vec{a} \\\\right) = \\\\prod_{n=1}^N \\\\sigma\\\\left(a_n\\\\right)^{t_n}\\\\left(1-\\\\sigma\\\\left(a_n\\\\right)\\\\right)^{1-t_n}=\\\\prod_{n=1}^N e^{a_n t_n} \\\\sigma\\\\left(-a_n\\\\right).\\n$$\\n\\nThe first term, $p(\\\\vec{a}_{N})$, is a normal distribution by (previous) construction, and therefore uni-modal. The second term, $p\\\\left(\\\\vec{t}_N \\\\mid \\\\vec{a}_N\\\\right)$, is a Bernoulli variable, and multiplication of a Bernoulli and a Gaussian is another (possibly skewed) distribution with a single mode (see [here](https://math.stackexchange.com/questions/3315169/what-is-the-distribution-of-the-product-of-a-bernoulli-0-1-a-normal-random-v), [here](https://stats.stackexchange.com/questions/27097/what-is-the-distribution-of-the-product-of-a-bernoulli-a-normal-random-variabl), and [here](https://www.math.wm.edu/~leemis/chart/UDR/PDFs/BernoulliP.pdf)). Bishop provides more details on why this is the case, and we can assume that $p(\\\\vec{a}_N|\\\\vec{t}_N)$ could be approximated by the Laplace method.\\n\\nThe approximation itself is a bit too cumbersome to go into detail. The idea is that one needs to first find the mode of $p(\\\\vec{a} |\\\\vec{t})$ by Taylor expanding the logarithm of $p(\\\\vec{a} | \\\\vec{t})$ and setting its derivative to zero. Second, the Hessian matrix helps to find the covariance matrix of the approximate distribution. This is because the inverse of the Hessian matrix of the log-likelihood function at the maximum likelihood estimate is asymptotically equal to the covariance matrix of the parameter estimates (see [Gaussian curvatures](https://en.wikipedia.org/wiki/Gaussian_curvature) and [Fisher information matrix](https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s)).\\n\\n\\n<style>\\n  .intermezzo22 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 40px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo22::before {\\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    background: white;\\n    padding: 10px 10px;\\n    color: #666;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo22\">\\n\\n\\nSay, the Laplace approximation yields \\n\\n$$\\n\\\\begin{align} \\nq(\\\\vec{a})= N \\\\left(\\\\vec{a} \\\\mid \\\\hat{\\\\vec{a}}, {H}^{-1}\\\\right)\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:gp_class_laplace_solution})\\n\\nwhere $\\\\hat{\\\\vec{a}}$ is the mean\\n(mode) of the approximated function and the covariance\\n$H = W + C^{\\\\top}$ with $W$ being a diagonal matrix with elements\\n$\\\\sigma(a_{i})(1 - \\\\sigma(a_i))$ while $a_n$ are the elements of\\n$\\\\vec{a}$ and $C$ being the prior\\'s covariance matrix. Then, the\\nGaussian approximation of $p(a^{*} | t)$ is\\n$p\\\\left(a^* \\\\mid \\\\vec{t}\\\\right)  \\\\approx {N}\\\\left(a^* \\\\mid \\\\mu_{a^*}, \\\\sigma_{a^*}^2\\\\right)$\\nis \\n\\n$$\\n\\\\begin{align} \\n\\\\mu_{a^*} &= \\\\vec{k}^{*^T}(\\\\vec{t}-\\\\vec{\\\\sigma}) \\\\\\\\\\n\\\\sigma_{a^*}^2 &= k^{* *} - \\\\vec{k}^{*^T}\\\\left({W}^{-1} + C\\\\right)^{-1} \\\\vec{k}^*\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:gp_class_final_solution})\\n\\nwhere $\\\\vec{\\\\sigma}$ is the vector\\nthat collects all the $\\\\sigma(a_i))$ ; $\\\\vec{k}^{*^T}$ all the values of\\nthe kernel function evaluated with the new data point $\\\\vec{x}^*$ and\\nthe rest of the data $X$, i.e., $K (\\\\vec{x}^*, X)$; $k^{* *}$ is the\\nkernel function evaluated at the point of interest, i.e.,\\n$K(\\\\vec{x}^*, \\\\vec{x}^*)$.\\n\\n$~$\\n## References\\n$~$\\n\\n-   [Short explanation](http://krasserm.github.io/2020/11/04/gaussian-processes-classification/)\\n\\n-   [See Bishop for details](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e5cf67c9-7f23-4a5c-b50c-74161e6c580c', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/linear_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 31014, 'creation_date': '2025-01-23', 'last_modified_date': '2025-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<center>\\n    <h1> Linear Models & Intro to Kernels  </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? ü§î\\n$~$\\n\\nThe workhorse of data modeling in many industries is still a linear model. If a such\\ncompany applies it somehow on their data, then in 90% of the cases\\nthey are confidently bragging online that\\n\"we are utilising Machine learning and AI\\\\\". Well, a linear model is a beautiful, simple and powerful tool,\\nand as experience comes you are more eager to use it rather than an advanced transformer-based-state-of-the-art \"monster\".\\n\\nEqually beautiful is a kernel model. Though, kernel models have slightly fallen from grace in\\nthe past few decades because of their scaling issues (read, can be slow on very large datasets), \\nthey are still a must-have tool for anybody working in ML.\\n\\nBoth of those tool rely heavily on probability theory so we start there.\\n\\n$~$\\n\\n$~$\\n## Exponential Family of Probability Distributions üé≤\\n$~$\\n\\nThe exponential family of distributions can be applied in a wide-range of scenario and\\nit is one of the easiest (if not the easiest) to use. \\nA large number of common modeling tools is based on it.\\n\\n$~$\\n\\n$~$\\n### Bernoulli\\n$~$\\n\\nI heard somewhere a person referring to a Bernoulli distribution as a\\n\"damaged coin\\\\\" distribution. It tries to predict the likelihood of a\\ncoin flip landing heads or tails, i.e., some binary event $x$. Thus, if\\na coin shines heads we win and have a success $x=1$, otherwise, we lose\\nand it is $x=0$. Since the coin is damaged, we cannot assume that the\\nprobability of it landing heads/tails is 50/50. The parameter $\\\\mu$\\ndescribes the probability of $x=1$: \\n\\n$$\\n\\\\begin{align}\\np(x=1 \\\\mid \\\\mu)=\\\\mu \\\\quad \\\\& \\\\quad p(x=0 \\\\mid \\\\mu)=1-\\\\mu \\\\ . \\n\\\\end{align}\\n$$\\n\\nSince the two outcomes are mutually exclusive, we can easily combine\\ntheir probabilities into a formula for both cases $x=1$ and $x=0$\\n\\n$$\\n\\\\begin{align}\\n{Bern}(x \\\\mid \\\\mu)=\\\\mu^x(1-\\\\mu)^{1-x}\\n\\\\end{align}\\n$$\\n\\nIf we had to bet our money on a coin flip, we would want to see how\\nlikely each event is, i.e., estimate $\\\\mu$. We could base our estimation\\non a data sample $D = [x_1 ... x_N]$ of independent and identically\\ndistributed random events. In other words, we would want to find the\\nlikelihood that $D$ occurred under $\\\\mu$ \\n\\n$$\\n\\\\begin{align}\\np(D \\\\mid \\\\mu)=\\\\prod_{n=1}^N p(x_n(\\\\mu))=\\\\prod_{n=1}^N \\\\mu^{x_n}(1-\\\\mu)^{1-x_n} \\\\ .\\n\\\\end{align}\\n$$\\n\\nThe maximum likelihood estimator of $\\\\mu$ is determined by maximizing\\nthe probability of sample data occurring,i.e., finding the most likely\\nparameter that could produce this data sample. The estimation of $\\\\mu$\\ncan be then via the maximum likelihood: \\n\\n$$\\n\\\\begin{align}\\n{\\\\underset{\\\\mu}{\\\\operatorname{argmax \\\\ }}} p(D | \\\\mu) \\\\equiv {\\\\underset{\\\\mu}{\\\\operatorname{argmax \\\\ }}}  \\\\ln \\\\ p(D | \\\\mu) \\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Binomial\\\\*\\n$~$\\n \\nThis (*) distribution is not in the exponential family but it is a generalisation of the Bernoulli distribution.\\nPLus, under certain condition it can be rewritten in the exponential form.\\n\\n$~$\\n\\nLet\\'s consider now that instead of a single coin flip we count the number of\\nsuccesses $m$ we get out of $N$ flips: \\n\\n$$\\n\\\\begin{align}\\n{Bin}(m \\\\mid N, \\\\mu)=\\\\binom{N}{m} \\\\mu^m(1-\\\\mu)^{N-m} \\\\\\\\\\n\\\\text{where} \\\\binom{N}{m}=\\\\frac{N!}{(N-m)!m!}\\n\\\\end{align}\\n$$\\n\\nThen, if we had $k$ trials of flipping a coin $N$ and observing $m_i$\\nsuccesses, estimating $\\\\mu$ can be done with the maximum likelihood\\n${\\\\underset{\\\\mu}{\\\\operatorname{argmax \\\\ }}} p(D | N, \\\\mu)$ where\\n\\n$$\\n\\\\begin{align} \\nP(D | N, \\\\mu)=\\\\prod_{i=1}^k p\\\\left(m_i | N, \\\\mu\\\\right) = \\\\prod_{i=1}^k \\\\operatorname{Bin}\\\\left(m_i | N, \\\\mu\\\\right)=\\\\prod_{i=1}^k\\\\binom{N}{m_i} \\\\mu^{m_i}(1-\\\\mu)^{N-m_i} \\\\ .\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Gamma\\n$~$\\n\\nGamma distribution is often used to describe a continues variable that\\nis always positive and has skewness in its distribution frequency. It is\\nparameterised with $\\\\alpha > 0$ and $\\\\beta > 0$ to control shape and\\nthickness of the probability density function. \\n\\n$$\\n\\\\begin{align} \\n\\\\begin{align}\\n&\\\\begin{align}\\n& y-\\\\operatorname{Ganma}(\\\\alpha, \\\\beta) \\\\\\\\\\n& p(y \\\\mid \\\\alpha, \\\\beta)=\\\\beta^\\\\alpha y^{\\\\alpha-1} e^{-\\\\beta y} \\\\frac{1}{\\\\Gamma(\\\\alpha)} \\\\ .\\n\\\\end{align}\\\\\\\\\\n&\\\\Gamma(\\\\alpha)=\\\\int^{\\\\infty} x^{\\\\alpha-1} e^{-x} d x\\n\\\\end{align}\\n\\\\end{align}\\n$$\\n\\nParameter $\\\\alpha$ controls the shape/skewness of the density\\nfunction. A common use case for the Gamma distribution is modeling the\\nwait time until $\\\\alpha$-th event occurs. Parameter $\\\\beta$ stretches or\\ncompresses the distribution horizontally. Larger values of $\\\\beta$\\nspread out the distribution, while smaller values concentrate it closer\\nto zero.\\n\\n$~$\\n\\nThe Gamma distribution is closely related to Exponential distribution\\nand Chi-Squared distribution where the later are special cases of Gamma.\\n\\nThe gamma function can be seen as an extension of the factorial function\\nbut for positive-real (and complex) numbers. It pretty much interpolates\\nthe common factorial function for integers making the function line\\nsmooth (and silky).\\n\\n$~$\\n\\n$~$\\n### Poisson\\n$~$\\n\\nModels the number of occurrences of an event in a given unit of time,\\ndistance, area or volume. For example, number of accidents in a day or\\nnumber of fruits/plants grown on a squared meter of land. The necessary\\nconditions for the Poisson probability distribution is that the events\\noccur independently and the probability of an event does not vary with\\ntime. \\n\\n$$\\n\\\\begin{align} \\nx \\\\sim Poisson (\\\\lambda) \\\\\\\\\\nP(x \\\\mid \\\\lambda)=\\\\frac{\\\\lambda^x e^{-\\\\lambda}}{x!}\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Beta \\n$~$\\n\\nThe Beta distribution is very frequently used in machine learning for it\\nis perfectly suited to represent percentages, proportions or\\nprobabilities since its domain is restricted from 0 to 1.\\n\\n$$\\n\\\\begin{align} \\n{Beta}(\\\\mu \\\\mid a, b)=\\\\frac{\\\\Gamma(a+b)}{\\\\Gamma(a) \\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1}\\n\\\\end{align}\\n$$\\n\\nwhere bot $a$ and $b$ change the shape of the probability density\\nfunction.\\n\\n$~$\\n\\nThis distribution is so used because its nice property of being a\\nfabulous conjugate prior. A little demonstration follows. Given the **Bayes\\' rule** \\n\\n$$\\n\\\\begin{align} \\np(\\\\mu \\\\mid D)=\\\\frac{p(D \\\\mid \\\\mu) p(\\\\mu)}{P(D)} = \\\\frac{p(D \\\\mid \\\\mu) p(\\\\mu)}{\\\\int_\\\\mu p(D | \\\\mu) p(\\\\mu) d \\\\mu} \\n\\\\end{align}\\n$$ \\n\\nwhere $D$ is the all the previous knowledge (e.g.,\\nparameters, observations) that we have acquired from the past data\\nsample(s). If we wanted to do estimation of our posterior the following would\\nhold: \\n\\n$$\\n\\\\begin{align} \\np(\\\\mu \\\\mid D) \\\\propto p(D \\\\mid \\\\mu) P(\\\\mu) \\\\ . \\n\\\\end{align}\\n$$\\n\\nRemember that the Binomial\\'s distribution parameter $\\\\mu$ is the\\nprobability of $m$ successes in $N$ trials and constrained to the range\\n$[0,1]$. What if we wanted to estimate the parameter $\\\\mu$ with just a\\nsingle experiment observation $(m,N)$-pair but also had a prior\\ninformation that $\\\\mu$ follows a Beta distribution. \\n\\n$$\\n\\\\begin{align} \\n\\\\begin{gathered}\\n\\\\{l=N-m\\\\} \\\\\\\\\\np(D|\\\\mu) = Bin(m \\\\mid N, \\\\mu)= \\\\binom{N}{m} \\\\mu^m(1-\\\\mu)^l  \\\\text{\\\\ \\\\ \\\\ (likelihood) } \\\\\\\\\\np(\\\\mu) = {Beta}(\\\\mu \\\\mid a, b) = \\\\frac{\\\\Gamma(a+b)}{\\\\Gamma(a) \\\\Gamma(b)} \\\\mu^{a-1}(1-\\\\mu)^{b-1} \\\\text{\\\\ \\\\ \\\\ (prior)}\\n\\\\end{gathered}\\n\\\\end{align}\\n$$\\n\\nThen, following the Equation 17\\n\\n$$\\n\\\\begin{align} \\np\\\\left(\\\\mu \\\\mid D = (m, l, a, b)\\\\right) &\\\\propto \\\\binom{N}{m} \\\\mu^m(1-\\\\mu)^l \\\\cdot \\\\frac{\\\\Gamma(a+b)}{\\\\Gamma(a) \\\\Gamma(b)} \\\\mu^{a-1} \\\\cdot(1-\\\\mu)^{b-1} \\\\\\\\\\n& \\\\text{\\\\{skipping the coefficient for Beta prior as it does not depend on\\\\ } \\\\mu \\\\} \\\\\\\\\\n& \\\\propto\\\\binom{N}{m} \\\\mu^{m+a-1}(1-\\\\mu)^{l+b-1} \\\\\\\\\\n& \\\\text{\\\\{skipping the binomial coefficient as it does not depend on\\\\ } \\\\mu \\\\} \\\\\\\\\\n& \\\\propto \\\\mu^{m+a-1}(1-\\\\mu)^{l+b-1}\\n\\\\end{align}\\n$$\\n\\nThe last equation looks like a non-normalised Beta distribution, i.e.,\\nfunctional form of $Beta(\\\\mu | m+a, l+b)$ can already be seen. We can\\nadd the normalisation constant to make it a proper distribution.\\nFinally, \\n\\n$$\\n\\\\begin{align} \\np(\\\\mu \\\\mid D) = Beta(\\\\mu | m+a, l+b) = p(\\\\mu \\\\mid m, l, a, b)=\\\\frac{\\\\Gamma(m+a+l+b)}{\\\\Gamma(m+a) \\\\cdot \\\\Gamma(l+b)} \\\\mu^{m+a-1}(1-\\\\mu)^{l+b-1}\\n\\\\end{align}\\n$$\\n\\nNote that we have gotten a full distribution for $\\\\mu$ instead of just a\\nsingle estimate as was the case for the maximum likelihood estimator.\\nHowever, this posterior distribution could also be used to produce a\\npoint estimate for $\\\\mu$ by calculating its mean or mode.\\n\\n$~$\\n\\nIf we were to make predictions based on the data and the prior, we can\\ncalculate the variance of $\\\\mu$ to access uncertainty about the\\nestimator and our future predictions. Optionally, we can also sample\\nfrom the posterior distribution to have a whole range of possible values\\nfor the next prediction phase. Then, we would use the posterior predictive\\ndistribution \\n\\n$$\\n\\\\begin{align} \\np(\\\\hat{m} \\\\mid D)=\\\\int p(\\\\hat{m} \\\\mid \\\\mu) p(\\\\mu \\\\mid D) d \\\\mu\\n\\\\end{align}\\n$$ \\n\\nwhere we can sample from each time we want to produce a\\npossible value for the next observation $\\\\hat{m}$. If you work out the\\nintegral, the predictive distribution is actually the expectation of\\n$p(\\\\mu \\\\mid D)$ \\n\\n$$\\n\\\\begin{align} \\np(\\\\hat{m} \\\\mid D)=\\\\mathbb{E}_{\\\\mu \\\\mid D}[p(\\\\hat{m} \\\\mid \\\\mu)]  \\\\ .\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Multinomial\\n$~$\\n\\nThe next distribution describes a vector $\\\\vec{x} \\\\in \\\\mathbb{R}^k$\\nwhere 1 of $k$ elements is 1 and the rest are 0\\'s, or \\n\\n$$\\n\\\\begin{align} \\n& \\\\vec{x}=(0,0,0,1,0,0)^{\\\\top} \\\\\\\\\\n& \\\\text { \\\\& } \\\\sum_{i=1}^k x_i=1 \\\\ .\\n\\\\end{align}\\n$$\\n\\nWe can describe the probability of each $\\\\vec{x}$ by generalising the\\nBernoulli distribution to more than two variables \\n\\n$$\\n\\\\begin{align} \\np(\\\\vec{x} \\\\mid \\\\vec{\\\\mu})=\\\\prod_{i=1}^K \\\\mu_i^{x_i} \\\\\\\\\\n\\\\vec{\\\\mu}=\\\\left(\\\\mu_{1 n} \\\\ldots, \\\\mu_k\\\\right)^{\\\\top} \\\\\\\\ \\n\\\\mu_i \\\\geq 0 \\\\text{ \\\\ \\\\& \\\\ }  \\\\sum_{i=1}^k \\\\mu_i=1 \\\\ . \\n\\\\end{align}\\n$$\\n\\nThe expectation of $\\\\vec{x}$ is then \\n\\n$$\\n\\\\begin{align} \\n\\\\mathbb{E}[\\\\vec{x} \\\\mid \\\\vec{\\\\mu}]=\\\\sum_{\\\\vec{x}} p(\\\\vec{x} | \\\\vec{\\\\mu}) \\\\vec{x}=\\\\left(\\\\mu_{1},  \\\\ldots, \\\\mu_k\\\\right)^{\\\\top}=\\\\vec{\\\\mu} \\\\ . \\n\\\\end{align}\\n$$\\n\\nThe likelihood of $N$ i.i.d. vectors $\\\\vec{x}$ is \\n\\n$$\\n\\\\begin{align} \\np(D \\\\mid \\\\vec{\\\\mu} ) &= \\\\prod_{n=1}^N \\\\prod_{i=1}^K \\\\mu_i^{x_{n, i}} = \\\\prod_{i=1}^K \\\\mu_i^{(\\\\sum_{n}^N x_{n i})} = \\\\\\\\\\n& = \\\\prod_{i=1}^K \\\\mu_i^{m_i}\\n\\\\end{align}\\n$$ \\n\\nwhere $m_i$ is the number of observations where $x_i$\\n= 1. For each dimension $i$ of $\\\\vec{x}$, we will have $m_i$ time\\nthat we observed 1. A simple example of $m_i$ would be how many times we\\nhad 4 after rolling a dice 20 times. Thus, the Multinomial distribution describes the joint probability of\\nobserving quantities $m_i$\\'s after $N$ total experiments (e.g, dice\\nrolls) \\n\\n$$\\n\\\\begin{align} \\n{Mult}\\\\left(m_1, m_2, \\\\ldots, m_k \\\\mid \\\\vec{\\\\mu}, N\\\\right)  = \\\\\\\\ \\n= \\\\binom{N}{\\\\left(m_1, m_2, \\\\ldots, m_k\\\\right)} \\\\cdot \\\\prod_{i=1}^k \\\\mu_i^{m_i}= \\\\\\\\ \\n= \\\\frac{N!}{m_{1}!m_{2}!\\\\ldots m_{k}!} \\\\cdot \\\\prod_{i=1}^k \\\\mu_i^{m i}\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Dirichlet\\n$~$\\n\\nDirichlet distribution is a conjugate prior for the Multinomial. It uses\\n$\\\\vec{\\\\alpha}$ to parameterise a function that is perfect for describing\\nthe variables $\\\\vec{\\\\mu}$ where $0 \\\\leq \\\\mu_{i} \\\\geq 1$ &\\n$\\\\sum_i \\\\mu_i = 1$. \\n\\n$$\\n\\\\begin{align} \\n{Dir}(\\\\vec{\\\\mu} \\\\mid \\\\vec{\\\\alpha})=\\\\frac{\\\\Gamma\\\\left(\\\\alpha_0\\\\right)}{\\\\Gamma\\\\left(\\\\alpha_1\\\\right) \\\\ldots \\\\Gamma\\\\left(\\\\alpha_k\\\\right)} \\\\prod_{i=1}^k \\\\mu_i^{\\\\alpha_i-1} \\\\ . \\n\\\\end{align}\\n$$\\n\\nThe posterior for multinomial distribution then becomes\\n\\n$$\\n\\\\begin{align} \\np(\\\\vec{\\\\mu} \\\\mid D, \\\\alpha)=\\\\operatorname{Dir}(\\\\vec{\\\\mu} \\\\mid \\\\vec{\\\\alpha}+\\\\vec{m})= \\\\\\\\ =\\\\frac{\\\\Gamma\\\\left(\\\\alpha_0+N\\\\right)}{\\\\Gamma\\\\left(\\\\alpha_1+m_1\\\\right) \\\\ldots \\\\Gamma\\\\left(\\\\alpha_k+m_k\\\\right)} \\\\prod_{i=1}^k \\\\mu_i^{\\\\alpha_i+m_i-1}\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Normal \\n$~$\\n\\nYet another distribution that probably does not need a lot of intro is a\\nGaussian single-variate \\n\\n$$\\n\\\\begin{align} \\nN\\\\left(x \\\\mid \\\\mu, \\\\sigma^2\\\\right)=\\\\frac{1}{\\\\left(2 \\\\pi \\\\sigma^2\\\\right)^{\\\\frac{1}{2}}} e^{-\\\\frac{1}{2 \\\\sqrt{\\\\sigma}}(x-\\\\mu)^2}\\n\\\\end{align}\\n$$ \\n\\nor a Gaussian multivariate \\n\\n$$\\n\\\\begin{align} \\nN(\\\\vec{x} \\\\mid \\\\vec{\\\\mu}, \\\\Sigma)=\\\\frac{1}{(2 \\\\pi)^{0 / 2}} \\\\frac{1}{\\\\mid \\\\Sigma^{1 / 2}} e^{-\\\\frac{1}{2}(\\\\vec{x}-\\\\vec{\\\\mu})^{\\\\top} \\\\Sigma^{-1}(\\\\vec{x} - \\\\vec{\\\\mu})} \\\\ . \\n\\\\end{align}\\n$$\\n\\nThe parameter $\\\\vec{\\\\mu} \\\\in \\\\mathbb{R}^D$ is the mean vector where $\\\\vec{\\\\mu}$\\nalso show the expected values for each dimension of the variable vector\\n$\\\\vec{x} \\\\in \\\\mathbb{R}^D$. The $\\\\Sigma$ matrix is called the covariance\\nmatrix and needs to be positive definite for the probability mass to\\nexist. The covariance matrix can also be diagonal. That helps with its\\ninversion ($\\\\Sigma^{-1}$ is often called the\\nprecision matrix) and makes all the variables $x_i$ independent of one\\nanother. Gamma is a conjugate prior to the Gaussian.\\n\\n$~$\\n\\n$~$\\n## Linear Models ‚Üó\\n$~$\\n\\nNow, we can move on to building simple linear models.\\n\\n$~$\\n### Simple Linear Regression\\n$~$\\n\\nLinear regression solves the problem of modeling a continues\\n$t \\\\in \\\\mathbb{R}$ based on the observed data points $\\\\vec{x}$\\'s. The\\nunderpinning assumption that we are making when modeling $t$ is that it\\nis generated by some deterministic linear process $y$ and a random\\n(white-noise) variable $\\\\varepsilon$. The random variable is normally\\ndistributed, zero centered and has a constant standard deviation\\n$\\\\sigma$, i.e., then the exponent of the Gaussian can be simplified\\n$\\\\frac{1}{2}(\\\\vec{t}-\\\\vec{\\\\mu})^{\\\\top} \\\\Sigma^{-1}(\\\\vec{t} - \\\\vec{\\\\mu}) = \\\\frac{1}{2} \\\\sum_{i=1}^N\\\\left(t_i-\\\\vec{\\\\omega}^{\\\\top} \\\\phi\\\\left(\\\\vec{x}_i\\\\right)\\\\right)^2$.\\nWe say $\\\\varepsilon \\\\sim N (0, \\\\beta^{-1})$ where $\\\\sigma^{-2} = \\\\beta$, and\\n$t$ follows \\n\\n$$\\n\\\\begin{align}\\nt \\\\sim N\\\\left(y(\\\\vec{x}, \\\\vec{w}), \\\\beta^{-1}\\\\right)\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_y})\\n\\nwhere $\\\\vec{x}$ is a vector of features and $\\\\vec{w}$\\nare the weights of the linear model. The model $y$ is in turn a linear\\ncombination of weights and basis function values\\n\\n$$\\n\\\\begin{align}\\ny(\\\\vec{x}, \\\\vec{\\\\omega}) = \\\\omega_0 + \\\\sum_{j=1}^{M+1} \\\\omega_j \\\\phi_j(\\\\vec{x}) = \\\\sum_{j=0}^{M} \\\\omega_j \\\\phi_j(\\\\vec{x})\\n\\\\end{align}\\n$$\\n\\nand\\n\\n$$\\n\\\\left\\\\{\\n\\\\begin{array}{c}\\n\\\\vec{x} \\\\in \\\\mathbb{R}^D \\\\\\\\\\n\\\\vec{w} \\\\in \\\\mathbb{R}^M\\n\\\\end{array}\\n\\\\right.\\n$$\\n[//]: # (\\\\label{eq:linear_reg_y_1})\\n\\nwhere $\\\\phi_j(\\\\cdot)$ is some (non-)linear basis\\nfunction, e.g., [radial\\nbasis](#https://hackernoon.com/radial-basis-functions-types-advantages-and-use-cases)\\nfunctions. Later, we will work with\\n$\\\\Phi = [\\\\vec{\\\\phi}_{1}^{\\\\top}(\\\\vec{x_1}), \\\\vec{\\\\phi}_{2}^{\\\\top}(\\\\vec{x_2}) ... \\\\vec{\\\\phi}_{N}^{\\\\top}(\\\\vec{x_N})]$\\nand\\n$\\\\vec{\\\\phi}_{i} = [\\\\phi_1(\\\\vec{x}_i), \\\\phi_2(\\\\vec{x}_i), ..., \\\\phi_M(\\\\vec{x}_i)]$.\\nIn other words, a feature matrix holds pre-calculated basis function\\noutput values for each data point $\\\\vec{x}$.\\n\\n$~$\\n\\nLet\\'s see what happens if we continue with the assumption that the pdf of $t$ is Gaussian \\n\\n$$\\n\\\\begin{align} \\nP(t \\\\mid \\\\vec{x}, \\\\vec{\\\\omega}, \\\\beta)=N\\\\left(t | y(\\\\vec{x}, \\\\vec{\\\\omega}), \\\\beta^{-1}\\\\right) \\\\  \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_y_2})\\n\\nand collect a bunch of data points\\n$D = \\\\{(\\\\vec{x}_1,t_1), (\\\\vec{x}_2,t_2) ... (\\\\vec{x}_N,t_N)\\\\}$ with the\\ntarget being $\\\\vec{t} = [t_1, t_2, ... t_N]$ and features\\n$X = [\\\\vec{x}_1, \\\\vec{x}_2, ... \\\\vec{x}_N]$. We can write the likelihood model\\nas \\n\\n$$\\n\\\\begin{align} \\nP(D | \\\\vec{w}) = p(\\\\vec{t} | X, \\\\vec{w}, \\\\beta) = \\\\prod_{i=1}^N N(t_i|\\\\vec{w} \\\\cdot \\\\phi(\\\\vec{x_i}, \\\\beta^{-1})) \\\\ . \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_y_3})\\n\\nTaking the logarithm of the likelihood function, we arrive to the log\\nlikelihood: \\n\\n$$\\n\\\\begin{align} \\n\\\\ln p(t \\\\mid X, \\\\vec{w}, \\\\beta) =\\\\sum_{i=1}^N \\\\ln N\\\\left(t_i \\\\mid \\\\vec{\\\\omega}^{\\\\top} \\\\phi\\\\left(\\\\vec{x}_i\\\\right), \\\\beta^{-1}\\\\right) = \\\\frac{N}{2} \\\\ln \\\\beta-\\\\frac{N}{2} \\\\ln (2 \\\\pi)- \\\\beta \\\\cdot {E}_D(\\\\vec{w})\\n\\\\end{align}\\n$$ \\n[//]: # (\\\\label{eq:linear_reg_y_3})\\n\\nwhere \\n\\n$$\\n\\\\begin{align} \\nE_{D}(\\\\vec{w})=\\\\frac{1}{2} \\\\sum_{i=1}^N\\\\left(t_i-\\\\vec{\\\\omega}^{\\\\top} \\\\phi\\\\left(\\\\vec{x}_i\\\\right)\\\\right)^2\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_y_3})\\n\\nis the sum-of-squares error.\\n\\nIn order to find the Maximum Likelihood Estimator for the mean we can\\ncontinue with \\n\\n$$\\n\\\\begin{align} \\n\\\\vec{w}_{MLE} = \\\\underset{w}{\\\\operatorname{argmax}} \\\\ p(D \\\\mid \\\\vec{\\\\omega}) \\\\Leftrightarrow \\\\underset{w}{\\\\operatorname{argmax}} \\\\prod_{i=1}^{N} p\\\\left(t_i \\\\mid \\\\vec{w}\\\\right) \\\\Leftrightarrow \\\\underset{w}{\\\\operatorname{argmax}} \\\\sum_{i=1}^N \\\\ln p\\\\left(t_i \\\\mid \\\\vec{w}\\\\right)\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_mle})\\n\\nTo do the Maximum Aposteriori Estimation, we take a step further  \\n\\n$$\\n\\\\begin{align} \\np(\\\\vec{w} \\\\mid D) = p(\\\\vec{w} \\\\mid x, \\\\vec{t}, \\\\beta, \\\\alpha) = \\\\frac{p(\\\\vec{t} \\\\mid x, \\\\vec{w}, \\\\beta) p(\\\\vec{w} \\\\mid \\\\alpha)}{ \\\\underbrace{p(\\\\vec{t} \\\\mid x, \\\\beta, \\\\alpha)}_\\\\text{independent of $\\\\vec{w}$} }\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_map_1})\\n\\n\\n$$\\n\\\\begin{align} \\n\\\\vec{w}_{MAP} &= \\\\underset{w}{\\\\operatorname{argmax}} \\\\prod_{i=1}^N p\\\\left(t_i \\\\mid \\\\vec{x}_i, \\\\vec{w}, \\\\beta\\\\right) P(\\\\vec{w} \\\\mid \\\\alpha) \\\\\\\\\\n&= \\\\underset{w}{\\\\operatorname{argmax}} \\\\sum_i^N\\\\left[\\\\ln \\\\left(p\\\\left(t_i \\\\mid \\\\vec{x}_i, \\\\vec{w}, \\\\beta\\\\right)\\\\right) + \\\\ln p(\\\\vec{w} | \\\\alpha)]\\\\right.\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_map_2})\\n\\nIf we assume an uninformative prior, i.e., the weights are i.i.d. and\\nhave a constant variance, the log probability takes form\\n\\n$$\\n\\\\begin{align} \\n\\\\ln (p(\\\\vec{w} \\\\mid \\\\alpha))=\\\\ln (N(0, \\\\alpha)) = \\\\ln \\\\left(\\\\frac{1}{2 \\\\pi} \\\\alpha\\\\right)^{1 / 2} e^{{-\\\\frac{1}{2} \\\\alpha} \\\\vec{w}^{\\\\top} \\\\vec{w}} \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_map_3})\\n\\nOptimising MAP becomes is then equivalent to extremising\\nthe partial derivative of the posterior w.r.t. the weights\\n\\n$$\\n\\\\begin{align} \\n\\\\frac{\\\\partial p(\\\\vec{w} \\\\mid D)}{\\\\partial \\\\vec{w}} &=  \\\\frac{\\\\partial \\\\ln p(\\\\vec{w} | \\\\Phi, \\\\vec{t}, \\\\alpha, \\\\beta)}{\\\\partial \\\\vec{w}} =  \\\\frac{\\\\partial \\\\ln p(\\\\vec{t} \\\\mid \\\\vec{w}, \\\\Phi, \\\\beta)}{\\\\partial \\\\vec{w}}+\\\\frac{\\\\partial \\\\ln p(\\\\vec{w} | \\\\alpha)}{\\\\partial \\\\vec{w}} = \\\\\\\\\\n&= \\\\frac{\\\\partial}{\\\\partial \\\\vec{w}}\\\\left(\\\\frac{1}{2 \\\\sigma_{\\\\beta}^2}(\\\\vec{t}-\\\\Phi \\\\vec{w})^{\\\\top}(\\\\vec{t}-\\\\Phi \\\\vec{w}) \\\\right) +\\\\frac{\\\\partial}{\\\\partial \\\\vec{w}}\\\\left(\\\\frac{1}{2\\\\sigma_{\\\\alpha}^2} \\\\vec{w}^{\\\\top} \\\\vec{w} \\\\right) \\\\\\\\ \\n&=\\\\frac{1}{\\\\sigma_{\\\\beta}^2}\\\\left(\\\\vec{w}^{\\\\top} \\\\Phi^{\\\\top} \\\\Phi-t^{\\\\top} \\\\Phi\\\\right)+\\\\frac{1}{\\\\sigma_\\\\alpha^2} w^{\\\\top} = \\\\beta\\\\left(\\\\vec{w}^{\\\\top} \\\\Phi^{\\\\top} \\\\Phi-t^{\\\\top} \\\\Phi\\\\right) + {\\\\alpha} \\\\vec{w}^{\\\\top} \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_map_5})\\n\\n\\n$$\\n\\\\begin{align} \\n\\\\beta\\\\left(\\\\vec{\\\\omega}^{\\\\top} \\\\Phi^{\\\\top} \\\\Phi-\\\\vec{t}^{\\\\top} \\\\Phi\\\\right)+\\\\alpha \\\\vec{w}^{\\\\top}=0 \\\\\\\\\\n\\\\vec{w}^{\\\\top}\\\\left(\\\\beta \\\\Phi^{\\\\top} \\\\Phi+\\\\alpha I\\\\right)-\\\\beta \\\\vec{t}^{\\\\top} \\\\Phi=0 \\\\\\\\\\n\\\\vec{w}^{\\\\top}\\\\left(\\\\beta \\\\Phi^{\\\\top} \\\\Phi+\\\\alpha I\\\\right)=\\\\beta \\\\vec{t}^{\\\\top} \\\\Phi \\\\\\\\\\n\\\\vec{w}^{\\\\top} = \\\\vec{t} \\\\Phi\\\\left(\\\\Phi^{\\\\top} \\\\Phi+\\\\frac{\\\\alpha}{\\\\beta} I\\\\right)^{-1} \\\\\\\\\\n\\\\vec{w}_{MAP}=\\\\left(\\\\Phi^{\\\\top} \\\\Phi + \\\\lambda \\\\mathbb{I} \\\\right)^{-1} \\\\Phi^{\\\\top} \\\\vec{t} \\\\ . \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_map_4})\\n\\nThe Maximum likelihood estimator is found in the same manner\\n\\n$$\\n\\\\begin{align} \\n\\\\vec{w}_{MLE}=\\\\left(\\\\Phi^{\\\\top} \\\\Phi \\\\right)^{-1} \\\\Phi^{\\\\top} \\\\vec{t} \\\\ .  \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:linear_reg_mle_solution})\\n\\nComparing the above MAP estimator to the MLE one, you can see that\\nformer is a scaled version of the later. That is because MAP here acts\\nas a regularisation technique (i.e., reducing variance in weights\\n$\\\\vec{w}$). Therefore, uninformative prior is equivalent to applying\\nregularisation on the weight coefficients and the higher the $\\\\lambda$\\nthe more regularisation we are applying.\\n\\n$~$\\n\\n$~$\\n### Bayesian Linear Regression\\n$~$\\n\\nBayesian regression describes the full distribution of $\\\\vec{w}$ and\\ndoes not assume the prior\\'s weights to be the same for all observations.\\nLet\\'s assume that the mean $m_0$ and covariance matrix $S_0$ describe the\\nprior normal distribution of model weights, then the posterior\\n\\n$$\\n\\\\begin{align} \\np\\\\left(\\\\vec{w} \\\\mid \\\\vec{t}, \\\\Phi, \\\\beta, m_0, S_0\\\\right) &= \\\\frac{p\\\\left(\\\\vec{w} | m_0, s_0\\\\right) \\\\cdot p(\\\\vec{t} \\\\mid \\\\Phi, \\\\vec{w}, \\\\beta)}{\\\\underbrace{p(\\\\vec{t} \\\\mid \\\\Phi, \\\\beta, m_0, S_0)}_{\\\\int_{\\\\vec{w}} p(\\\\vec{t} | \\\\Phi, \\\\vec{w}, m_0, S_0, \\\\beta) \\\\cdot p(\\\\vec{w}| m_0, S_0)d\\\\vec{w}} } \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_posterior_1})\\n\\n$$\\n\\\\begin{align} \\np\\\\left(\\\\vec{w} \\\\mid \\\\vec{t}, \\\\Phi, \\\\beta, m_0, S_0\\\\right) & \\\\propto  p\\\\left(\\\\vec{w} | \\\\vec{m}_0, s_0\\\\right) \\\\cdot p\\\\left(\\\\vec{t} | \\\\Phi, \\\\vec{w}_0, \\\\beta \\\\right) \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_posterior_2})\\n\\nNow, when we extremise the log posterior, both the prior and the\\nlikelihood are Gaussian\\'s \\n\\n$$\\n\\\\begin{align} \\n\\\\ln (\\\\text{posterior}) &= \\\\ln (\\\\text{normalising const. of the prior} \\\\times \\\\text{exponent of the prior}) + \\\\\\\\ \\n& + \\\\ln(\\\\text{normalising const. of the likelihood} \\\\times \\\\text{exponent of the likelihood}) \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_ln_posterior_form})\\n\\nThen,\\n\\n$$\\n\\\\begin{align} \\n\\\\ln p(\\\\vec{w} \\\\mid D) &= -\\\\frac{1}{2}\\\\left(\\\\sigma^{-2}(\\\\Phi \\\\vec{w} - \\\\vec{t})^T (\\\\Phi \\\\vec{w}-\\\\vec{t})\\\\right) - \\\\frac{1}{2}\\\\left( (\\\\vec{m}_0-\\\\vec{w})^{\\\\top} S_0^{-1} (\\\\vec{m}_0-\\\\vec{w})\\\\right) = \\\\\\\\ \\n& = \\\\frac{1}{2}(\\\\underbrace{\\\\vec{w}^{\\\\top}\\\\left(\\\\sigma^{-2} \\\\Phi^{\\\\top} \\\\Phi+S_0^{-1}\\\\right) \\\\vec{w}}_{(1)}) + \\\\underbrace{\\\\left(\\\\sigma^2 \\\\Phi^{\\\\top} t+S_0^{-1} \\\\vec{m}_0\\\\right)^{\\\\top}}_{(2)} \\\\vec{w}) - \\\\\\\\\\n& - \\\\frac{1}{2}\\\\underbrace{(\\\\sigma^{-2} \\\\vec{t}^{\\\\top} \\\\vec{t} - \\\\vec{m}_0 S_0^{-1} \\\\vec{m}_0)}_{(3)} \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_ln_posterior_1})\\n\\nWe know by conjugacy that multiplying two Guassians (prior $\\\\times$\\nlikelihood) will give us a Gaussian with the new parameters $\\\\vec{m}_N$\\nand $S_N$ \\n\\n$$\\n\\\\begin{align} \\n\\\\ln p(\\\\vec{w} \\\\mid D) &= \\\\ln N\\\\left(\\\\vec{w} | \\\\vec{m}_N, S_N\\\\right) = -\\\\frac{1}{2}\\\\left(\\\\vec{w}-\\\\vec{m}_N\\\\right)^{\\\\top} S_N^{-1}\\\\left(\\\\vec{w}-\\\\vec{m}_N\\\\right) = \\\\\\\\\\n&= -\\\\frac{1}{2} \\\\underbrace{\\\\vec{w}^{\\\\top} S_N^{-1} \\\\vec{w}}_{(1)} + \\\\underbrace{\\\\vec{m}_N^{\\\\top} S_N^{-1} \\\\vec{w}}_{(2)} - \\\\frac{1}{2} \\\\underbrace{\\\\vec{m}_N^{\\\\top} S_N^{-1} \\\\vec{m}_N}_{(3)} \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_ln_posterior_2})\\n\\nWe see that the both sides of the expressions (1)\\n& (2) can be equated to each other and solved separately. The expression\\n(3) can be ignored since it is independent of $\\\\vec{w}$. Thus, \\n\\n$$\\n\\\\begin{align} \\n\\\\vec{m}_N &= S_N \\\\left(S_0^{-1} \\\\vec{m}_0 + \\\\sigma^2 \\\\Phi^{\\\\top} \\\\vec{t} \\\\right)  \\\\\\\\\\nS_N^{-1} &= \\\\sigma^{-2} \\\\Phi^{\\\\top} \\\\Phi + S_0^{-1}\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_solution})\\n\\nIf a uninformative zero-mean and isotropic (i.e., same variance in all\\ndirections) prior is assumed, i.e.,\\n$p(\\\\vec{w} \\\\mid \\\\alpha) = N\\\\left(\\\\vec{w} \\\\mid 0, \\\\alpha^{-1} I\\\\right)$,\\nthen the solution becomes \\n\\n$$\\n\\\\begin{align} \\n& \\\\vec{m}_N=\\\\beta S_N \\\\Phi^{\\\\top} \\\\vec{t} \\\\\\\\ \\n& S_N^{-1}=\\\\alpha I+\\\\beta \\\\Phi^{\\\\top} \\\\Phi\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_uninformative_proir_solution})\\n\\nFinally, the predictive function given by \\n\\n$$\\n\\\\begin{align} \\np(t \\\\mid \\\\vec{t}, \\\\alpha, \\\\beta)=\\\\int_{\\\\vec{w}} p(t \\\\mid \\\\vec{w}, \\\\beta) p(\\\\vec{w} \\\\mid \\\\vec{t}, \\\\alpha, \\\\beta) \\\\mathrm{d} \\\\vec{w} \\\\ , \\\\\\\\\\n\\\\end{align}\\n$$\\n\\nwhich is a joint Gaussian distribution. If we marginalise over $\\\\vec{w}$, the resulting Gaussian distribution is \\n\\n$$\\n\\\\begin{align}\\np(t \\\\mid \\\\vec{x}, \\\\vec{t}, \\\\alpha, \\\\beta)=\\\\mathcal{N}\\\\left(t \\\\mid \\\\vec{m}_N^{\\\\top} \\\\vec{\\\\phi}(\\\\vec{x}), \\\\sigma_N^2(\\\\vec{x})\\\\right) \\\\ . \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_predictive_func})\\n\\nIf we go for the predictive point estimate (mean of the predictive\\ndistribution) for $t$ based on a new observation $\\\\vec{x}$ then\\n\\n$$\\n\\\\begin{align} \\ny(\\\\vec{x}, \\\\vec{w}) &= \\\\sum_j^{M} w_j \\\\phi_j(\\\\vec{x}) = \\\\vec{w}^{\\\\top} \\\\vec{\\\\phi}(\\\\vec{x}) = \\\\vec{m_N}^{\\\\top} \\\\vec{\\\\phi}(\\\\vec{x}) = \\\\beta \\\\vec{\\\\phi}^{\\\\top}(\\\\vec{x}) {S}_N \\\\Phi^{\\\\top} \\\\vec{t} = \\\\\\\\\\n& = \\\\sum_{i=1}^N \\\\underbrace{\\\\beta \\\\vec{\\\\phi}(\\\\vec{x}) S_N \\\\vec{\\\\phi}\\\\left(\\\\vec{x}_i\\\\right)}_\\\\text{weight} \\\\underbrace{t_i}_\\\\text{past observation}\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_predictive_func})\\n\\nWhat we get back is the fact that we can rewrite the result of Bayesian\\nregression as the weighted sum of past observations\\n\\n$$\\n\\\\begin{align} \\ny(\\\\vec{x}, \\\\vec{\\\\omega})=\\\\sum_{i=1}^N k\\\\left(\\\\vec{x}, \\\\vec{x}_i\\\\right) t_i\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:bayes_linear_reg_predictive_func_is_with_kernel})\\n\\nwhere the weight of each data point is determined by the kernel\\n$k\\\\left(\\\\vec{x}, \\\\vec{x}_i\\\\right) = \\\\vec{\\\\phi}(\\\\vec{x}) S_N \\\\vec{\\\\phi}\\\\left(\\\\vec{x}_i\\\\right)$.\\nNote that $\\\\sum_{i=1}^N k\\\\left(\\\\vec{x}_i, x_i\\\\right)=1$ which is not\\nobvious from the first sight. It is however logical if you think about\\nit. If it was not true, one would get inconsistent results, i.e., higher\\nor much lower than (already observed) $t_i$. We can also notice that the\\nkernel function\\'s outputs are proportionate to the covariance matrix $S_N$\\nof the posterior/weights. It indicates that if a new data point\\n$\\\\vec{x}$ is close to the past $\\\\vec{x}_i$ in the basis function space,\\ni.e., dot product $\\\\vec{\\\\phi}(\\\\vec{x})\\\\vec{\\\\phi}\\\\left(\\\\vec{x}_i\\\\right)$\\nscaled by the strength coefficients of linear relationships between the\\nobserved data points $S_N$ (read, covariances), then the observation $t_i$ contributes more\\nto the new prediction $t$.\\n\\n$~$\\n\\n$~$\\n### Equivalent Kernel\\n$~$\\n\\nLet\\'s see if we can rewrite a MAP (Ridge Regression) estimator in terms\\nof kernels. The loss function (opposite of likelihood) becomes\\n\\n$$\\n\\\\begin{align} \\nL(\\\\vec{w}) &= \\\\sum_{i=1}^N\\\\left(t_i-y\\\\left(\\\\vec{x}_i, \\\\vec{w}\\\\right)\\\\right)^2 +\\\\lambda / 2 \\\\vec{w}^{\\\\top} \\\\vec{w} \\\\\\\\\\n&= \\\\|\\\\Phi \\\\vec{w}-\\\\vec{t}\\\\|_2^2+ \\\\frac{\\\\lambda}{2}\\\\vec{w}^{\\\\top} \\\\vec{w} \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:ridge_reg_kernel_0})\\n\\nOr equivalently, \\n\\n$$\\n\\\\begin{align} \\n\\\\vec{w} &= (\\\\Phi^{\\\\top} \\\\Phi - \\\\lambda \\\\mathbb{1})^{-1} \\\\Phi^{\\\\top} \\\\vec{t} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\text{(Primal Form)}  \\\\\\\\\\n\\\\lambda \\\\vec{w} &= \\\\Phi^{\\\\top} (\\\\Phi \\\\vec{w} + \\\\vec{t}) \\\\\\\\\\n\\\\vec{w} &= \\\\Phi^{\\\\top}\\\\left(\\\\frac{1}{\\\\lambda}(\\\\Phi \\\\vec{w} + t)\\\\right) \\\\\\\\\\n\\\\vec{w} &= \\\\Phi^{\\\\top}a  \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\  \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\text{(Dual Form)} \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:ridge_reg_kernel_1})\\n\\nWe know that the dual form exists and we can rewrite the weights as\\n$\\\\Phi^{\\\\top}a$ \\n\\n$$\\n\\\\begin{align} \\nL(\\\\vec{a}) &= (\\\\Phi \\\\Phi \\\\vec{a}-\\\\vec{t})^{\\\\top}(\\\\Phi \\\\Phi \\\\vec{a}-\\\\vec{t}) + \\\\frac{\\\\lambda}{2} \\\\vec{w} \\\\vec{w} \\\\\\\\ \\n&= \\\\frac{1}{2}\\\\left(\\\\vec{a}^{\\\\top} \\\\Phi \\\\Phi^{\\\\top} \\\\Phi \\\\Phi^{\\\\top} \\\\vec{a}\\\\right. - \\\\left.\\\\vec{t}^{\\\\top} \\\\Phi \\\\Phi^{\\\\top} \\\\vec{a}-\\\\Phi \\\\Phi^{\\\\top} \\\\vec{a} \\\\vec{t}-\\\\vec{t}^{\\\\top} \\\\vec{t}\\\\right) +\\\\lambda / 2 \\\\quad \\\\vec{a}^{\\\\top} \\\\Phi \\\\Phi^{\\\\top} \\\\vec{a} \\\\\\\\ \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:ridge_reg_kernel_2})\\n\\n$$\\n\\\\begin{align} \\n\\\\frac{\\\\partial L(\\\\vec{a})}{\\\\partial \\\\bar{a}} = \\\\frac{1}{2}\\\\left(2 \\\\Phi \\\\Phi^{\\\\top} \\\\Phi \\\\Phi^{\\\\top} \\\\vec{a}\\\\right. \\\\left.-2 \\\\Phi^{\\\\top} \\\\Phi \\\\vec{t}\\\\right)+\\\\lambda \\\\Phi \\\\Phi^{\\\\top} \\\\vec{\\\\Phi}\\\\vec{a} &= 0 \\\\\\\\\\n\\\\Phi \\\\Phi^{\\\\top} \\\\vec{a}-\\\\vec{t}+\\\\lambda \\\\vec{a} &= 0 \\\\\\\\\\n\\\\left(\\\\Phi \\\\Phi^{\\\\top}+\\\\lambda I\\\\right) \\\\vec{a} &= \\\\vec{t} \\\\\\\\\\n\\\\vec{a} &= \\\\left( \\\\underbrace{\\\\Phi \\\\Phi^{\\\\top}}_\\\\text{$K$ or Gram matrix} - \\\\lambda I\\\\right)^{-1} \\\\vec{t}\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:ridge_reg_kernel_3})\\n\\nFor inference, \\n\\n$$\\n\\\\begin{align} \\ny(\\\\vec{x})=\\\\vec{w}^{\\\\top} \\\\phi(\\\\vec{x}) = \\\\vec{a}^{\\\\top} \\\\Phi \\\\phi(\\\\vec{x})=\\\\Phi \\\\phi(\\\\vec{x})(K + \\\\lambda \\\\mathbb{1})^{-1} \\\\vec{t} = \\\\Phi \\\\phi(\\\\vec{x})(\\\\Phi \\\\Phi^{\\\\top} + \\\\lambda \\\\mathbb{1})^{-1} \\\\vec{t} \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:ridge_reg_kernel_4})\\n\\nWhy is this dual formulation useful? The Gram matrix $\\\\Phi\\\\Phi^{\\\\top}$\\nis a lot of dot products N (instead of $M$ as for primal form solution\\nwhere it is $\\\\Phi^{\\\\top}\\\\Phi$), plus we need to invert it too. Well,\\nwhen we want to represent our data in a very very high dimensional\\nspace, we can choose $\\\\phi(\\\\cdot)$ for which the dot product\\n$\\\\phi(\\\\vec{x})^{\\\\top}\\\\phi(\\\\vec{x})$ is \"well-defined\" and easy to\\ncalculate formula (that needs to have certain properties, look at the Kernalisation section  \\n[here](https://ml-course.github.io/master/intro.html)). \\nThen, instead of doing all the products and explicitly defining\\n$\\\\vec{\\\\phi}(\\\\vec{x}) \\\\in \\\\mathbb{R}^{100000}$ (i.e., coming up with a formula for each $\\\\phi(\\\\cdot)_i$, \\nand keeping the results of those calculations in-memory) and doing the dot product, \\nwe just do all the calculations according to the \"well-defined\" formula \\nthat is easy to compute (aka the kernel trick). This enables us to easily transition -\\nconveniently compute dot products - between raw observation space\\n$\\\\vec{x}$ and the kernel space, that is implicitly very high\\ndimensional, to model more complex relationships with a lower memory\\nfootprint. Thus, the ideal scenario when kernels would be very useful is when for\\n$\\\\vec{x} \\\\in \\\\mathbb{R}^k$ and $\\\\vec{\\\\phi}(\\\\vec{x}) \\\\in \\\\mathbb{R}^m$,\\n$k<<m$ and the number of observations $N$ is small enough to fit in\\nmemory. Use kernels when you do not want to explicitly calculate feature\\ndesign matrix $\\\\Phi$ and do all the dot products.\\n\\n$~$\\n\\n$~$\\n### Kernel Regression\\n$~$\\n\\nThere could be a pure non-parametric regression defined with kernels.\\nHere, kernels will measure the distance (in the space defined by the\\nkernel) between the new observation $\\\\vec{x}$ and all the past\\nobservations giving higher weights to the ones that are closer. We will\\nnot assume a functional form and rely on how the regression task is (probabilistically) defined: \\n\\n$$\\n\\\\begin{align} \\ny(\\\\vec{x}) = E(t \\\\mid \\\\vec{x}) = \\\\int {t} p (t \\\\mid \\\\vec{x})dt = \\\\frac{\\\\int t p(t,\\\\vec{x})dt}{p(\\\\vec{x})} \\\\ . \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:kernel_reg_0})\\n\\nA simple estimation for $E(t \\\\mid \\\\vec{x})$ could a local area around $\\\\vec{x}$. \\nWe would take all target values of the $\\\\vec{x}$\\'s neighbors and weigh them equally.\\nThus, we take a point($\\\\vec{x}_0$) and make a prediction for it ($t_0$) based on its neighbours\\nin the observed data. Then, we could consider a point $\\\\vec{x}_i$ a neighbor if it\\nis within distance $h$ from $\\\\vec{x}_0$ and if we weigh each neighbour\\nequally \\n\\n$$\\n\\\\begin{align} \\ny\\\\left(\\\\vec{x}_0\\\\right) &= \\\\frac{\\\\sum_{i=1}^N t_i I\\\\left(\\\\left\\\\|\\\\vec{x}_i-\\\\vec{x}_0\\\\right\\\\| \\\\leq h\\\\right)}{\\\\sum_{i=1}^N I\\\\left(\\\\left\\\\|\\\\vec{x}_i-\\\\vec{x}_0\\\\right\\\\| \\\\leq h\\\\right)} \\\\\\\\ \\n&= \\\\sum_{i=1}^N \\\\frac{\\\\left.t I\\\\left(\\\\left|\\\\vec{x}_i-\\\\vec{x}_0\\\\right| \\\\leq h\\\\right)\\\\right)}{\\\\sum_{i=1}^I\\\\left(\\\\left|\\\\vec{x}_i-\\\\vec{x}_0\\\\right| \\\\leq h\\\\right)}\\\\\\\\\\n&= N_{i=1}^N \\\\omega_i\\\\left(\\\\vec{x}_0\\\\right) t_i \\\\ .\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:kernel_reg_1})\\n\\nTo make the the things more complex (and possibly more accurate),\\ninstead of a counter function $I(\\\\cdot)$, we can apply a continues\\nfunction that would measure the distance between a pair of observations\\nin some arbitrary space and produce a weights:\\n\\n$$\\n\\\\begin{align} \\ny\\\\left(\\\\vec{x}_0\\\\right)=\\\\sum_{i=1}^N \\\\frac{t_i K\\\\left(x_{i s} \\\\vec{x}_0\\\\right)}{\\\\sum_{i=1}^N K\\\\left(\\\\vec{x}_i, \\\\vec{x}_0\\\\right)}=\\\\sum_{i=1}^N \\\\omega_i\\\\left(\\\\vec{x}_0\\\\right) t_i\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:kernel_reg_2})\\n\\nIf we want to parameterise a kernel regression then we can use weighted\\nleast squares approximator where $K(\\\\vec{x}_i, \\\\vec{x}_j) = w_{ij}$ and\\nthe model weights are $\\\\beta$ \\n\\n$$\\n\\\\begin{align} \\n\\\\beta = (X^{\\\\top}WX)^{-1}X^{\\\\top}W\\\\vec{t} \\\\ . \\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:kernel_reg_3})\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9dfeb100-1f2b-4801-a897-818d7b892fd6', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/svd/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 32884, 'creation_date': '2025-01-14', 'last_modified_date': '2025-01-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<center>\\n    <h1> Singular Value Decomposition  </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? ü§î\\n$~$\\n\\nThis post gives you a whiff of a tool called Singular Value Decomposition (SVD) that is used\\nin problems related to compression and uncovering hidden structures within your data.\\nThis thin slice of Linear Algebra is ubiquitous in modeling and \\nthe applications of this technique discussed here stretch into a number of domains.\\n\\n$~$\\n\\n$~$\\n## Matrix-Vector Multiplication üìê\\n$~$\\n\\nWe start by giving an overview of different visual interpretations of\\nmatrix vector product. \\n\\n$~$\\n\\n$~$\\n### Transformation\\n$~$\\n\\n[//]: # (::: wrapfigure)\\n[//]: # (R0.5)\\n[//]: # (![image]&#40;pics/svd/matrix_is_vector_transofrmation.png&#41;{width=\"0.9\\\\\\\\linewidth\"})\\n[//]: # (:::)\\n<p align=\"center\" id=\"fig:test_train_error\">\\n    <br>\\n    <strong> Figure 1: Matrix Vector Transformation </strong>\\n    <br>\\n  <img width=\"500\" height=\"350\" src=\"/posts/svd/images/matrix_is_vector_transofrmation.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nFor many people, the most common definition of matrix vector\\nmultiplication (dot product) is a transformation of a vector from one\\nform to another. It is a function that takes in the components of a\\nvector, does product and sum operations on them, and spits out a new\\nvector. The resulting vector is then a rotated and/or stretched version\\nof the original.\\n\\nFor example, 90 degrees rotation matrix is $A = \\\\begin{bmatrix}\\n   0 & -1 \\\\\\\\[3pt]\\n   1 &  0 \\\\\\\\\\n\\\\end{bmatrix}$. If we wanted to position vector $\\\\vec{v}$\\n90anticlockwise, we can multiply it with $-A$ (see\\nFigure 1). Additionally, scaling\\nthe vector while rotating it in space could be done with just applying a\\nconstant factor to the multiplication.\\n\\nSimilarly, instead of a single vector you can transform an (infinite)\\ncollection of vectors. In that case you can represent the same points in\\nspace with different values.\\n\\n$~$\\n\\n$~$\\n###  Change of Basis\\n$~$\\n\\nA matrix in linear algebra can also be viewed as a set of basis vectors\\nthat transforms a vector represented in one basis to another. Commonly,\\n2D vectors are represented with the standard basis\\n$B = (\\\\vec{b}_1, \\\\vec{b}_2)$ where\\n$\\\\vec{b}_1 = \\\\left[\\\\begin{array}{l} 1 \\\\\\\\ 0 \\\\end{array}\\\\right]$,\\n$\\\\vec{b}_2 = \\\\left[\\\\begin{array}{l} 0 \\\\\\\\ 1 \\\\end{array}\\\\right]$. There\\nare situations however where you\\'d want to have a different basis\\n$B^{\\'}= (\\\\vec{b^{\\'}}_1, \\\\vec{b^{\\'}}_2)$ to represent your vectors. For\\nexample, let\\'s imagine that\\n$\\\\vec{b^{\\'}}_1 = 2 \\\\cdot \\\\vec{b}_1  + 1 \\\\cdot \\\\vec{b}_2 = \\\\left[\\\\begin{array}{l} 2 \\\\\\\\ 1 \\\\end{array}\\\\right]$,\\n$\\\\vec{b^{\\'}}_2 = -1 \\\\cdot \\\\vec{b}_1  + 1 \\\\cdot \\\\vec{b}_2 = \\\\left[\\\\begin{array}{l} -1 \\\\\\\\ 1 \\\\end{array}\\\\right]$.\\nThen, you can check that a vector with coordinates\\n$\\\\left[\\\\begin{array}{l} - 4 \\\\\\\\ 1 \\\\end{array}\\\\right]$ in $B$ will have\\nthe same position as vector\\n$\\\\left[\\\\begin{array}{l} - 1 \\\\\\\\ 2 \\\\end{array}\\\\right]$ in $B^{\\'}$. To get\\nto that result, it is sufficient to apply the change of basis matrix on\\nthe vector that you want to know the coordinates of in the standard\\nbasis\\n\\n$$\\n\\\\begin{align}\\n\\\\underbrace{\\\\left[\\\\begin{array}{cc}2 & -1 \\\\\\\\ 1 & 1\\\\end{array}\\\\right]_{B^{\\'} \\\\mapsto B}} _{A} \\\\underbrace{\\\\left[\\\\begin{array}{l} -1 \\\\\\\\ 2 \\\\end{array}\\\\right]}_{\\\\vec{v^{\\'}}} = \\\\underbrace{\\\\left[\\\\begin{array}{c} -4 \\\\\\\\ 1\\\\end{array}\\\\right]_B}_{\\\\vec{v}} .\\n\\\\end{align}\\n$$\\n\\nwhere $A = (\\\\vec{a_1}, \\\\vec{a_2}) = (\\\\vec{b^{\\'}}_1, \\\\vec{b^{\\'}}_2)$ and\\n$a_1 = 2 \\\\cdot \\\\vec{b_1} + 1 \\\\cdot \\\\vec{b_2} =  \\\\left[\\\\begin{array}{l} 2 \\\\\\\\ 1 \\\\end{array}\\\\right]$\\nand\\n$a_2 = -1 \\\\cdot \\\\vec{b_1} + 1 \\\\cdot \\\\vec{b_2} =  \\\\left[\\\\begin{array}{l} -1 \\\\\\\\ 1 \\\\end{array}\\\\right]$\\nare the \\\\\"transformation\\\\\" vectors that map $B^{\\'}$ to $B$. \\nTo verify, you can use the fact that the change of basis matrix is\\nrepresented in units of $B$ and calculate the coordinates of\\n$\\\\vec{v^{\\'}}$:\\n\\n$$\\n\\\\begin{align}\\n\\\\vec{v} &= -1 \\\\vec{b^{\\'}}_1 + 2 \\\\vec{b^{\\'}}_2 \\\\\\\\\\n                   &= -1 \\\\left[\\\\begin{array}{l} 2 \\\\\\\\ 1 \\\\end{array}\\\\right] + 2 \\\\left[\\\\begin{array}{l} -1 \\\\\\\\ 1 \\\\end{array}\\\\right]  = \\\\left[\\\\begin{array}{l} -4 \\\\\\\\ 1 \\\\end{array}\\\\right] .\\n\\\\end{align}\\n$$\\n\\nTherefore, by applying a matrix you transition to a new coordinate\\nsystem (e.g., if the change of basis matrix is described in standard units then we move into the standard basis). \\nThis is extremely useful if you are given a problem (e.g., 90 degrees rotation) and your basis is\\ndescribed in units that are hard to work with (e.g., a shape is\\ndescribed in weird transformed coordinates system). Then, one can change\\nbasis, solve the problem (e.g., apply a known 90 degrees rotation matrix\\non the shape) and change the basis back to get the final vector.\\n\\n$~$\\n\\nAssume, we are given $\\\\vec{v}$ and we want to find a $\\\\vec{v}`$ that is\\na 90 degrees rotation of $\\\\vec{v}$. We can first apply the change of\\nbasis matrix that would produce a vector in our standard basis\\n\\n$$\\n\\\\begin{align}\\nC  \\\\vec{v} \\\\ .\\n\\\\end{align}\\n$$\\n\\nNow, we can work with our standard coordinates and apply the desired 90\\ndegree rotation transformation matrix\\n\\n$$\\n\\\\begin{align}\\nA C  \\\\vec{v} \\\\ .\\n\\\\end{align}\\n$$\\n\\nFinally, we revert back to the initial coordinates system described by\\napplying the inverted change of basis matrix\\n\\n$$\\n\\\\begin{align}\\n C^{-1}  A  C  \\\\vec{v} \\\\ .\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n### Projection \\n$~$\\n\\n[//]: # (::: wrapfigure)\\n[//]: # (R0.5 ![image]&#40;pics/svd/projection.png&#41;{width=\"0.9\\\\\\\\linewidth\"})\\n[//]: # (:::)\\n<p align=\"center\" id=\"fig:projection-line\">\\n    <br>\\n    <strong> Figure 2: Matrix-Vector Multiplication as Projection </strong>\\n    <br>\\n   <img width=\"400\" height=\"300\" src=\"/posts/svd/images/projection.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nLast but not least, a matrix vector product can be connected to the dot\\nproduct operation and be seen as a projection onto a line defined in the\\nsame space. In turn, we can say that the projection is multiplication of a unit\\nvector on the line scaled by some factor. Figure 2 depicts $c\\\\cdot \\\\vec{v}$;\\nthe projection vector that appears on the line as a result of drawing a distance vector that connects \\n$\\\\vec{x}$ and the line orthogonally. Therefore,\\n\\n$$\\n\\\\begin{align}\\n\\\\vec{v}\\\\left( \\\\vec{x} - c\\\\vec{v}\\\\right) = 0 \\\\\\\\\\nc = \\\\frac{\\\\vec{x} \\\\vec{v}}{\\\\vec{v} \\\\vec{v}} \\\\implies \\\\\\\\\\n\\\\operatorname{Proj}_L(\\\\vec{x})= c\\\\cdot \\\\vec{v} =\\\\frac{\\\\vec{x} \\\\vec{v}}{\\\\vec{v} \\\\vec{v}}\\\\vec{v}  \\\\ .\\n\\\\end{align}\\n$$\\n\\nIf $\\\\vec{v}$ is a unit vector, i.e., $\\\\vec{v}\\\\vec{v} = ||\\\\vec{v}|| = 1$,\\n\\n$$\\n\\\\begin{align}\\n\\\\operatorname{Proj}_L(\\\\vec{x})= \\\\left( \\\\vec{x} \\\\vec{v} \\\\right) \\\\vec{v}  \\\\ .\\n\\\\end{align}\\n$$\\n\\nThen we can rewrite the vector product as a matrix-vector product \\n\\n$$\\n\\\\begin{align}\\n\\\\left( \\\\vec{x} \\\\vec{v} \\\\right) \\\\vec{v}  \\\\equiv A\\\\vec{x} \\\\ \\n\\\\end{align}\\n$$\\n\\nwhere $A = \\\\left[\\\\vec{b_1} \\\\  \\\\vec{b_2}\\\\right]$ and\\n\\n$$\\n\\\\begin{align}\\n\\\\vec{b_1} =  \\\\left(\\\\left[\\\\begin{array}{l} 1 \\\\\\\\ 0 \\\\end{array}\\\\right] \\\\cdot \\\\vec{v} \\\\right) \\\\vec{v} \\\\\\\\ \\n\\\\vec{b_2} =  \\\\left(\\\\left[\\\\begin{array}{l} 0 \\\\\\\\ 1 \\\\end{array}\\\\right] \\\\cdot \\\\vec{v} \\\\right) \\\\vec{v}\\n\\\\end{align}\\n$$\\n\\n[//]: # (::: wrapfigure)\\n[//]: # (R0.5 ![image]&#40;pics/svd/projection_C_A.png&#41;{width=\"0.9\\\\\\\\linewidth\"})\\n[//]: # (:::)\\n<p align=\"center\" id=\"fig:projection-plane\">\\n    <br>\\n    <strong> Figure 3: Matrix-Vector Multiplication as Projection </strong> \\n    <br>\\n  <img width=\"400\" height=\"300\" src=\"/posts/svd/images/projection_C_A.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nOh gods, but what if we have $A\\\\vec{x} = \\\\vec{b}$ where\\n$A \\\\in \\\\mathit{R}^{n \\\\times k}$, $A \\\\in \\\\mathit{R}^{k}$ and\\n$b \\\\in \\\\mathit{R}^{n}$. This means we have a so called\\n\\\\\"over-determined\\\\\" system where $k$ = rank of $A < n$. Hence, the\\ndimensionality of the vector $dim(\\\\vec{x})$ and the dimensionality of\\nits projection $dim(\\\\vec{b})$ are different. Since $\\\\vec{b}$ is not in\\nthe columns space of $A$ ($C(A)$), we will try to find $\\\\vec{x}^{*}$\\nsuch that (Euclidean) distance between $\\\\vec{b}$ and $A\\\\vec{x}^{*}$ is\\nminimised. See Figure 3 for another visual.\\n\\nFormally,\\n\\n$$\\n\\\\begin{align}\\nmin \\\\  ||\\\\vec{b} - A\\\\vec{x}^{*} ||\\n\\\\end{align}\\n$$\\n\\nor,\\n\\n$$\\n\\\\begin{align}\\n\\\\left|\\\\left|\\\\begin{array}{l}b_1-v_1 \\\\\\\\ b_2-v_2 \\\\\\\\ ... \\\\\\\\ b_n-v_n\\\\end{array}\\\\right|\\\\right|=\\\\sqrt{\\\\left(b_1-v_1\\\\right)^2+\\\\left(b_2+v_2\\\\right)^2+\\\\ldots+\\\\left(b_n-v_A\\\\right)^2}\\n\\\\end{align}\\n$$\\n\\nWell, the closest vector in $C(A)$ to $\\\\vec{b}$ will be\\n$\\\\operatorname{Proj}_{C(A)}(\\\\vec{b})$\\n\\n$$\\n\\\\begin{align}\\nA \\\\vec{x}^*=\\\\operatorname{Proj_{C(A)}}(\\\\vec{b}) \\\\ .\\n\\\\end{align}\\n$$\\n\\nThen,\\n\\n$$\\n\\\\begin{align}\\nA \\\\vec{x}^*-\\\\vec{b} = \\\\operatorname{Proj}_{C(A)}(\\\\vec{b})-\\\\vec{b} \\\\ .\\n\\\\end{align}\\n$$\\n\\nAnd by definition of orthogonal projections\\n\\n$$\\n\\\\begin{align}\\nA \\\\vec{x}^{*}- \\\\vec{b} \\\\perp C(A) \\\\ .\\n% this is because Ax* and b create a 90 degree angle since Ax* is the projection of b\\n\\\\end{align}\\n$$\\n\\nOr, in other words, $A \\\\vec{x}^{*}- \\\\vec{b} \\\\in C(A)^{\\\\perp}$. We also\\nknow that $C(A)^{\\\\perp} = N(A^{T})$, i.e., orthogonal component of $A$\\nis in the null space of $A^{T}$. Hence,\\n$A \\\\vec{x}^{*}- \\\\vec{b} \\\\in N(A^{T})$, or\\n\\n$$\\n\\\\begin{align}\\n\\\\begin{array}{l}A^{\\\\top}\\\\left(A \\\\vec{x}^*-\\\\vec{b}\\\\right)=\\\\overrightarrow{0} \\\\\\\\ A A \\\\vec{x}^*-A \\\\vec{b}=\\\\overrightarrow{0} \\\\\\\\ A^{\\\\top} A \\\\vec{x}^*=A^{T} \\\\vec{b} \\\\\\\\ \\\\vec{x}^*=\\\\left(A^{\\\\top} A\\\\right)^{-1} A^{\\\\top} \\\\vec{b} \\\\ .\\n\\\\end{array}\\n\\\\end{align}\\n$$\\n\\nThis is also what we call the least-square approximation of a regression problem.\\n\\n$~$\\n\\n$~$\\n## Eigenvectors üìè\\n$~$\\n\\nAs we saw, a matrix multiplication is a transformation operation. Well,\\nthere are some vectors that keep staying on the the same line when the transformation is applied on\\nthem. Those vectors are called eigenvectors. While they keep being on\\nthe same line, they often get scaled this means they might change their\\ndirection and/or magnitude. Mathematically, a single eigenvector could be represented as follows\\n\\n$$\\n% \\\\label{eq:eigen_vectors_values}\\n\\\\begin{align}\\n\\\\begin{array}{l}\\nA \\\\vec{v}=\\\\lambda \\\\vec{v}  \\\\\\\\\\nA \\\\vec{v}=\\\\lambda \\\\vec{v} \\\\\\\\ A \\\\vec{v}=\\\\lambda I \\\\vec{v} \\\\\\\\\\n(A-\\\\lambda I) \\\\vec{v}=0 \\\\\\\\\\n\\\\{ \\\\text{by the property of the determinant} \\\\} \\\\\\\\\\n\\\\operatorname{det}(A-\\\\lambda I)=0\\n\\\\end{array}\\n\\\\end{align}\\n$$\\n\\nWe can rewrite this equation for all eigenvectors of $A$ as\\n\\n$$\\n\\\\begin{align}\\nA \\\\vec{v} & =\\\\lambda \\\\vec{v} \\\\\\\\\\nA {Q} & = {Q} {\\\\Lambda} \\\\\\\\\\nA &= {Q} {\\\\lambda} {Q}^{-1}\\n\\\\end{align}\\n$$\\n\\nto produce the eigendecomposition of matrix $A$. Interesting fact that the eigenvectors of symmetric matrices\\nare orthogonal to each other and span the entire vector space of $A$.\\n\\n$~$\\n\\n$~$\\n## Singular Value Decomposition ü¶†üîÑ\\n$~$\\n\\nThis technique can be used to reconstruct a data sample with fewer\\nfeatures and/or inspect patterns within it. All of this becomes possible\\nthrough finding a set of orthogonal vectors that encode our original\\nfeatures into a (lower) dimensionality where linear relationships\\nbetween features are assumed. That makes our data look linearly\\ndecorrelated while also capturing the maximum variation.\\n\\n$~$\\n\\n$~$\\n### Why decorrelate & capture variance? \\n$~$\\n\\nRemember all familiar formulas for variance and covariance\\n\\n$$\\n\\\\begin{align}\\n{Var}(X_{:,j})&=\\\\frac{1}{N} \\\\sum_i\\\\left(X_{i j} - \\\\bar{x}_j\\\\right)^2 \\\\\\\\\\n{Cov}(X_{:,j}, X_{:,z}) &= \\\\frac{1}{N} \\\\sum_i\\\\left(X_{i j} - \\\\bar{x}_j\\\\right)\\\\left(X_{i z} - \\\\bar{x}_z\\\\right) \\\\ .\\n\\\\end{align}\\n$$\\n\\nIn a nutshell, variance tells us how much variation there is in the\\ndistribution of values for a single feature. A higher variance means\\nthat the data points are more spread out across the feature\\'s domain and\\nthere is diversity in the data that we might want to account\\nfor/capture. Similarly, covariance describes the degree of one feature\\nvarying linearly with respect to the other. A higher covariance signals\\nthat the two features might be linearly dependent, and informs about\\npossible patterns within the data. Let us find the intuition behind the\\nreasons to decorrelate data. The assumption here is that if the data\\nshows that there are two linearly dependent features, we might want to create\\na space where it is no longer the case, i.e., a single latent variable\\nwould capture this linear relationship and encode the dependent features.\\n\\n[//]: # (![Data points collected for three features. There seems to be a linear)\\n[//]: # (relationship between features $d_1$ and)\\n[//]: # ($d_3$.]&#40;pics/svd/svd_intro_change_of_basis_a.png&#41;{#fig:svd_intro_change_of_basis_a)\\n[//]: # (width=\"90%\" height=\"0.32\\\\\\\\textheight\"})\\n[//]: # ()\\n[//]: # (![We can encode each data point with two features instead of three. The)\\n[//]: # (original features could be easily reconstructed since parameters $a$ and)\\n[//]: # ($b$ are)\\n[//]: # ([//]: # &#40;learnt.]&#40;pics/svd/svd_intro_change_of_basis_b.png&#41;{#fig:svd_intro_change_of_basis_b&#41;width=\"90%\"})\\n<p align=\"center\" id=\"fig:svd_intro_change_of_basis\">\\n    <br>\\n    <strong> Figure 4: Encoding features </strong>\\n    <div style=\"display: flex; justify-content: center;\">\\n        <img src=\"/posts/svd/images/svd_intro_change_of_basis_a.png\" alt=\"Image 1\" style=\"width: 30%; margin-right: 10px;\">\\n        <img src=\"/posts/svd/images/svd_intro_change_of_basis_b.png\" alt=\"Image 2\" style=\"width: 30%; margin-left: 10px;\">\\n        <br>\\n        <br>\\n    </div>\\n    <br>\\n</p>\\n\\n\\nImagine that we have collected a data sample with three features. Within that sample,\\na pair of features $d_1$ and $d_3$ have a perfect (negative)\\nlinear relationship with each other and, therefore, \"strong\\\\\"\\ncovariance. Further, imagine that there is a non-zero \"weak\\\\\" covariance\\nbetween $d_2$ and $d_3$. Besides, this weak covariance $d_2$ and $d_3$\\ncould be merely a result of random noise effecting the data collection\\nprocess. Figure 4-Left tries miserably to display such scenario.\\n\\nLet\\'s focus on $d_1$ and $d_3$ because their variation\\ncan be captured perfectly. Using a modeling technique, we can parameterise the relationship between\\n$d_1$ and $d_3$ and reconstruct them in a single latent variable $d_0$.\\nGraphically, it means that $d_1$ and $d_3$ are squashed into a single\\nline and the feature space is transformed, see Figure 4-Right. \\nSince the old correlated features no longer exist, the covariance between the new features on the\\n2D plane is zero. Our data becomes decorrelated. Finally, the two\\nfeatures are encoded into one so we no longer need to store three values\\nper data point. We can only store two values while remembering the\\ncoefficients of the linear model. We have achieved lossless data\\ncompression as we can perfectly reconstruct the original data with fewer\\nvalues!\\n\\n$~$\\n\\nIn the real world, perfect linear relationships between features are\\nextremely rare. We can however still capture linear relationship within\\ndata and decorrelate it. The only difference is that we are not able to\\n(perfectly) encode the data with fewer values. This is because each\\nlinear relationship modeled will have a random error component of\\ndifferent magnitude. In general, the stronger the linear relationship\\nthe more variation in the data is captured by it, and the less the error\\nis. That is why when encoding features, if we keep latent variables with\\nthe strongest linear dependence (and low reconstruction error), \\nwe can be able to compress the data and only lose minimal amount of information. \\nAs already mentioned, a simple estimate of  the strength of a linear relationship is (co)-variance.\\nHence, maximising the (co)-variance we are trying to ensure that we are\\nefficiently encoding more of our data.\\n\\n$~$\\n\\nMost often, all $M$ features that would be in the data set have\\nnon-zero covariances. The idea behind Singular Value Decomposition (SVD)\\nis that instead of creating just one latent variable, we create $M$ of\\nthem. Those variables will capture linear relationships\\nand de-correlate the data which will enable us to inspect the patterns occurring in a new space.\\nIn the next section, we see what we can actually do to\\ncreate this latent space. There are multiple ways to look at the\\nprocedure of de-correlating data, e.g., linear problem for\\nreconstructing original points, projection operation that maximises\\nvariance, but we take an approach where it is presented as a change of\\nbasis operation previously discussed.\\n\\n$~$\\n\\n$~$\\n### Changing basis with the Covariance Matrix \\n$~$\\n\\nOne can think of a set of data points as representing a matrix of some\\nsort ($X$). The data points are scattered around the feature space and\\none can find the average vector over all the points ($\\\\vec{m}$). Thus,\\nin order to arrive to any data point vector $\\\\vec{x_i}$ we would need to add\\nup the average vector and the difference vector\\n$\\\\vec{s_i} = \\\\vec{x_i} - \\\\vec{m}$. The difference vector then can be\\nexpressed with a new basis represented by a change of basis matrix $A$.\\nThe new basis assumes that the data points are represented with some\\n(latent) vector $\\\\vec{z_i}$ where the vector addition\\n$\\\\vec{x_i} = \\\\vec{s_i} + \\\\vec{m} = A\\\\vec{z_i} + \\\\vec{m}$ holds. \\nWithin the new basis, all the data points are decorrelated,\\n$Cov(X) = \\\\mathit{1}$, i.e., $z_i$\\'s constitute a white\\nnoise data. However, once multiplied by the change of basis matrix $A$ \\nthey actually show the behaviour that our original data had,\\ni.e., they show how the white noise got spread out in our multidimensional space.\\nThus, the columns of $A$ will be some sort of combination of basis vectors and tell us in\\nwhich directions the data gets stretched.\\n\\n[//]: # (![Each data point\\'s vector $\\\\vec{x_i}$ can be represented as the sun of)\\n[//]: # (the average vector $\\\\vec{m}$ and the difference vector)\\n[//]: # ($\\\\vec{s_i}$.]&#40;pics/svd/svd_change_of_basis_a.png&#41;{#fig:first)\\n[//]: # (width=\"\\\\\\\\textwidth\"})\\n[//]: # ()\\n[//]: # (![The difference vector can be represented in a new basis $\\\\vec{z_i}$)\\n[//]: # (which is a scaled and translated version of)\\n[//]: # ($\\\\vec{s_i}$.]&#40;pics/svd/svd_change_of_basis_b.png&#41;{#fig:third)\\n[//]: # (width=\"\\\\\\\\textwidth\"})\\n<p align=\"center\" id=\"fig:svd_intro_change_of_basis\">\\n    <br>\\n    <strong> Figure 5: Change of basis </strong>\\n    <div style=\"display: flex; justify-content: center;\">\\n        <img src=\"/posts/svd/images/svd_change_of_basis_a.png\" alt=\"Image 1\" style=\"width: 30%; margin-right: 10px;\">\\n        <img src=\"/posts/svd/images/svd_change_of_basis_b.png\" alt=\"Image 2\" style=\"width: 30%; margin-left: 10px;\">\\n        <br>\\n        <br>\\n    </div>\\n    <br>\\n</p>\\n\\n*Now, I will closely follow Peter Bloem\\'s tutorials/book. I read quite a\\nfew resources that try to explain PCA/SVD in an intuitive manner. None\\nof them connected with me as much as the explanations that Peter gave.*\\n\\n$~$\\n\\nLet\\'s generalise the results from a single vector $\\\\vec{x_i}$ to the\\nwhole dataset. Our data point vectors become columns in $X^T$ while\\n$\\\\vec{z_i}$\\'s become columns in $Z^T$\\n\\n$$\\n\\\\begin{align}\\n% X^T - \\\\left[\\\\begin{array}{l} \\\\vec{m} \\\\\\\\ \\\\vdots \\\\\\\\\\\\vec{m} \\\\end{array}\\\\right]  &= AZ^T \\\\\\\\\\nX^T - \\\\vec{m}  &= AZ^T &\\\\text{(simplified the notation)} \\n% \\\\label{eq:svd_normalised_data}\\n\\\\end{align}\\n$$\\n\\nRemember that we assumed that our $z_i$\\'s are distributed with\\n$Cov(Z)=\\\\mathit{1}$. Then, look at the covariance of the left-hand side\\n$X - \\\\vec{m}$ and find an interesting relationship. Note that the\\n${Cov}(X - \\\\vec{m}) = {Cov}(X)$ since $\\\\vec{m}$ is just like subtracting\\na constant,\\n\\n$$\\n\\\\begin{align}\\n{Cov}(X)=\\\\frac{1}{n} A {Z}^{\\\\top}\\\\left(A {Z}^{\\\\top}\\\\right)^{\\\\top}=\\\\frac{1}{n} A {Z}^{\\\\top} {Z} {A}^{\\\\top}=A {Cov}({Z}) A^{\\\\top}=A \\\\mathit{1} A^{\\\\top}=AA^{\\\\top} = S \\\\ .\\n% \\\\label{eq:svd_cov_x}\\n\\\\end{align}\\n$$\\n\\nThe result is mind-blowing, right? We have the covariance matrix popping\\nup in the transformation of white noise! What?! Who would have expected\\nthat?! Moreover, we once again see that the covariance matrix is\\nsymmetric, and by the spectral theorem - [the\\nproof](#https://peterbloem.nl/blog/pca-3) of which is not the easiest to\\ndo, and takes quite a bit of time to go through - we know that it can be\\ndecomposed with orthogonal basis consisting of eigenvectors of $S$ (and\\nreal eigenvalues).\\n\\n$$\\n\\\\begin{align}\\n S = AA^{\\\\top} = PDP^{\\\\top} \\\\ .\\n\\\\end{align}\\n$$\\n\\nWhere $P$ is an orthogonal matrix ($P^{-1} = P^{T}$), i.e., square\\nmatrices in which all columns are mutually orthogonal unit vectors. This\\nis so neat because we can easily find $A$\\n\\n$$\\n\\\\begin{align}\\nA A^{\\\\top} &= PDP^{\\\\top} = {P D}^{\\\\frac{1}{2}} {D}^{\\\\frac{1}{2}^{\\\\top}} {P}^{\\\\top} \\\\\\\\\\n& \\\\text { then } {A}=P{D}^{\\\\frac{1}{2}} \\\\text{ and } {A}^{-1}={D}^{-\\\\frac{1}{2}}P^{\\\\top}\\n\\\\end{align}\\n$$\\n\\nAs you can see the behaviour of your data can be described by the\\neigenvalues and eigenvectors of your covariance matrix. Thus,\\ntransforming one\\'s data from white noise to something that actually has\\nvariance and correlations/patterns along all the dimension, one needs to\\nscale white noise data by ${D}^{\\\\frac{1}{2}}$, transform by $P$ and add\\n$\\\\vec{m}$, i.e.,\\n$A\\\\vec{z_i} + \\\\vec{m} = P{D}^{\\\\frac{1}{2}}\\\\vec{z_i} + \\\\vec{m} \\\\ \\\\forall i$.\\n\\nAlternatively, you can also say that if you take the standard basis,\\ndivide it by the square root of your covariance matrix\\'s eigenvalues and\\ntransform by the eigenvectors, you will arrive to the basis that makes\\nyour data\\'s covariance be a diagonal matrix, i.e., your data becomes\\nwhite noise. Intuitively, this also means that your standard basis,\\nwhere all your dimensions are a unit vector, gets reshaped. Then, some\\ndimensions are more squeezed than others to accommodate for the\\ndata variance and make the data along those dimensions white noise!\\nThus, $P$ and ${D}^{\\\\frac{1}{2}}$ give you all the info about your\\ndata\\'s variance and how to describe it with just linear transformations.\\n\\n$~$\\n\\n$~$\\n### Finding Maximum Variance Directions \\n$~$\\n\\nWe just saw how to transform (stretch and squeeze) our space to make the\\ndata be decorrelated and back. Encoding our features into a set of\\nlatent variables is therefore possible. However, as already briefly\\nmentioned in the Section about the reasons to decorrelate your data, we need to make sure\\nthat the linear relationships being created as a result of changing the basis are actually \"strong\\\\\"\\nand not just random deviations in the data. Thus, we are trying to\\nfigure out how to capture (co)-variance efficiently so that if we were\\nto reduce the number of dimensions in the data we would not be getting rid of too much\\ninformation.\\n\\n$~$\\n\\nHere, the concepts of maximum (co)-variance and eigenvectors/values of\\nthe covariance matrix need to be connected together. Previously, we said\\nthat we want to encode the \"strongest\\\\\" relationships between features\\nand that the (co)-variance can serve as a gauge for this strength.\\nTherefore, if we can find the directions of maximum variance, we can\\nproject the data on them, i.e., model the \"stronger\\\\\" linear\\nrelationships between different features.\\n\\n$~$\\n\\nWe can look at the problem of finding the maximum variance from a different\\npoint of view. How can we extrapolate our data further away from the\\norigin - we assume that the data is already normalised - based on the\\nexisting linear relationships within the sample? The covariance matrix\\nalready describes our data/features in terms of their (co)-variance, so we\\ncan thus check the \"(co)-variance score\" of each direction by (i.e., the quadratic form).\\n\\n$$\\n\\\\begin{align}\\n\\\\nu(\\\\vec{w}) = \\\\vec{w}^{\\\\top} S \\\\vec{w}\\n\\\\end{align}\\n$$\\n\\nThis score is a polynomial of the second degree and linear in its\\nparameters ($S$) that means that if $\\\\nu(\\\\vec{w_i}) > \\\\nu(\\\\vec{w_j})$\\nthen $\\\\nu(c  \\\\vec{w_i}) > \\\\nu(c  \\\\vec{w_j})$ where $c$ is some constant.\\nInformally, if we were to generate data points as multiples of some\\n$\\\\vec{w_{max}}$ they would spread our data from the origin the most,\\ni.e., maximum variability. The direction of maximum variance in the data\\nis nothing more than a combination of feature values (a data point\\nvector $\\\\vec{w_{max}}$) where dimensions with higher data variances and\\ncovariances are given higher values relative to the rest of features\\nwith lower (co)-variances. If take a 2D example\\n\\n$$\\n\\\\begin{align}\\n\\\\vec{w}^{\\\\top} S \\\\vec{w} = \\\\vec{w}^{\\\\top}\\\\left(\\\\begin{array}{ll}\\n0.5 & 2 \\\\\\\\\\n2 & 1.1\\n\\\\end{array}\\\\right) \\\\vec{w} =  0.5 w_1{ }^2 + 2 w_2 w_1+ 2 w_1 w_2 + 1.1 w_2{ }^2 \\\\ .\\n\\\\end{align}\\n$$\\n$\\\\vec{w_{max}}$ would be the proportion $w_1 / w_2$ \\nthat gives the highest result to $\\\\vec{w}^{\\\\top} S \\\\vec{w}$\\ngiven that $||\\\\vec{w}||=c$ where $c$ could be in arbitrary scalar.\\n\\n$~$\\n\\nIn other words, if we only had one unit of value to redistribute across\\nall the features how much would we weigh each one so that the resulting\\ncombination would produce the highest overall (co)-variance score?\\nThe problem becomes \\n\\n$$\\n\\\\begin{align}\\n{\\\\underset{w}{\\\\operatorname{argmax}}} \\\\ & \\\\vec{w}^{\\\\top} S \\\\vec{w} \\\\\\\\\\n\\\\text{s.t. } & \\\\vec{w}^{\\\\top}\\\\vec{w} = 1   \\\\ .\\n% \\\\label{eq:svd_max_varaince_quadratic_form}\\n\\\\end{align}\\n$$\\n\\nIf we are to assign one unit of value\\n$||\\\\vec{w}||=1$ between $w_1$ and $w_2$, one might be tempted to just\\nhave $w_1=0$ and $w_2=1$ (because the variance for the second dimension is the highest).\\nbut we would lose out on the variance from the interactions then. \\nThat is why the maximum variance is reached if the\\nunit variance distributed in this proportion $w_1/w_2= 0.375/0.625$, or\\nin other words $\\\\vec{w} = [0.375, 0.625]$.\\n\\n$~$\\n\\nThe grand finale.. Rewrite the max variance \\nequation with the knowledge about\\nthe covariance matrix (of spectral theorem) that we acquired in the\\nprevious section \\n$$\\n\\\\begin{align}\\n{\\\\underset{w}{\\\\operatorname{argmax}}} \\\\ & \\\\vec{w}^{\\\\top} PDP^{\\\\top} \\\\vec{w} \\\\\\\\\\n\\\\text{s.t. } & \\\\vec{w}^{\\\\top}\\\\vec{w} = 1   \\\\ .\\n% \\\\label{eq:svd_max_varaince_eigen_quadratic_form}\\n\\\\end{align}\\n$$\\n\\nNow, if nothing is blowing your mind still, let me highlight two things\\nhere: $P$ is orthogonal and the column vectors $\\\\vec{P_{:,i}}$, i.e.,\\neigenvectors of $S$, are orthonormal vectors while the matrix $D$ is\\ndiagonal and all values but the diagonal ones being zero. Thus, the sum\\n$\\\\vec{w}^{\\\\top}PDP^{\\\\top}\\\\vec{w}$ is maximised when (a) the inner\\nproduct between $\\\\vec{w}$ and $\\\\vec{P_{:,i}}$ is maximised and (b) then\\nmultiplied by the maximum value from the diagonal of $D$. However, the\\ninner product is maximised when $\\\\vec{w}$ and an eigenvector\\n$\\\\vec{P_{:,i}}$ are pointing in the same direction! The inner product\\nwill actually be equal to 1 since $\\\\vec{P_{:,i}}$ and $\\\\vec{w}$ are both unit vectors.\\nHence, the first maximum variance direction $\\\\vec{w}_{max}$ can be\\ndefined by $\\\\vec{P_{:,i}}$ where $D_{i,i}>D_{j,j} \\\\forall j$, i.e., the\\neigenvector of $S$ with the highest eigenvalue. What about the second\\nmaximum direction which is also orthogonal to the first one? Using the\\nsame logic/intuition and the fact that there can only be $M$ ($Rank(S)$)\\northogonal vectors in total, the second maximum direction can then be\\nthe eigenvector with the second largest eigenvalue.\\n\\n$~$\\n\\n$~$\\n### Singular Vectors & Values\\n$~$\\n\\nIn the last two sections, we got a high level understanding of two ideas.\\nFirst one, how we can de-correlate our data and transfer it to a different\\nlatent space, i.e., different vector basis. Second, how eigenvectors of\\nthe data covariance matrix are the directions of maximum variance, i.e.,\\n$\\\\vec{P}_{:,i} = \\\\vec{w}_{max}$.\\n\\n$~$\\n\\nWe can now finally intro the term *right singular vector* of $X$. That is a\\nvector $\\\\vec{v}$ which is an eigenvector of the covariance matrix\\n$S = X^{\\\\top}X$. Right singular vectors of a matrix show the directions\\nof (maximum) variance in the data, see\\nSection *Finding Maximum Variance Directions*. Those directions also\\ncorrespond to the stretch direction that indicate where our data is\\nspread more from its white noise form if converted back to the\\n(original) standard basis, see\\nSection *Change of Basis*. Therefore, right singular vectors\\nserve our original purpose of encoding linear relationships within the\\ndata and capturing maximum (co)-variance while doing so.\\n\\n$~$\\n\\nMoreover, we find exactly how much variance is captured with each\\nsingular vector. In order to do this, one needs to see what value each of\\nour data points will take if we encode them in the new latent space with\\nthe vector $\\\\vec{v}$. The equivalent operation for this is\\n$\\\\vec{y} = X \\\\vec{v}$ or projecting (encoding) our data onto $\\\\vec{v}$,\\ni.e., this is exactly what we implicitly did to get $d_0$ in\\nSection *Why decorrelate & capture variance?*. Since throughout last\\nparts of this article we assumed that our data $X$ is mean-centered, the\\nvariance is just a multiplication of each of the (latent) variable\\nvalues with itself \\n\\n$$\\n\\\\begin{align}\\n||\\\\vec{y}||^2 =  \\\\|X \\\\vec{v}\\\\|^2=\\\\vec{v}^{\\\\top} X^{\\\\top} X \\\\vec{v} = \\\\vec{v}^{\\\\top} D_{i,i} \\\\vec{v} = D_{i,i} \\\\vec{v}^{\\\\top} \\\\vec{v}= D_{i,i} \\\\\\\\ \\n% \\\\label{eq:svd_singular_values_are_eigenvalues_of_S}\\n\\\\end{align}\\n$$ \\nwhere $D_{i,i}$ is the eigenvalue of\\n$\\\\vec{P}_{:,i} = \\\\vec{w}_{max} = \\\\vec{v}$. This value also the value\\nindicating how much variance we are capturing with the latent variable\\ndefined by $\\\\vec{v}$.\\n\\n$~$\\n\\nAs already mentioned, $\\\\vec{y}$ can be seen as a result of transferring\\nour data into a latent space defined by $\\\\vec{v}$. If we are to do this\\nfor all singular vectors $\\\\vec{v}$, we create a new space that would\\ndecorrelate our data. This is very similar to what we did in\\nthe section about Change of Basis, when we were changing our bases.\\nIn the new space, we would have our data decorrelated and some latent\\nvariables would have higher variance than others. We can then also\\ndefine the direction of variance $\\\\vec{u}$ for a latent variable that\\ncomes as a result of projecting $X$ on a vector $\\\\vec{v}$. In other\\nwords, $X \\\\vec{v} = y = \\\\sigma \\\\vec{u}$ where $\\\\sigma$ is indicating the\\ndegree of variance for that latent variable, or how much unit vector\\n$\\\\vec{u}$ gets stretched. It is easy to find out this amount of stretch\\n\\n$$\\n\\\\begin{align}\\n\\\\sigma = \\\\frac{||\\\\vec{y}||}{||\\\\vec{u}||} = \\\\frac{\\\\sqrt{(y_{1}^2 + y_{2}^2 + ... + y_{N}^2)}}{1} = \\\\sqrt{Var(\\\\vec{y})} = \\\\sqrt{D_{i,i}}\\n% \\\\label{eq:svd_singular_values_are_eigenvalues_of_S}\\n\\\\end{align}\\n$$ \\n\\nWe call $\\\\sigma$ a *singular value* of $X$. Finally,\\n$\\\\vec{u}$ is called the *right singular vector* of $X$.\\n\\n$~$\\n\\n$~$\\n### Decomposition\\n$~$\\n\\nLet\\'s iterate again why we are doing what we are doing: finding the directions\\nof maximum variance and going through a fair share of linear algebra\\n(really quickly) to do it. Meanwhile, our goal is to represent our \\ndata points with new set of latent variables that encode linear\\nrelationships between features and decorrelate them. In the past\\nsections, we saw that we are killing two birds with one stone if we are\\nlooking for a set of orthogonal vectors that are also maximum variance directions.\\nWe already saw that projecting our data onto a direction of maximum\\nvariance ($\\\\vec{w}_{max} = \\\\vec{v}$) gives the values of the latent\\nvariable produced by this projection. \\n\\n$$\\n\\\\begin{align}\\nX \\\\vec{v} =  \\\\vec{u} \\\\sigma  \\\\ . \\\\\\\\\\n% \\\\label{eq:svd_svd_equation_vectors}\\n\\\\end{align}\\n$$\\n\\nWhat if we were to project our data on to the space created by all\\n$\\\\vec{v}$\\'s, i.e., stack $\\\\vec{v}$ into a matrix $V$? We would then get\\nthe representation of our data in the new latent space (for all latent\\nvariables). The unit vector $\\\\vec{u}$\\'s and their $\\\\sigma$\\'s would also\\nneed to be stacked up on the right side to create the equivalent\\nmatrix - matrix multiplication $U\\\\Sigma$, i.e., matrix $U$ for\\n$\\\\vec{v}$\\'s and $\\\\Sigma$ for $\\\\sigma$\\'s. Previously, we mentioned that\\n$X$ is $NxM$ matrix and, since in the most of the real world cases it is\\nvery overdetermined ($N>>M$) and its covariance matrix $S=X^{\\\\top}X$ is\\npositive semi-definite with exactly $M$ non-zero eigenvalues/vectors, we\\ncan say that $Rank(X)=M$, and we will have exactly $M$ $\\\\vec{v}$ vectors.\\nThis makes matrix $V$ to be $MxM$. We can then say that matrix $U$ must\\nbe $NxN$. In order for the multiplication to work and stacked-up\\n$\\\\vec{u}$\\'s be scaled properly, we need $\\\\Sigma$ to be diagonal matrix\\nof $NxM$. One can easily check that if we are filling gaps with zero\\'s,\\nwe are not changing the assumptions that we have imposed earlier (e.g.,\\ndecorrelated data in latent space and maximum variance captured). Hence,\\n\\n$$\\n\\\\begin{align}\\nX V =  U \\\\Sigma  \\\\ . \\\\\\\\\\n% \\\\label{eq:svd_svd_equation_vectors}\\n\\\\end{align}\\n$$\\n\\nGiven that $V$ is a square and orthogonal matrix, thus\\n$V^{-1} = V^{\\\\top}$. The decomposition of $X$ then becomes\\n$$\\n\\\\begin{align}\\nX =  U \\\\Sigma  V^{\\\\top} \\\\ . \\\\\\\\\\n% \\\\label{eq:svd_svd_equation_vectors}\\n\\\\end{align}\\n$$\\n\\n$~$\\n\\n$~$\\n## References üìú\\n$~$\\n\\n- [This blogpost would not be possible without Peter Bloem\\'s clarifications](https://peterbloem.nl/blog/)   \\n- [Mathy explanation of change of basis](https://dbalague.pages.ewi.tudelft.nl/openlabook/Chapter4/ChangeOfBasis.html)\\n\\n-   [Explanation of matrix as transformation](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:matrices-as-transformations/a/matrices-as-transformations)\\n\\n-   [Explanation for matrix vector product as a projection](http://mitran-lab.amath.unc.edu/courses/MATH547/lessons/Lesson12.pdf)\\n\\n-   [Great read about intuition behind PCA](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"~/pdev/yaub/frontend/simple-yaub/public/\", recursive=True)\n",
    "documents_all = reader.load_data()\n",
    "documents = list()\n",
    "for doc in documents_all:\n",
    "    if \"text\" in doc.metadata[\"file_type\"]:\n",
    "        documents.append(doc) \n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2325b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic parsing into Node's \n",
    "# Node is a chunk of text that we run an embedding model on, \n",
    "# so instead of doc's we embed smaller chunks, i.e., node's  \n",
    "parser = SentenceSplitter(chunk_overlap=0) # no overlap between sentences\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "154af1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '32b041d7-a1cc-408c-b978-7e167d0bc4ff',\n",
       " 'embedding': None,\n",
       " 'metadata': {'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md',\n",
       "  'file_name': 'post.md',\n",
       "  'file_type': 'text/markdown',\n",
       "  'file_size': 12575,\n",
       "  'creation_date': '2025-01-13',\n",
       "  'last_modified_date': '2025-01-13'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='44b3cb82-03e2-4a40-93a2-f98ff9ea3086', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, hash='47f771e72fc571d243e362352f0ce48d2f12bdcdf2e1c305cc333771d77a0be2'),\n",
       "  <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c2796694-6cb2-4039-bdf4-51124d0db7c5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f2cf8ccfc14bf438dda609282c4a78ec3122c08dfe0d47da04e884bbf598e43c')},\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_separator': '\\n',\n",
       " 'text': '<center>\\n    <h1> Bias-Variance Decomposition </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? ü§î\\n$~$\\n\\nThe point of this exercise is to try to create an understanding of what\\noverfitting and underfitting mean in the context of machine learning.\\n\\n$~$\\n\\n$~$\\n## The Model Error ‚öôÔ∏è\\n$~$\\n\\nLet\\'s start by saying that it does really not matter what kind of machine learning model \\nwe want to evaluate a simple regression model, Variation Autoencoded,  transfformer model, Large Language model,\\nclimate model of the Earth, or a model of the Universe. \\nThe principle stays the same - you want to predict an output given some input.\\ninput. Imagine a scenario of the regression task just because the math for it is the easiest to do.\\n**Given** a dataset represented by i.i.d. pairs $x, y$,\\ni.e., observations of independent random variables that follow some (joint) statistical distribution, \\nwe can write the dataset as\\n\\n$$ \\nD=\\\\left\\\\{\\\\left(x_1 ; y_1\\\\right), \\\\ldots, \\\\left(x_n ; y_n\\\\right)\\\\right\\\\}   .\\n$$\\n\\n**We then want to** predict the expected target $\\\\bar{y}(x)$ given an\\nobservation (input) $x$\\n\\n$$\\n\\\\bar{y}(x) = \\\\mathit{E}_{y|x}(y) = \\\\int_{y} y p(y | x) d y  .\\n$$\\n\\n**What we do** is use an algorithm $\\\\mathrm{A}$ to arrive to a function\\n$h$ based on the data sample $D$\\n\\n$$\\n{h}_{D} = \\\\mathrm{A}(D)\\n$$\\n\\nIn our example, $\\\\mathcal{A}$ could be any modelling technique, e.g.,\\nANN, SVM, OLS regression, with its hyperparameter values. Then,\\n$h_D$ is a resulting (trained) model, i.e., model hypothesis.\\nIn order to judge the quality of our trained model we want to compare it\\nto the \"true\\\\\" observed values from the population described by the\\nfunction $\\\\rho$. Thus, **our evaluation function** becomes the\\nexpected/mean squared error\\n\\n$$\\n\\\\mathit{E}_{x, y \\\\sim \\\\rho}\\\\left[\\\\left(h_D(x)-y\\\\right)^2\\\\right]=\\\\iint_{x y} p_{x,y} \\\\cdot \\\\left(h_D(x)-y\\\\right)^2 d x d y\\n$$\\n\\nwhere $\\\\left\\\\{p_{x, y}=Pr(x, y)\\\\right\\\\}$. The caveat is that we usually do not have access to the generative function $\\\\rho$.\\nTherefore, we have to evaluate our models based on the already sampled\\ndataset $D$, i.e., subdivide it into the training and test sets.  \\n\\n$~$\\n\\nIf we could sample infinitely many datasets $D$, we would have a model $\\\\bar{h}$ such that\\n$$\\n\\\\bar{h}=\\\\mathit{E}_{D \\\\sim \\\\rho^{(n)}}[\\\\mathit{A}(D)]=\\\\int h_D \\\\cdot \\\\operatorname{Pr}(D) d D  . \\n$$\\n\\nIn other words, if we had infinite time to sample and observe infinitely\\nmany data samples $D$ from the population, and at the same time run/train the algorithm $\\\\mathit{A}$ with them,\\nwe would have arrived to the best possible model $\\\\bar{h}$. We cannot do better than $\\\\bar{h}$ \\nwhich is our \"expected function\\\\\" given $\\\\mathit{A}$.\\n\\n$~$\\n\\nThe variable $D$ becomes just another random variable in our analysis\\nif we can condition on it. We can thus apply the expectation operator\\nover $D$ and do all sort of crazy things. Speaking of crazy things,\\nlet\\'s rewrite our expected error as if we trained and evaluated our\\nmodel on infinitely many dataset $D$ of size $n$ sampled from the\\nunderlying $\\\\rho$\\n\\n$$\\n\\\\mathit{E}_{\\\\substack{x, y \\\\sim \\\\rho \\\\\\\\ D \\\\sim \\\\rho^{(n)}}}\\\\left[ \\\\left(h_D(x)-y\\\\right)^2\\\\right]\\n=\\\\int_D^{} \\\\int_x^{} \\\\int_y^{} \\\\left(h_D(x)-y\\\\right)^2 \\\\cdot p(x, y) p(D) d x d y d D \\\\ .',\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': 0,\n",
       " 'end_char_idx': 3204,\n",
       " 'metadata_seperator': '\\n',\n",
       " 'text_template': '{metadata_str}\\n\\n{content}'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d567d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "emb_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "for node in nodes: \n",
    "    node.embedding = emb_model.get_text_embedding(node.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd0e984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n",
    "# OR client = QdrantClient(path=\"path/to/db\") # Persists changes to disk, fast prototyping\n",
    "\n",
    "vector_store = QdrantVectorStore(collection_name=\"blog\", client=qdrant_client)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# TODO: does this insert a nodes into the index? \n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model=emb_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97f5fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this context, \"bias\" refers to the error between the true target function ($\\bar{y}$) and the average prediction of the best estimator ($\\bar{h}$). It indicates how much our algorithm is biased towards some other explanation that is not present in the data. Essentially, bias measures the model's systematic error when making predictions.\n",
      "\n",
      "\"Variance,\" on the other hand, refers to the variability of a model prediction for a given data point or set of data points under different training sets. When variance is high, it means that if we were to train our model with different samples, it might result in very different hypotheses due to more parameters being tuned and higher degrees of freedom. High variance indicates that the model has been overfitting the noise in the training dataset rather than learning the underlying patterns.\n",
      "\n",
      "Together, bias and variance form what is known as the bias-variance trade-off in machine learning. Balancing these two aspects allows one to achieve a model that generalizes well to unseen data, avoiding both high bias (underfitting) and high variance (overfitting).\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama  \n",
    "model = Ollama(model=\"qwen2:7b\")\n",
    "\n",
    "query_engine = index.as_query_engine(llm=model)\n",
    "response = query_engine.query(\"What does author mean by bias and variance?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ca6c998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='d20428d5-23bb-4424-82df-91df845518d9', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='44b3cb82-03e2-4a40-93a2-f98ff9ea3086', node_type='4', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, hash='47f771e72fc571d243e362352f0ce48d2f12bdcdf2e1c305cc333771d77a0be2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ca03c5ea-c260-44b8-98c2-56b36d166769', node_type='1', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, hash='17d4137efd3237ea97c2359a5c531581892e4bbed14ec72c470bb8e2b3b82d9e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The Regime #1 is when error due to the variance component prevails. Then, the data is\\nfitted well on the training dataset but when evaluated on a different\\nsample, a high error is returned. That is a trait of high variance since our\\nmodel was \\\\\"too niche\\\\\" to generalise well.\\n\\n$~$\\n\\n$~$\\n## References üìú\\n$~$\\n-   [Analogous derivation](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html8)\\n    \\n-   [Trade-off explained](https://www.youtube.com/watch?v=zrEyxfl2-a8)\\n\\n-   [Nice blog with illustrations](https://mlu-explain.github.io/bias-variance/)\\n\\n-   Nice math read: Elements of Statistical Learning (Chapter 7.3 in 2nd\\n    edition, Jan 2017)', mimetype='text/plain', start_char_idx=11885, end_char_idx=12552, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6737521193774052),\n",
       " NodeWithScore(node=TextNode(id_='ca03c5ea-c260-44b8-98c2-56b36d166769', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='44b3cb82-03e2-4a40-93a2-f98ff9ea3086', node_type='4', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, hash='47f771e72fc571d243e362352f0ce48d2f12bdcdf2e1c305cc333771d77a0be2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='636986fd-1ef9-4443-a87c-ddb480bcb034', node_type='1', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/bias_variance_decomposition/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 12575, 'creation_date': '2025-01-13', 'last_modified_date': '2025-01-13'}, hash='ab8684f53181f2908f6818253143b53537375ab0984319c2b065ceb79fa87638'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d20428d5-23bb-4424-82df-91df845518d9', node_type='1', metadata={}, hash='abbe85b6f8e84bfaa453dfac5408ae09b355045ca545a36b67c908a017e336a9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Next, the bias term includes the true target function $\\\\bar{y}$ and\\nessentially shows what our average error between the best estimator\\n$\\\\bar{h}$ and the true values $\\\\bar{y}$ is. Said differently: how much our\\nalgo $\\\\mathit{A}$ is biased towards some other explanation that is not\\nin the data. Well, as for the noise term. Bad news, we will never know what it\\nis...\\n\\n$~$\\n\\n$~$\\n## The Trade-off ‚öñÔ∏è\\n$~$\\n\\nWhy does bias increase when you reduce variance and vice versa? \\nThat is not clearly shown by the last formula we drew out. \\nAn intuitive explanation would be: the larger your hypothesis pool (more\\noptions of choosing $h_D$), the more likely you are to be close to the\\ntrue function (global minima of the error) but you are also more likely to\\narrive to a different variation of the model hypothesis given a single\\nsampled dataset (since you have more parameters to tune, more degrees of\\nfreedom). Figure 1 tries to illustrate this point. An\\ninflexible model can only have a \\\\\"simple\\\\\" form that is likely to miss\\nthe true underlying target function, but it will always stay within a\\nsmall area of its hypothesis set. While a flexible, highly non-linear\\nmodel is more likely to capture the true, possibly, very complex\\nfunction, but because the algo $\\\\mathit{A}$ will be \\\\\"choosing\\\\\" a model\\nfrom such a variety, it will be very dependent on the $D$ that it gets.\\n\\n<p align=\"center\" id=\"fig:trade_off_intuitive\">\\n    <br>\\n    <strong> Figure 1: Trade-off between Bias and Variance </strong>\\n    <br>\\n    <img width=\"500\" height=\"250\" src=\"/posts/bias_variance_decomposition/pics/trade_off2.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html\\n\\nFor a more practical example of bias-variance inverse relationship with\\na $sin$ function approximation, you can watch\\n[this](https://www.youtube.com/watch?v=zrEyxfl2-a8) video.\\nUnfortunately, I could not find or come up with a derivation or proof of this trade-off\\nrelationship. I would very much appreciate if someone can point me to\\none.\\n\\n$~$\\n\\n$~$\\n## The Regimes ‚ÜóÔ∏è ‚ÜòÔ∏è\\n$~$\\n\\nHopefully, the seed of intuition behind bias and variance, and their connection, is\\nplanted. As a result of this relationship, we have to two common cases, or \"regimes\" when we train a model.\\nFigure 2  attempts to show what regularly happens with the train and test error in practice.\\n\\n<p align=\"center\" id=\"fig:test_train_error\">\\n    <br>\\n    <strong> Figure 2: Two common Bias-Variance regimes </strong>\\n    <br>\\n  <img width=\"500\" height=\"250\" src=\"/posts/bias_variance_decomposition/pics/test_train_error.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nCase #1 is when we trained a model and end up with a low training error\\nwhile our test error is high. This means that $\\\\mathit{A}$ was able to\\nchoose an approximation that is flexible and fitted a sample well. But\\nthe evaluation on another (test set) sample showed that the model was\\nnot general enough.\\n\\nIn Case #2 our training error was higher than an acceptable threshold\\nand we did not capture the complexity of the training dataset that well.\\nNonetheless, our test error is now close to the acceptable level meaning\\nthat implicit assumptions that were made during training might result in\\nbetter generalisation.\\n\\n$~$\\n\\nWhen modeling it is important to strike a balance between bias and variance to achieve a model that is most likely to generalise well.\\nHere are some tips:\\n\\n<p align=\"center\" id=\"fig:test_train_error\">\\n    <br>\\n    <strong> Figure 3: Dealing with The Trade </strong>\\n    <br>\\n  <img width=\"500\" height=\"450\" src=\"/posts/bias_variance_decomposition/pics/regimes3.png\">\\n    <br>\\n    <br>\\n</p>\\n\\nIt is often said that the Regime #2 is when we do not have enough\\npower in the model. It lacks \\\\\"range\\\\\" and cannot capture the\\nrelationships between $x$ and $y$ well so it ends up making\\nsimplifying assumptions. Our error is then high due to high bias.', mimetype='text/plain', start_char_idx=8001, end_char_idx=11883, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6702342886521709)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705bfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
