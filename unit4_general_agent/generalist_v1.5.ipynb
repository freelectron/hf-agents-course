{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Additional tools\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "\n",
    "# RAG system imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: Hello! How can I assist you today?...\n",
      "Initializing RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/longformer-base-4096. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Qdrant collection: web_content\n",
      "RAG system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "WEB_SOURCE_URL_KEY = \"href\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_RETRIES = 3\n",
    "TEMP_DIR = \"./temp_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize RAG system components\n",
    "print(\"Initializing RAG system...\")\n",
    "\n",
    "# Initialize sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer('allenai/longformer-base-4096')\n",
    "embedding_dimension = 768  # Dimension the embedding model \n",
    "\n",
    "# Initialize Qdrant in-memory client\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"web_content\"\n",
    "try:\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(f\"Created Qdrant collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create Qdrant collection: {e}\")\n",
    "\n",
    "print(\"RAG system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scroll through the collection\n",
    "scroll_result, next_page = qdrant_client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    limit=10,              # how many points to fetch at once\n",
    "    with_payload=True,     # include stored payload\n",
    "    with_vectors=False     # set True if you also want full vectors\n",
    ")\n",
    "len(scroll_result[0].payload[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/6mkw37fs20q07p9x11q7d7k80000gq/T/ipykernel_21697/4268506751.py:2: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  res = qdrant_client.search(collection_name=collection_name, query_vector=emb_ve)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='e4a40781-6850-4dcf-8533-fd1d8cdb9d0b', version=0, score=0.9218921661376953, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 10, 'text': 's Sosa sang on YouTube . Retrieved 3 March 2010. ^ a b c d e f g h \"Latin artist Mercedes Sosa dies\" . BBC . 4 October 2009 . Retrieved 5 October 2009 . ^ Karush, Matthew (2017). Musicians in Transit: Argentina and the Globalization of Popular Music . Duke. p.\\xa0168. ISBN \\xa0 978-0-8223-7377-3 . ^ a b Associated Press [ dead link ] ^ a b \"Biografía\" . Fundación Mercedes Sosa (in Spanish) . Retrieved 8 March 2025 . ^ \"El folclore argentino llora la muerte de Daniel Toro - Notas - Viva la Radio\" . Cadena 3 Argentina (in Spanish) . Retrieved 14 March 2025 . ^ Nilsson, Leopoldo Torre (7 April 1971), Güemes - la tierra en armas (Drama, History), Alfredo Alcón, Norma Aleandro, Gabriela Gili, Producciones Cinematográficas Cerrillos , retrieved 8 March 2025 ^ Rodrigo (10 September 2020). \"Patricio Manns: Cuando me acuerdo de mi país (1983) | PERRERAC: La canción, un arma de la revolución\" (in Spanish) . Retrieved 14 March 2025 . ^ a b Lopez, Vicente F. (18 January 1983). \"ARTISTAS EXILIADOS HAN REGRESADO A ARGENTINA\" . El Nuevo Herald . p.\\xa08 . Retrieved 7 March 2025 . ^ Drosdoff, Daniel (30 October 1983). \"ARGENTINIAN VOTE TO END DICTATORSHIP PERONIST AND RADICAL IN LEAD FOR PRESIDENCY\" . Miami Herald . pp.\\xa016A . Retrieved 7 March 2025 . ^ Interview with Mercedes Sosa Archived 16 October 2009 at the Wayback Machine , Magazin Berliner Zeitung , 25 October 2003. (in German) ^ Mercedes Sosa in concert Archived 4 January 2008 at the Wayback Machine ^ Meyer, Bill (7 October 2009). \"A U.S. musician pays tribute to Mercedes Sosa\" . People\\'s World . Retrieved 5 December 2023 . ^ \"In Profile: Mercedes Sosa\" . soundsandcolours.com . 26 August 2010 . Retrieved 27 March 2018 . ^ Balderrama by Mercedes Sosa on YouTube – a tribute to Che Guevara ^ \"Latin Grammys: Ganadores – Años Anteriores (2000)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Latin Grammys: Ganadores – Años Anteriores (2003)\" . Latin Grammys (in Spanish). The Latin Recording Academy', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='b0c5d93a-5c9c-4f68-a617-9369c032df2a', version=0, score=0.9215954542160034, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 12, 'text': '74\" . Al Jazeera . 4 October 2009 . Retrieved 5 October 2009 . ^ \"Continúa la procesión en el Congreso para despedir a Mercedes Sosa\" . ^ a b Helen Popper (4 October 2009). \"Argentine singer Mercedes Sosa dies at 74\" . Reuters . Archived from the original on 11 October 2009 . Retrieved 5 October 2009 . ^ \"Celebrating Mercedes Sosa\" . Doodles Archive, Google . 31 January 2019. ^ \"The 200 Greatest Singers of All Time\" . Rolling Stone . 1 January 2023 . Retrieved 9 March 2023 . Further reading [ edit ] Braceli, Rodolfo (2010). Mercedes Sosa: La Negra (in Spanish). Roma: Perrone. ISBN \\xa0 978-88-6004-347-4 . OCLC \\xa0 904969081 . Christensen, Anette (2019). Mercedes Sosa: The Voice of Hope . Herning, Denmark: Tribute2life Publishing. ISBN \\xa0 978-87-998216-5-5 . OCLC \\xa0 1107989240 . } Christensen, Anette (2019). Mercedes Sosa: More Than a Song . Herning, Denmark: Tribute2life Publishing. ISBN \\xa0 978-87-998216-7-9 . OCLC \\xa0 1144932294 . (Abridged version of Mercedes Sosa: The Voice of Hope ) Matus, Fabián (2016). Mercedes Sosa: La Mami (in Spanish). Buenos Aires, Argentina: Planeta. ISBN \\xa0 978-950-49-5247-3 . OCLC \\xa0 960916958 . External links [ edit ] Wikiquote has quotations related to Mercedes Sosa . Wikimedia Commons has media related to Mercedes Sosa . Tribute to Mercedes Sosa (in Brazilian Portuguese) Mercedes Sosa\\'s website at the Wayback Machine (archived 16 October 2009) (in Spanish) Mercedes Sosa\\'s News (in Spanish) Mercedes Sosa at IMDb Mercedes Sosa discography at Discogs v t e Mercedes Sosa Songs \" La Maza \" Albums La Voz De La Zafra (1962) Canciones Con Fundamento (1965) Yo No Canto Por Cantar (1966) Hermano (1966) Para Cantarle A Mi Gente (1967) Con Sabor A Mercedes Sosa (1968) Mujeres Argentinas (1969) El Grito De La Tierra (1970) Navidad Con Mercedes Sosa (1970) Homenaje a Violeta Parra (1971) Hasta La Victoria (1972) Cantata Sudamericana (1972) Traigo Un Pueblo En Mi Voz (1973) A Que Florezca Mi Pueblo (1975) En Dirección Del Viento (1976) Mercedes Sosa Interpreta', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='292aa39c-20ed-41f2-8bd4-4678a54eaa7f', version=0, score=0.9147681593894958, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 9, 'text': 'a York, 1974 Label: Sony Music Argentina Compilation albums [ edit ] Year Album details 1975 Disco de Oro Label: Philips 1983 Recital Label: Philips 1988 Amigos Míos Label: Philips 1993 30 Años Label: Polygram Argentina 1995 Oro Label: Polygram 1997 The Best of Mercedes Sosa Label: Mercury 2013 Siempre en Ti Label: Universal Music Filmography [ edit ] Güemes, la tierra en armas (1971) Argentinísima (1972) Esta es mi Argentina (1974) Mercedes Sosa, como un pájaro libre (1983) Será possible el sur: Mercedes Sosa (1985) Historias de Argentina en vivo (2001) References [ edit ] ^ Mercedes Sosa at BrainyHistory.com ^ \"Singer Mercedes Sosa: The voice of the \\'voiceless ones\\' outlasts South American dictatorships\" . ^ a b c Heckman, Don (29 October 1995). \"POP MUSIC\\xa0: The Voice Heard Round the World\\xa0: Mercedes Sosa, a compelling figure in world music and a social activist, will make a rare L.A. appearance\" . Los Angeles Times . Retrieved 5 December 2023 . ^ a b c d e f g h \"Legendary folk singer Mercedes Sosa dies at 74\" . France 24 . 4 October 2009 . Retrieved 5 October 2009 . ^ a b c d Bernstein, Adam (5 October 2009). \"Argentine folk singer who championed social justice\" . Los Angeles Times . Retrieved 8 March 2025 . ^ Mercedes Sosa: The Voice of Latin America . Dir. Rodrigo H. Villa. First Run Features, 2013. Web. ^ a b c d e f g h \"Mercedes Sosa: Obituary\" . The Daily Telegraph . 4 October 2009 . Retrieved 5 October 2009 . ^ The presentation by Jorge Cafrune and the song Mercedes Sosa sang on YouTube . Retrieved 3 March 2010. ^ a b c d e f g h \"Latin artist Mercedes Sosa dies\" . BBC . 4 October 2009 . Retrieved 5 October 2009 . ^ Karush, Matthew (2017). Musicians in Transit: Argentina and the Globalization of Popular Music . Duke. p.\\xa0168. ISBN \\xa0 978-0-8223-7377-3 . ^ a b Associated Press [ dead link ] ^ a b \"Biografía\" . Fundación Mercedes Sosa (in Spanish) . Retrieved 8 March 2025 . ^ \"El folclore argentino llora la muerte de Daniel Toro - Notas - Viva la Radio\" . Cad', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='636a778b-7c92-411d-a361-913350a49194', version=0, score=0.9122532606124878, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 20, 'text': 'ana Belinky Teresa Aguiar Vicente Salles Zabé da Loca Apiwtxa ABGLT ABI Associação Comunidade Yuba Centro Cultural Piollin Coletivo Nacional de Cultura do MST Giramundo Teatro de Bonecos Instituto Baccarelli Mestres da Guitarrada Música no Museu Quasar Cia de Dança 2009 Aderbal Freire Filho Afoxé Filhos de Gandhy Alexandre Wollner Ângela Maria Ataulfo Alves Balé Popular do Recife Beatriz Sarlo Bispo do Rosário Boaventura de Sousa Santos Carlos Manga Carmen Miranda Chico Anysio Deborah Colker Davi Kopenawa Yanomami Elifas Andreato Fernanda Abreu Fernando Peixoto Fundação Iberê Camargo Gerson King Combo Gilvan Samico Heleny Guariba Instituto Olga Kos de Inclusão Cultural Ivaldo Bertazzo José Eduardo Agualusa José Miguel Wisnik Laerte Coutinho Luiz Olimecha Lydia Hortélio Mamulengo Só-Riso Manoel de Oliveira Maria Lúcia Godoy Maracatu Estrela de Ouro de Aliança Mestre Vitalino Mia Couto Miguel Rio Branco Nathalia Timberg Ney Matogrosso Noca da Portela Os Gêmeos Patativa do Assaré Paulo Vanzolini Paulo Bruscky Raul Seixas Roberto Burle Marx Sérgio Rodrigues Teatro Vila Velha ONG Video nas Aldeias Walmor Chagas Zeca Pagodinho 2010 Andrea Tonacci Anna Bella Geiger Armando Nogueira Azelene Kaingang Cândido Mendes de Almeida Carlos Drummond de Andrade Carlota Albuquerque Cazuza Cesária Évora Companhia de Danças Folclóricas Aruanda Demônios da Garoa Denise Stoklos Época de Ouro Escuela Internacional de Cine y Televisión Gal Costa Glória Pires Hermeto Pascoal Ilo Krugli Ismael Ivo Ítalo Rossi Jaguar João Cabral de Melo Neto João Carlos de Souza Gomes Joaquim Nabuco Joênia Wapixana Lavadeiras de Almenara Leon Cakoff Leonardo Boff Lira Ceciliana Maracatu Estrela Brilhante de Igarassú Mário Gruber Correia Maureen Bisilliat Maurício Segall Mestre Alberto da Paz Moacir Werneck de Castro Nelson Rodrigues Pedro Casaldáliga Rogério Duarte Tonico Vinicius de Moraes 2011 Academia Brasileira de Letras Adriana Varejão Afonso Borges Ana Montenegro Antônio Nóbrega Antônio Pitanga Apolônio', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='7c0ca748-5447-425d-b486-eb83663293e2', version=0, score=0.9117670059204102, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 8, 'text': \"abel: Philips 1979 Serenata Para la Tierra de Uno Label: Philips 1981 A Quien Doy / Cuando Me Acuerdo de Mi País Label: Philips 1982 Como Un Pájaro Libre Label: Philips 1983 Mercedes Sosa Label: Philips 1984 ¿Será Posible el Sur? Label: Philips 1985 Vengo a Ofrecer Mi Corazón Label: Philips 1986 Mercedes Sosa '86 Label: Philips 1987 Mercedes Sosa '87 Label: Philips 1993 Sino Label: Philips/ Polygram 1994 Gestos de Amor Label: Polydor 1996 Escondido en Mi País Label: Polydor 1997 Alta Fidelidad (w/ Charly García ) Label: Mercury 1998 Al Despertar Label: Mercury 1999 Misa Criolla Label: Mercury 2005 Corazón Libre Label: Edge 2009 Cantora 1 (w/various artists) Label: RCA 2009 Cantora 2 (w/various artists) Label: RCA 2011 Censurada Label: Philips 2015 Lucerito Label: RCA EPs [ edit ] Year EP details 1975 Niño de Mañana Label: Philips Live albums [ edit ] Year Album details 1973 Si Se Calla El Cantor (with Gloria Martin) Label: Philips 1980 Gravado Ao Vivo No Brasil Label: Philips 1982 Mercedes Sosa en Argentina Label: Phonogram /Philips 1985 Corazón Americano (with Milton Nascimento & León Gieco ) Label: Philips 1989 Live in Europe Label: Tropical Music/Polygram Argentina 1991 De Mí Label: Philips 2002 Acústico en Vivo Label: Sony Music Argentina 2003 Argentina Quiere Cantar (with Víctor Heredia & León Gieco ) Label: Odeon / EMI 2010 Deja la Vida Volar (En Gira) Label: RCA 2014 Angel Label: Universal Music 2024 En vivo en el Gran Rex 2006 Label: INAMU Discos Mercedes Sosa en Nueva York, 1974 Label: Sony Music Argentina Compilation albums [ edit ] Year Album details 1975 Disco de Oro Label: Philips 1983 Recital Label: Philips 1988 Amigos Míos Label: Philips 1993 30 Años Label: Polygram Argentina 1995 Oro Label: Polygram 1997 The Best of Mercedes Sosa Label: Mercury 2013 Siempre en Ti Label: Universal Music Filmography [ edit ] Güemes, la tierra en armas (1971) Argentinísima (1972) Esta es mi Argentina (1974) Mercedes Sosa, como un pájaro libre (1983) Será possible el sur\", 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='0b586451-0a74-4587-b76b-0cb2a0d9f3b0', version=0, score=0.9112681746482849, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 13, 'text': 'ercedes Sosa Songs \" La Maza \" Albums La Voz De La Zafra (1962) Canciones Con Fundamento (1965) Yo No Canto Por Cantar (1966) Hermano (1966) Para Cantarle A Mi Gente (1967) Con Sabor A Mercedes Sosa (1968) Mujeres Argentinas (1969) El Grito De La Tierra (1970) Navidad Con Mercedes Sosa (1970) Homenaje a Violeta Parra (1971) Hasta La Victoria (1972) Cantata Sudamericana (1972) Traigo Un Pueblo En Mi Voz (1973) A Que Florezca Mi Pueblo (1975) En Dirección Del Viento (1976) Mercedes Sosa Interpreta A Atahualpa Yupanqui (1977) Mercedes Sosa en Argentina (1982) Cantora 1 & 2 (2009) Related articles Nueva canción v t e Latin Grammy Lifetime Achievement Award 2000s 2004: Antonio Aguilar / Roberto Carlos / Willie Colón / José José / Mercedes Sosa 2005: Rocío Dúrcal / Generoso Jiménez / Jorge Ben Jor / Sérgio Mendes / Johnny Pacheco / Sandro 2006: León Gieco / Graciela / César Camargo Mariano / Richie Ray & Bobby Cruz / Paloma San Basilio / Alberto Vázquez / Johnny Ventura 2007: Alberto Cortez / Lucho Gatica / Olga Guillot / Os Paralamas do Sucesso / Los Tigres del Norte / Chavela Vargas 2008: Vikki Carr / Cheo Feliciano / Astrud Gilberto / Angélica María / María Dolores Pradera / Estela Raval 2009: Cándido Camero / Beth Carvalho / Charly García / Tania Libertad / Marco Antonio Muñiz / Juan Romero 2010s 2010: João Donato / Las Hermanas Márquez / Armando Manzanero / Joseíto Mateo / Jorge Oñate / Susana Rinaldi 2011: Joe Arroyo / Gal Costa / José Feliciano / Álex Lora / Les Luthiers / Rubén Rada / Linda Ronstadt 2012: Luz Casal / Leo Dan / Rita Moreno / Milton Nascimento / Daniela Romo / Poncho Sanchez / Toquinho 2013: Oscar D\\'León / Juan Formell / Roberto Menescal / Totó la Momposina / Palito Ortega / Eddie Palmieri / Miguel Ríos 2014: Willy Chirino / César Costa / Carlos do Carmo / Dúo Dinámico / Los Lobos / Valeria Lynch / Ney Matogrosso 2015: Gato Barbieri / Ana Belén / Ángela Carrasco / Djavan / El Gran Combo de Puerto Rico / Víctor Manuel / Pablo Milanés 2016: El Consorc', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='6aacda07-8230-4fcf-a238-727aa239bfec', version=0, score=0.9110128879547119, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 11, 'text': 'ician pays tribute to Mercedes Sosa\" . People\\'s World . Retrieved 5 December 2023 . ^ \"In Profile: Mercedes Sosa\" . soundsandcolours.com . 26 August 2010 . Retrieved 27 March 2018 . ^ Balderrama by Mercedes Sosa on YouTube – a tribute to Che Guevara ^ \"Latin Grammys: Ganadores – Años Anteriores (2000)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Latin Grammys: Ganadores – Años Anteriores (2003)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Latin Grammys: Ganadores – Años Anteriores (2006)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Latin Grammys: Ganadores – Años Anteriores (2009)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Latin Grammys: Ganadores – Años Anteriores (2011)\" . Latin Grammys (in Spanish). The Latin Recording Academy . Retrieved 7 July 2021 . ^ \"Premios Konex 1995: Música Popular\" . Fundación Konex (in Spanish) . Retrieved 7 July 2021 . ^ \" \"En ningún momento sufrió\", dijo el hijo de Mercedes Sosa\" (in Spanish). October 2009. Archived from the original on 4 October 2009 . Retrieved 1 October 2009 . ^ a b c Javier Doberti (4 October 2009). \"Argentine singer Mercedes Sosa, \\'voice of Latin America,\\' dies at 74\" . CNN . Retrieved 5 October 2009 . ^ \"Argentine folk legend Mercedes Sosa dead at 74\" . Bangkok Post . 4 October 2009 . Retrieved 5 October 2009 . ^ a b \"Argentine folk icon Sosa dies at 74\" . Al Jazeera . 4 October 2009 . Retrieved 5 October 2009 . ^ \"Continúa la procesión en el Congreso para despedir a Mercedes Sosa\" . ^ a b Helen Popper (4 October 2009). \"Argentine singer Mercedes Sosa dies at 74\" . Reuters . Archived from the original on 11 October 2009 . Retrieved 5 October 2009 . ^ \"Celebrating Mercedes Sosa\" . Doodles Archive, Google . 31 January 2019. ^ \"The 200 Greatest Singers of All Time\" . Rolling Stone . 1 January 2023 . Retrieved 9 March 2023 . Further reading [ ed', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='79db8a7e-e9c8-424e-bb8a-e9a3f991102c', version=0, score=0.9099428653717041, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 26, 'text': 'Bahia Companhia Teatral Vem Viver Feira do Livro de Porto Alegre Festival Amazonas de Ópera Festival de Parintins Grupo Corpo Grupo Gira Dança Museu da Diversidade Sexual Museu da Língua Portuguesa Museu dos Quilombos e Favelas Urbanos - Muquifu Ocupa MinC Organizações Globo Spetaculu - Escola de Arte e Tecnologia Portals : Argentina Latin music Authority control databases International ISNI VIAF FAST WorldCat National Germany United States France BnF data Italy Spain Netherlands Norway Chile Argentina Korea Israel Catalonia Artists MusicBrainz People Deutsche Biographie DDB Other IdRef SNAC Yale LUX Retrieved from \" https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&oldid=1306102227 \" Categories : 1935 births 2009 deaths 20th-century Argentine women singers 20th-century drummers Argentine activists Argentine people of Diaguita descent Argentine people of French descent Argentine people of Quechua descent Argentine women activists Bombo legüero players Deaths from kidney failure in Argentina Latin Grammy Award winners Latin Grammy Lifetime Achievement Award winners Nueva canción musicians People from San Miguel de Tucumán Recipients of the Order of Cultural Merit (Brazil) Women in Latin music Hidden categories: All articles with dead external links Articles with dead external links from June 2024 CS1 Spanish-language sources (es) Webarchive template wayback links Articles with German-language sources (de) Articles with short description Short description is different from Wikidata Wikipedia indefinitely move-protected pages Use dmy dates from July 2025 Articles with hCards All articles with unsourced statements Articles with unsourced statements from December 2023 Commons category link is on Wikidata Articles with Brazilian Portuguese-language sources (pt-br) Articles with Spanish-language sources (es) Search Search Mercedes Sosa 49 languages Add topic', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='ef4031e7-6689-4575-adee-2e5a16493581', version=0, score=0.9087234735488892, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 21, 'text': 'o Rossi Jaguar João Cabral de Melo Neto João Carlos de Souza Gomes Joaquim Nabuco Joênia Wapixana Lavadeiras de Almenara Leon Cakoff Leonardo Boff Lira Ceciliana Maracatu Estrela Brilhante de Igarassú Mário Gruber Correia Maureen Bisilliat Maurício Segall Mestre Alberto da Paz Moacir Werneck de Castro Nelson Rodrigues Pedro Casaldáliga Rogério Duarte Tonico Vinicius de Moraes 2011 Academia Brasileira de Letras Adriana Varejão Afonso Borges Ana Montenegro Antônio Nóbrega Antônio Pitanga Apolônio Melônio Associação Capão Cidadão Associação dos Artesãos de Santana do Araçuaí Beth Carvalho Campos de Carvalho Capiba Casa de Produtos Indígenas Wariró Central Única das Favelas Clarice Lispector Claudett de Jesus Ribeiro Dançando para não dançar Dzi Croquettes Espedito Seleiro Evando dos Santos Festival de Dança de Joinville Festival Santista de Teatro Glênio Bianchetti Grupo Galpão Gustavo Dahl Héctor Babenco Helena Kolody Herbert de Sousa Ítala Nandi Jair Rodrigues João do Vale João das Neves José Renato Pécora Leila Diniz Lélia Abramo Luiz Melodia Lygia Bojunga Nunes Maracatu Estrela de Tracunhaém Mário Lago Memorial Jesuíta Unisinos Nelson Cavaquinho Paulo Freire Paulo Gracindo Quinteto Violado Samba de Cumbuca Teatro Tablado Tereza Costa Rêgo Vik Muniz Valdemar de Oliveira Zuzu Angel 2012 Abelardo da Hora Aguinaldo Silva Alceu Valença Almir Narayamoga Suruí Amácio Mazzaropi Anna Muylaert Associação Carnavalesca Bloco Afro Olodum Autran Dourado Breno Silveira Carlos Alberto Cerqueira Lemos Cleodes Maria Piazza Julio Ribeiro Dener Pamplona de Abreu Elba Ramalho Fafá de Belém Felipe Schaedler Hebe Camargo Herivelto Martins Humberto Piva Campana and Fernando Piva Campana Escola de Dança e Integração Social Para Criança e Adolescente Fundação Municipal de Artes de Montenegro Ifigênia Rosa de Oliveira Isay Weinfeld Ismail Xavier Jorge Amado José Sarney Marieta Severo Mário Schenberg Martha Medeiros Miguel Chikaoka Milton Guran Movimento Gay de Minas Museu de Valores do Banco', 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='36596915-b111-4d24-ba50-dac4696ae34c', version=0, score=0.9080615043640137, payload={'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa', 'chunk_index': 19, 'text': \"Mestre Verequete Ministerio de Educación, Cultura y Deporte Moacir Santos MAX Paulo César Saraceni Pompeu Christovam de Pina Racionais MC's Ray-Güde Mertin Rodrigo Melo Franco Sábato Magaldi Santos Dumont Sivuca Tânia Andrade Lima Teodoro Freire Tomie Ohtake Vladimir Carvalho 2007 Abdias Nascimento Álvaro Siza Vieira Antônio Carlos Jobim Associação Cultural Cachuera Banda Cabaçal dos Irmãos Aniceto Bárbara Heliodora Cacique Raoni Cartola Celine Imbert Cildo Meireles Castelo Rá-Tim-Bum Claude Lévi-Strauss Clube do Choro de Brasília Dodô e Osmar Escola de Circo Picolino Glauber Rocha Grande Otelo Grupo Nós do Morro Hermilo Borba Filho Lina Bo Bardi Lia Robatto Luiz Gonzaga Luiz Otavio Souza Santos Luiz Mott José Aparecido de Oliveira Jean-Claude Bernardet Jorge Benjor Judith Malina Kanuá Kamayurá Marcelo Grassmann Moniz Bandeira Museu Paraense Emílio Goeldi Orides Fontela Oscar Niemeyer Ronaldo Fraga Selma do Coco Sérgio Britto Solano Trindade Tônia Carrero Tostão Vânia Toledo Walter Smetak 2008 Ailton Krenak Altemar Dutra Anselmo Duarte Athos Bulcão Benedito Ruy Barbosa Bule-Bule Carlos Lyra Claudia Andujar Dulcina de Moraes Edu Lobo Efigênia Ramos Rolim Eva Todor Goiandira do Couto Guimarães Rosa Hans-Joachim Koellreutter João Candido Portinari Johnny Alf Leonardo Villar Maria Bonomi Marlene Mercedes Sosa Milton Hatoum Nelson Triunfo Marcantônio Vilaça Otávio Afonso Orlando Miranda Paulo Emílio Sales Gomes Paulo Moura Pixinguinha Roberto Corrêa Ruy Guerra Sérgio Ricardo Tatiana Belinky Teresa Aguiar Vicente Salles Zabé da Loca Apiwtxa ABGLT ABI Associação Comunidade Yuba Centro Cultural Piollin Coletivo Nacional de Cultura do MST Giramundo Teatro de Bonecos Instituto Baccarelli Mestres da Guitarrada Música no Museu Quasar Cia de Dança 2009 Aderbal Freire Filho Afoxé Filhos de Gandhy Alexandre Wollner Ângela Maria Ataulfo Alves Balé Popular do Recife Beatriz Sarlo Bispo do Rosário Boaventura de Sousa Santos Carlos Manga Carmen Miranda Chico Anysio Deborah Colker Dav\", 'total_chunks': 27, 'title': 'Wikipedia Mercedes Sosa - Wikipedia', 'summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'search_order': 1}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_ve = embedding_model.encode(\"studio albums\")\n",
    "res = qdrant_client.search(collection_name=collection_name, query_vector=emb_ve)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalist Nodes\n",
    "\n",
    "The following code will be simple base functions that will describe capabilities of the generalist agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classification System\n",
    "@dataclass\n",
    "class QuestionAnalysis:\n",
    "    question_analysis: str\n",
    "    deep_web_search: bool  \n",
    "    video_processing: bool \n",
    "    audio_prcessing: bool \n",
    "    image_processing: bool \n",
    "    structured_data_processing: bool    \n",
    "    unstructered_data_processing: bool  \n",
    "    code_math_writing: bool \n",
    "\n",
    "def classify_question(question: str, attachments: List[str] = None) -> QuestionAnalysis:\n",
    "    \"\"\"\n",
    "    Use LLM to analyze a question and determine what capabilities/steps are needed.\n",
    "    \n",
    "    Args:\n",
    "     question (str): text of the question \n",
    "     attachments (str): list of files that are related to the question \n",
    "\n",
    "    Returns: \n",
    "        QuestionAnalysis: dataclass that describes what answering this question requires \n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments mentioned: {', '.join(attachments)}\"\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "You are a highly intelligent routing agent. Your primary function is to analyze a user's question and determine the precise capabilities required to answer it accurately and efficiently.\n",
    "\n",
    "Question: {question}\n",
    "Attachments: {attachment_info}\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided question and determine the most logical and efficient plan to answer it using the capabilities listed below. Your analysis must be detailed in a step-by-step plan and then summarized in a series of boolean flags.\n",
    "\n",
    "**Capabilities:**\n",
    "- `deep_web_search`: Executing iterative search queries to find websources that answer the query the best, i.e., find and evaluate web content. This steps also often includes SIMPLE synthesize/understanding information from web pages. Use this for questions requiring up-to-date or niche knowledge.\n",
    "- `video_processing`: Processing a video file to analyze its content, typically by extracting frames for image analysis or audio for transcription/analysis.\n",
    "- `audio_processing`: Processing an audio file to transcribe speech, identify sounds, or analyze acoustic properties.\n",
    "- `image_processing`: Visually analyzing an image to identify objects, read text, or understand its content.\n",
    "- `structured_data_processing`: Analyzing, querying, or visualizing data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `unstructured_data_processing`: Performing detailed analysis on a provided block of raw text or multiple (retrieved) documents (e.g., summarization, sentiment analysis, entity extraction, processing multiple pieces of text). This is for analyzing *provided* text.\n",
    "- `code_math_writing`: Generating or executing code, solving mathematical problems, or performing complex computations.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze and Plan:** First, create a clear, direct and concise description of AI capabilities needed to answer this question, put the answer in the `question_analysis` field.\n",
    "2.  **Select Minimum Capabilities:** Based on your description/plan, set the corresponding boolean flags to `true`. Only activate the capabilities that are *absolutely necessary* for your plan. For example, a simple fact-lookup might not require `unstructured_data_processing` on top of `deep_web_search`.\n",
    "3.  **Ensure Consistency:** The capabilities mentioned in your `question_analysis` text MUST perfectly match the boolean flags set to `true`.\n",
    "4.  **Respond in JSON:** Your entire output must be in the exact JSON format specified below.\n",
    "\n",
    "**Better Examples:**\n",
    "\n",
    "Question: \"What is the boiling point of water at sea level?\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"This is a direct factual query. It requires a single deep web search to look up a well-known scientific constant.\",\n",
    "    \"deep_web_search\": \"true\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Summarize the attached meeting notes for me.\" (with a .txt file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"The user has provided a text document and wants a summary. This requires unstructured data processing to read the attached text and generate a concise summary of its key points.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"true\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Count the number of cars in this video and plot their positions on a heatmap.\" (with a video file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"This is a multi-step task. First, it requires video processing to extract frames from the attached video file. Second, it needs image processing to be run on those frames to detect and count objects identified as 'cars' and log their coordinates. Finally, it requires code and mathematical computations to aggregate these coordinates and generate a heatmap visualization.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"true\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"true\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"true\"\n",
    "}}\n",
    "\n",
    "---\n",
    "**Begin Analysis**\n",
    "\n",
    "Question: \"{question}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"question_analysis\": \"Breakdown of what this question requires, including all necessary capabilities and processes\",\n",
    "    \"deep_web_search\": \"true\"/\"false\",\n",
    "    \"video_processing\": \"true\"/\"false\",\n",
    "    \"audio_prcessing\": \"true\"/\"false\",\n",
    "    \"image_processing\": \"true\"/\"false\",\n",
    "    \"structured_data_processing\": \"true\"/\"false\",\n",
    "    \"unstructered_data_processing\": \"true\"/\"false\",\n",
    "    \"code_math_writing\": \"true\"/\"false\"\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(classification_prompt)\n",
    "\n",
    "    response_text = response.text.strip()\n",
    "    result = json.loads(response_text)\n",
    "    return QuestionAnalysis(\n",
    "        question_analysis=result[\"question_analysis\"],\n",
    "        deep_web_search=result[\"deep_web_search\"],\n",
    "        video_processing=result[\"video_processing\"], \n",
    "        audio_prcessing=result[\"audio_prcessing\"], \n",
    "        image_processing=result[\"image_processing\"], \n",
    "        structured_data_processing=result[\"structured_data_processing\"],    \n",
    "        unstructered_data_processing=result[\"unstructered_data_processing\"],  \n",
    "        code_math_writing=result[\"code_math_writing\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def parse_into_list(text:str, separator: str = \"|\") -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return text.strip().strip(\"[]\").split(separator)\n",
    "\n",
    "\n",
    "def question_to_query(question: str) -> list[str]:\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a set of general search engine queries for the following question: \"{question}\". \n",
    "    \n",
    "    Make sure that:\n",
    "    - Your output is a list separated by \"|\" sing and nothing else\n",
    "    - Give maximum of two options where each query should be uniquely phrased \n",
    "    - Never use double or single quates anywhere in the answer\n",
    "    - Do not mention any specific website where the information should be searched\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"question_to_query prompt: \", prompt)\n",
    "    # Send this to an llm \n",
    "    query_responses = llm.complete(prompt)\n",
    "    \n",
    "    # Parse the response \n",
    "    return parse_into_list(query_responses.text)\n",
    "\n",
    "# @tool\n",
    "def duckduckgo_search(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No search results found for '{query}'\"\n",
    "\n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(results, 1):\n",
    "                title = result.get('title', 'No title')\n",
    "                body = result.get('body', 'No description')\n",
    "                href = result.get(WEB_SOURCE_URL_KEY, 'No URL')\n",
    "                \n",
    "                formatted_results.append({\"search_order\": i, \"web_page_title\": title, \"web_page_summary\": body, \"url\": href})\n",
    "            return formatted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {e}\"\n",
    "\n",
    "def drop_non_unique_dicts(lst:list, unique_key:str=WEB_SOURCE_URL_KEY):\n",
    "    seen_hrefs = set()\n",
    "    result_list = []\n",
    "    \n",
    "    for item in lst:\n",
    "        href = item.get(unique_key)\n",
    "        \n",
    "        if href not in seen_hrefs:\n",
    "            result_list.append(item)\n",
    "            seen_hrefs.add(href)\n",
    "            \n",
    "    return result_list\n",
    "\n",
    "def web_search_question(question: str, web_search_links_per_query: int=3):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Disambiguate the question into a query\n",
    "    candidate_queries = question_to_query(question)\n",
    "    \n",
    "    print(\"Candidate queries for the question are: \", candidate_queries)\n",
    "\n",
    "    # Search for relevant sources \n",
    "    sources = list()\n",
    "    for query in candidate_queries:\n",
    "        sources_query = duckduckgo_search(query, web_search_links_per_query)\n",
    "        sources.extend(sources_query)\n",
    "    \n",
    "    print(\"Found resources are: \", sources)\n",
    "    return drop_non_unique_dicts(sources)\n",
    "\n",
    "\n",
    "def extract_text_with_links(raw_html: str, base_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, converting hyperlinks to Markdown format.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        base_url (str): Base URL to resolve relative links (e.g., Wikipedia base)\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with hyperlinks in Markdown format: [text](url)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert all <a> tags to Markdown-style links\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link_text = a_tag.get_text(separator=\" \", strip=True)\n",
    "            if not link_text:  # Skip if link has no text\n",
    "                a_tag.replace_with(\"\")\n",
    "                continue\n",
    "                \n",
    "            href = a_tag['href']\n",
    "            \n",
    "            # Resolve relative URLs\n",
    "            if href.startswith('/'):\n",
    "                full_url = urllib.parse.urljoin(base_url, href)\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            # Replace the <a> tag with Markdown link\n",
    "            markdown_link = f\"[{link_text}]({full_url})\"\n",
    "            a_tag.replace_with(markdown_link)\n",
    "        \n",
    "        # Extract all text (now with Markdown links)\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, removing hyperlinks and unwanted elements.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted plain text with no hyperlinks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "\n",
    "        # Extract all visible text\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_base_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract base URL (scheme + netloc) from a full URL.\n",
    "    Example:\n",
    "        Input:  https://en.wikipedia.org/wiki/Prussia\n",
    "        Output: https://en.wikipedia.org\n",
    "    \"\"\"                                                                                         \n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "def download_text_content(url: str):\n",
    "    \"\"\"\n",
    "    Download and process content from approved sources.\n",
    "    \"\"\"\n",
    "    text_content = None\n",
    "    \n",
    "    try:\n",
    "        print(f\"⬇️ Starting to download: {url}\")\n",
    "        with urllib.request.urlopen(url, timeout=30) as response:\n",
    "            html_content = response.read()\n",
    "            text_content = extract_clean_text(html_content) #extract_text_with_links(html_content, base_url=get_base_url(url))         \n",
    "    # ToDo: create custom errors    \n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"❌ URL Error for {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error for {url}: {e}\")\n",
    "    \n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG system functions implemented successfully!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_from_wiki(raw_text: str) -> str:\n",
    "    import mwparserfromhell\n",
    "    wikicode = mwparserfromhell.parse(raw_text)\n",
    "    return str(wikicode)\n",
    "\n",
    "\n",
    "def preprocess_text_w_llm(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text to be in Markdown format by making a call to Ollama model and asking it to clean up the format.\n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): Raw text content to be preprocessed\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text formatted in Markdown with proper structure\n",
    "    \"\"\"\n",
    "    preprocessing_prompt = f\"\"\"\n",
    "You are an expert text formatter and technical writer. Your task is to convert the provided raw text into clean, well-structured Markdown format.\n",
    "\n",
    "**Instructions:**\n",
    "1. **General Formatting**: Convert the text to proper Markdown format with appropriate headers, lists, emphasis, and structure\n",
    "2. **Tables**: If you find any tabular data or information that looks like a table:\n",
    "    - Convert it to JSON format in a code block\n",
    "    - Add a descriptive annotation before the JSON explaining what the table contains\n",
    "    - Use this format:\n",
    "    ```\n",
    "    **Table Description: [Brief description of what this data represents]**\n",
    "    ```json\n",
    "    {{\n",
    "        \"data\": [your JSON structure here]\n",
    "    }}\n",
    "    ```\n",
    "3. **Equations**: If you find mathematical equations or formulas:\n",
    "    - Format them properly using Markdown/LaTeX syntax when possible\n",
    "    - Add annotations explaining what each equation represents\n",
    "    - Use this format:\n",
    "    ```\n",
    "    **Equation: [Brief description of what this equation represents]**\n",
    "    $$equation here$$\n",
    "    ```\n",
    "4. **Lists**: Convert any list-like content to proper Markdown lists\n",
    "5. **Headers**: Create appropriate heading hierarchy using # ## ### etc.\n",
    "6. **Emphasis**: Use **bold** and *italic* appropriately for important terms\n",
    "7. **Code**: Wrap any code snippets in appropriate code blocks with language specification\n",
    "8. **Links**: Preserve and properly format any URLs or references\n",
    "\n",
    "**Quality Standards:**\n",
    "- Maintain all original information and data\n",
    "- Ensure the output is readable and well-organized\n",
    "- Use consistent formatting throughout\n",
    "- Remove any formatting artifacts or noise from the original text\n",
    "- Preserve the logical structure and flow of information\n",
    "\n",
    "**Raw Text to Process:**\n",
    "{raw_text}\n",
    "\n",
    "**Output the cleaned Markdown version:**\n",
    "\"\"\"\n",
    "        \n",
    "    print(\"Processing text with Ollama for Markdown formatting...\")\n",
    "    response = llm.complete(preprocessing_prompt)\n",
    "    cleaned_text = response.text.strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "        \n",
    "\n",
    "def embed_and_store_document(text: str, url: str, metadata: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Embed document text and store it in Qdrant vector store.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text content to embed and store\n",
    "        url (str): Source URL of the document\n",
    "        metadata (dict): Additional metadata to store with the document\n",
    "        \n",
    "    Returns:\n",
    "        str: Document ID that was stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split text into chunks for better retrieval\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=2000,        \n",
    "            chunk_overlap=500,     \n",
    "            separator=\"\"            \n",
    "        )\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Preprocess chunk\n",
    "            cleaned_chunk = preprocess_text_from_wiki(chunk) #preprocess_text_w_llm(chunk)\n",
    "            \n",
    "            # Generate embedding for the chunk\n",
    "            embedding = embedding_model.encode(cleaned_chunk).tolist()\n",
    "            \n",
    "            # Create document ID\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Prepare metadata\n",
    "            chunk_metadata = {\n",
    "                \"url\": url,\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk,\n",
    "                \"total_chunks\": len(chunks)\n",
    "            }\n",
    "            if metadata:\n",
    "                chunk_metadata.update(metadata)\n",
    "            \n",
    "            # Store in Qdrant\n",
    "            point = PointStruct(\n",
    "                id=doc_id,\n",
    "                vector=embedding,\n",
    "                payload=chunk_metadata\n",
    "            )\n",
    "            \n",
    "            qdrant_client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                points=[point]\n",
    "            )\n",
    "        \n",
    "        print(f\"Stored {len(chunks)} chunks from {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing document: {e}\")\n",
    "\n",
    "def search_vector_store(query: str, limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search the vector store for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        limit (int): Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of relevant documents with metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Embed the query\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        \n",
    "        # Search in Qdrant\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for result in search_results:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"text\": result.payload.get(\"text\", \"\"),\n",
    "                \"url\": result.payload.get(\"url\", \"\"),\n",
    "                \"chunk_index\": result.payload.get(\"chunk_index\", 0),\n",
    "                \"metadata\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching vector store: {e}\")\n",
    "        return []\n",
    "\n",
    "def answer_from_rag(question: str, context_limit: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG approach - search vector store and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_limit (int): Maximum number of context chunks to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer based on retrieved context\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search for relevant documents\n",
    "        relevant_docs = search_vector_store(question, limit=context_limit)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return \"No relevant information found in the vector store.\"\n",
    "        \n",
    "        # Combine context from relevant documents\n",
    "        context_pieces = []\n",
    "        urls_used = set()\n",
    "        \n",
    "        for doc in relevant_docs:\n",
    "            context_pieces.append(f\"Source: {doc['url']}\\nContent: {doc['text']}\")\n",
    "            urls_used.add(doc['url'])\n",
    "        \n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # Generate answer using LLM with retrieved context\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following retrieved information, answer the user's question. \n",
    "        Use only the information provided in the context. If the answer cannot be found \n",
    "        in the context, say so explicitly.\n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        RETRIEVED CONTEXT:\n",
    "        {combined_context}\n",
    "\n",
    "        ANSWER:\"\"\"\n",
    "        \n",
    "        response = llm.complete(prompt)\n",
    "        \n",
    "        # Add source information\n",
    "        sources_info = f\"\\n\\nSources used: {', '.join(urls_used)}\"\n",
    "        \n",
    "        return response.text + sources_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating RAG answer: {e}\"\n",
    "\n",
    "print(\"RAG system functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-enhanced web search functions implemented successfully!\n"
     ]
    }
   ],
   "source": [
    "def synthesize_answer_rag(question: str, url: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG-based web page analysis that stores content in vector store and queries it.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The original question to answer\n",
    "        url (str): The URL of the webpage to download and analyze\n",
    "        \n",
    "    Returns:\n",
    "        str: Answer generated using RAG approach\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the content of the page \n",
    "        page_content = download_text_content(url)\n",
    "        \n",
    "        if not page_content:\n",
    "            return f\"Could not retrieve content from {url}\"\n",
    "        \n",
    "        # Store the content in vector store\n",
    "        embed_and_store_document(page_content, url)\n",
    "        print(f\"Seemed like successfully stored content from {url} in vector store\")\n",
    "        \n",
    "        # Now use RAG to answer the question\n",
    "        rag_answer = answer_from_rag(question)\n",
    "        \n",
    "        return rag_answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error in RAG-based synthesis for {url}: {e}\"\n",
    "\n",
    "def web_search_with_rag(question: str, web_search_links_per_query: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced web search that uses RAG system to store and query multiple web pages.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Final answer based on RAG analysis of multiple sources\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get search results\n",
    "        search_results = web_search_question(question, web_search_links_per_query)\n",
    "        \n",
    "        if not search_results:\n",
    "            return \"No search results found for the question.\"\n",
    "        \n",
    "        print(f\"Found search results:\\n{search_results} \")\n",
    "        \n",
    "        # Process each search result and store in vector store\n",
    "        for i, search_result in enumerate(search_results[:3]):  # Limit to top 3 results\n",
    "            print(f\"Processing result {i+1}: {search_result['url']}\")\n",
    "            try:\n",
    "                page_content = download_text_content(search_result[\"url\"])\n",
    "                if page_content:\n",
    "                    # Add search result metadata\n",
    "                    metadata = {\n",
    "                        \"title\": search_result.get(\"web_page_title\", \"\"),\n",
    "                        \"summary\": search_result.get(\"web_page_summary\", \"\"),\n",
    "                        \"search_order\": search_result.get(\"search_order\", i+1)\n",
    "                    }\n",
    "                    embed_and_store_document(page_content, search_result[\"url\"], metadata)\n",
    "                else:\n",
    "                    print(f\"Could not retrieve content from {search_result['url']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {search_result['url']}: {e}\")\n",
    "        \n",
    "        # Generate final answer using RAG\n",
    "        final_answer = answer_from_rag(question, context_limit=5)\n",
    "        \n",
    "        return final_answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error in RAG-based web search: {e}\"\n",
    "\n",
    "print(\"RAG-enhanced web search functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\"\n",
    "# search_results = web_search_question(question, web_search_links_per_query=3)\n",
    "\n",
    "# if not search_results:\n",
    "#     print(\"!!!No search results found for the question!!!\")\n",
    "\n",
    "# print(f\"Found search results:\\n{search_results}\")\n",
    "\n",
    "# # Process each search result and store in vector store\n",
    "# for i, search_result in enumerate(search_results[:3]):  # Limit to top 3 results\n",
    "#     print(f\"Processing result {i+1}: {search_result['url']}\")\n",
    "#     try:\n",
    "#         page_content = download_text_content(search_result[\"url\"])\n",
    "#         if page_content:\n",
    "#             # Add search result metadata\n",
    "#             metadata = {\n",
    "#                 \"title\": search_result.get(\"web_page_title\", \"\"),\n",
    "#                 \"summary\": search_result.get(\"web_page_summary\", \"\"),\n",
    "#                 \"search_order\": search_result.get(\"search_order\", i+1)\n",
    "#             }\n",
    "#             embed_and_store_document(page_content, search_result[\"url\"], metadata)\n",
    "#         else:\n",
    "#             print(f\"Could not retrieve content from {search_result['url']}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {search_result['url']}: {e}\")\n",
    "\n",
    "# search_vec = embedding_model.encode(\"studio albums\")\n",
    "\n",
    "# search_results = qdrant_client.search(\n",
    "#     collection_name=collection_name,\n",
    "#     query_vector=search_vec,\n",
    "#     with_payload=True\n",
    "# )\n",
    "# search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG-based web search...\n",
      "Question: How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
      "================================================================================\n",
      "question_to_query prompt:  \n",
      "    Create a set of general search engine queries for the following question: \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\". \n",
      "\n",
      "    Make sure that:\n",
      "    - Your output is a list separated by \"|\" sing and nothing else\n",
      "    - Give maximum of two options where each query should be uniquely phrased \n",
      "    - Never use double or single quates anywhere in the answer\n",
      "    - Do not mention any specific website where the information should be searched\n",
      "    \n",
      "Candidate queries for the question are:  ['\"number of studio albums released by Mercedes Sosa between 2000 and 2009\" ', ' \"Mercedes Sosa\\'s total studio album count from 2000 to 2009\"']\n",
      "Found resources are:  [{'search_order': 1, 'web_page_title': 'Wikipedia Mercedes Sosa - Wikipedia', 'web_page_summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa'}, {'search_order': 2, 'web_page_title': 'Wikipedia Category:Mercedes Sosa albums - Wikipedia', 'web_page_summary': 'The following 4 pages are in this category, out of 4 total. This list may not reflect recent changes.', 'url': 'https://en.wikipedia.org/wiki/Category:Mercedes_Sosa_albums'}, {'search_order': 3, 'web_page_title': 'Discogs Mercedes Sosa Discography: Vinyl, CDs, & More | Discogs', 'web_page_summary': \"Explore Mercedes Sosa 's biography, discography, and artist credits. Shop rare vinyl records, top albums , and more on Discogs.\", 'url': 'https://www.discogs.com/artist/333361-Mercedes-Sosa'}, {'search_order': 1, 'web_page_title': 'Wikipedia Mercedes Sosa - Wikipedia', 'web_page_summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa'}, {'search_order': 2, 'web_page_title': 'Wikipedia Category:Mercedes Sosa albums - Wikipedia', 'web_page_summary': 'The following 4 pages are in this category, out of 4 total . This list may not reflect recent changes.', 'url': 'https://en.wikipedia.org/wiki/Category:Mercedes_Sosa_albums'}, {'search_order': 3, 'web_page_title': 'BestEverAlbums.com Mercedes Sosa : Best Ever Albums', 'web_page_summary': 'Mercedes Sosa is ranked number 4,010 in the overall artist rankings with a total rank score of 226 .', 'url': 'https://www.besteveralbums.com/thechart.php?b=3270'}]\n",
      "Found search results:\n",
      "[{'search_order': 1, 'web_page_title': 'Wikipedia Mercedes Sosa - Wikipedia', 'web_page_summary': '6 days ago - Her career spanned four decades ... two posthumous Latin Grammy Award for Best Folk Album in 2009 and 2011. She won the Premio Gardel in 2000 , the main musical award in Argentina. She served as an ambassador for UNICEF. Sosa was born on 9 July 1935, in San Miguel de Tucumán, ...', 'url': 'https://en.wikipedia.org/wiki/Mercedes_Sosa'}] \n",
      "Processing result 1: https://en.wikipedia.org/wiki/Mercedes_Sosa\n",
      "⬇️ Starting to download: https://en.wikipedia.org/wiki/Mercedes_Sosa\n",
      "Stored 27 chunks from https://en.wikipedia.org/wiki/Mercedes_Sosa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/6mkw37fs20q07p9x11q7d7k80000gq/T/ipykernel_21697/2664070970.py:140: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG-based Answer:\n",
      "The retrieved information does not explicitly state the number of studio albums published by Mercedes Sosa between 2000 and 2009. Therefore, I cannot provide an explicit answer based on the context given. The focus seems to be more on her discography as a whole rather than within that specific time frame.\n",
      "\n",
      "Sources used: https://en.wikipedia.org/wiki/Mercedes_Sosa\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG-based web search system\n",
    "question = \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\"\n",
    "\n",
    "print(\"Testing RAG-based web search...\")\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the new RAG-based web search\n",
    "rag_answer = web_search_with_rag(question)\n",
    "print(\"\\nRAG-based Answer:\")\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing RAG-based analysis of specific URL...\n",
      "⬇️ Starting to download: https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6\n",
      "Stored 4 chunks from https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6\n",
      "Seemed like successfully stored content from https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6 in vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/6mkw37fs20q07p9x11q7d7k80000gq/T/ipykernel_21697/2664070970.py:140: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Analysis Result for https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6:\n",
      "The context provided does not contain information about the exact number of studio albums published by Mercedes Sosa between 2000 and 2009.\n",
      "\n",
      "Sources used: https://en.wikipedia.org/wiki/Mercedes_Sosa\n"
     ]
    }
   ],
   "source": [
    "# Test RAG-based analysis of a specific URL\n",
    "print(\"\\nTesting RAG-based analysis of specific URL...\")\n",
    "specific_url = \"https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6\"\n",
    "rag_result = synthesize_answer_rag(question, specific_url)\n",
    "print(f\"\\nRAG Analysis Result for {specific_url}:\")\n",
    "print(rag_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image analysis with Llava \n",
    "\n",
    "Llava does not usually provide accurate and correct results for specific queries related to images (e.g., tasks like counting object or describing what text is on the image). It can however describe in very very general terms what is on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Analyzer Tool with LLaVA Integration\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from typing import Literal\n",
    "\n",
    "@tool\n",
    "def image_analyzer_llava(image_path: str, task: str = Literal[\"describe\", \"text\"]) -> str:\n",
    "    \"\"\"\n",
    "    Analyze images using local LLaVA instance - describe content, analyze chess positions, read text, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return f\"Image file not found: {image_path}\"\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get basic image information\n",
    "        width, height = image.size\n",
    "        mode = image.mode\n",
    "        format_type = image.format\n",
    "        file_size = os.path.getsize(image_path)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Convert image to base64 for API transmission\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Create task-specific prompts\n",
    "        if task == \"describe\":\n",
    "            prompt = \"Describe what you see in this image in detail.\"\n",
    "        elif task == \"text\":\n",
    "            prompt = \"Extract and read any text visible in this image.\"\n",
    "        else:\n",
    "            prompt = f\"Analyze this image for the following task: {task}\"\n",
    "        \n",
    "        # Placeholder for LLaVA API call\n",
    "        llava_response = send_to_llava(img_base64, prompt)\n",
    "        \n",
    "        basic_info = f\"Image: {file_name}\\nSize: {width}x{height}\\nMode: {mode}\\nFormat: {format_type}\\nFile size: {file_size} bytes\"\n",
    "        \n",
    "        return f\"{basic_info}\\n\\nLLaVA Analysis:\\n{llava_response}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image '{image_path}': {e}\"\n",
    "\n",
    "def send_to_llava(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder function to send image to local LLaVA instance.\n",
    "    Replace this with actual API call to your LLaVA server.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Placeholder for actual LLaVA API integration\n",
    "        # This would typically be a POST request to localhost:11434 or similar\n",
    "        \n",
    "        # Example of what the actual implementation might look like:\n",
    "        import requests\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_base64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", \n",
    "                               json=payload, \n",
    "                               timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No response from LLaVA\")\n",
    "        else:\n",
    "            return f\"LLaVA API error: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with LLaVA: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured (tabular) data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processor tool implemented successfully\n"
     ]
    }
   ],
   "source": [
    "# File Processor Tool\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "@tool\n",
    "def file_processor(file_path: str, file_type: str = \"auto\") -> str:\n",
    "    \"\"\"\n",
    "    Process various file types - Excel files, CSV files, Parquet files, text files, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "        \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            # Process Excel files\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Excel file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Process CSV files with automatic delimiter detection\n",
    "            def detect_delimiter(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sample = f.read(1024)\n",
    "                    sniffer = csv.Sniffer()\n",
    "                    delimiter = sniffer.sniff(sample).delimiter\n",
    "                    return delimiter\n",
    "            \n",
    "            try:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                \n",
    "                # Basic analysis\n",
    "                shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "                columns_info = f\"Columns: {list(df.columns)}\"\n",
    "                delimiter_info = f\"Detected delimiter: '{delimiter}'\"\n",
    "                \n",
    "                # Calculate totals for numeric columns\n",
    "                numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                totals_info = \"\"\n",
    "                if len(numeric_cols) > 0:\n",
    "                    totals = df[numeric_cols].sum()\n",
    "                    totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "                \n",
    "                sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "                \n",
    "                return f\"CSV file: {file_name}\\nFile size: {file_size} bytes\\n{delimiter_info}\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "            \n",
    "            except Exception as csv_error:\n",
    "                # Fallback to text processing if CSV parsing fails\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                line_count = len(content.split('\\n'))\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                \n",
    "                return f\"CSV file (read as text due to parsing error): {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nError: {csv_error}\\n\\nContent preview:\\n{preview}\"\n",
    "        \n",
    "        elif file_path.endswith('.parquet'):\n",
    "            # Process Parquet files\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Parquet file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        else:\n",
    "            # Read as text file for all other formats (including Python files)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            \n",
    "            line_count = len(content.split('\\n'))\n",
    "            word_count = len(content.split())\n",
    "            char_count = len(content)\n",
    "            \n",
    "            preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "            \n",
    "            file_type_desc = \"Python file\" if file_path.endswith('.py') else \"Text file\"\n",
    "            \n",
    "            return f\"{file_type_desc}: {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nWords: {word_count}\\nCharacters: {char_count}\\n\\nContent preview:\\n{preview}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file '{file_path}': {e}\"\n",
    "\n",
    "print(\"File processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
