{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Additional tools\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "\n",
    "# RAG system imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "# ollama run qwen2:7b\n",
    "# ollama run phi4:14b\n",
    "# ollama run deepseek-r1:14b\n",
    "# ollama run qwen2.5:14b\n",
    "#  try Falcon3-10B-Instruct ?\n",
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "WEB_SOURCE_URL_KEY = \"href\"\n",
    "\n",
    "CHUNK_SIZE_TEXT = 10000 \n",
    "CHUNK_SIZE_RAG = 4000\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_RETRIES = 3\n",
    "TEMP_DIR = \"./temp_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize RAG system components\n",
    "print(\"Initializing RAG system...\")\n",
    "\n",
    "# Initialize sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer('allenai/longformer-base-4096')\n",
    "embedding_dimension = 768  # Dimension the embedding model \n",
    "\n",
    "# Initialize Qdrant in-memory client\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"web_content\"\n",
    "try:\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(f\"Created Qdrant collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create Qdrant collection: {e}\")\n",
    "print(\"RAG system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalist Nodes\n",
    "\n",
    "The following code will be simple base functions that will describe capabilities of the generalist agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the task\n",
    "Involves:\n",
    "1. Defining what is asked  \n",
    "2. Defining the steps to answer the question: planning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a task, determine a step-by-step action plan of what needs to be done to accomplish this task and output the answer/result. \n",
    "    The most important actions that are taken: \n",
    "     1. Define the goal: what result is asked to be produced.\n",
    "     2. List the steps: provide a short explanation for each action that needs to be taken.       \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert project planner. Your task is to create a concise, step-by-step action plan to accomplish the user's goal.\n",
    "\n",
    "User's Goal:\n",
    "---\n",
    "{task}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "1. Clarify the Core Objective: Start by rephrasing the user's goal as a single, clear, and specific objective.\n",
    "2. Develop a Chronological Action Plan: Break down the objective into a logical sequence of high-level steps.\n",
    "\n",
    "Guiding Principles for the Plan:\n",
    "- Tool-Agnostic: Focus on the action required, not the specific tool to perform it (e.g., use \"Gather data on market trends\" instead of \"Search Google for market trends\").\n",
    "- Information First: The initial step should almost always be to gather and analyze the necessary information before taking further action.\n",
    "- S.M.A.R. Steps: Each step must be Specific, Measurable, Achievable, and Relevant. The focus is on the logical sequence, not specific deadlines.\n",
    "- Concise: Include only the critical steps needed to reach the objective.\n",
    "\n",
    "Example Output Format (ALWAS **JSON** ):\n",
    "{{\n",
    "  \"objective\": \"Plan and execute a one-day offsite event for a team of 10 people focused on team building and strategic planning.\",\n",
    "  \"plan\": [\n",
    "    \"Gather requirements including budget, potential dates, and key goals for the offsite from team leadership\",\n",
    "    \"Research and shortlist suitable venues and activity options that fit the budget and goals\",\n",
    "    \"Create a detailed agenda and budget proposal for approval\",\n",
    "    \"Book the selected venue, catering, and activities upon approval\",\n",
    "    \"Send out official invitations and manage attendee confirmations and dietary requirements\",\n",
    "    \"Finalize all logistical details and communicate the full itinerary to the team\"\n",
    "  ]\n",
    "}}\n",
    "where\n",
    "  \"objective\" 's value in the json is a clear, one-sentence summary of the end goal,\n",
    "  \"plan\" 's value in the json is a list **ALWAYS SEPARATED BY PYTHON NEWLINE CHARCTER** like \n",
    "  [\n",
    "    A short explanation of the first logical step\", \n",
    "    A short explanation of the next step that follows from the first\",\n",
    "    And so on...\"\n",
    "  ]\n",
    "\"\"\"\n",
    "    task_response = llm.complete(prompt)\n",
    "\n",
    "    return task_response.text\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    question: str\n",
    "    objective: str\n",
    "    plan: list[str]\n",
    "\n",
    "\n",
    "def define_task(task:str) -> str: \n",
    "    task_plan_response = create_plan(task)\n",
    "\n",
    "    # Assume llm outputs smth json-like with the correct keys.\n",
    "    result = json.loads(task_plan_response)\n",
    "\n",
    "    return Task(\n",
    "      question=task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Capabilities:\n",
    "    analysis: str\n",
    "    deep_web_search: bool  \n",
    "    video_processing: bool \n",
    "    audio_prcessing: bool \n",
    "    image_processing: bool \n",
    "    structured_data_processing: bool    \n",
    "    unstructered_data_processing: bool  \n",
    "    code_math_writing: bool \n",
    "\n",
    "def get_capabilities_required(task: str, attachments: List[str] = None) -> Capabilities:\n",
    "    \"\"\"\n",
    "    Specification of the task at hand and use an LLM to analyze it. \n",
    "    Determine what capabilities are needed.\n",
    "    \n",
    "    Args:\n",
    "     task (str): description of the task\n",
    "     attachments (str): list of files that are related to the task \n",
    "\n",
    "    Returns: \n",
    "        QuestionAnalysis: dataclass that describes what executing this task requires \n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments mentioned: {', '.join(attachments)}\"\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "You are a highly intelligent routing agent. Your primary function is to analyze a user's task and determine the precise capabilities required to execute it accurately and efficiently.\n",
    "\n",
    "Task: {task}\n",
    "Attachments: {attachment_info}\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided question and determine the most logical and efficient plan to answer it using the capabilities listed below. Your analysis must be detailed in a step-by-step plan and then summarized in a series of boolean flags.\n",
    "\n",
    "**Capabilities:**\n",
    "- `deep_web_search`: Executing iterative search queries to find websources that answer the query the best, i.e., find, evaluate and download web content. This steps DOES NOT INCLUDE synthesize/understanding information from the web resources. Use this for questions requiring up-to-date or niche knowledge.\n",
    "- `video_processing`: Processing a video file to analyze its content, typically by extracting frames for image analysis or audio for transcription/analysis.\n",
    "- `audio_processing`: Processing an audio file to transcribe speech, identify sounds, or analyze acoustic properties.\n",
    "- `image_processing`: Visually analyzing an image to identify objects, read text, or understand its content.\n",
    "- `structured_data_processing`: Analyzing, querying, or visualizing data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `unstructured_data_processing`: Performing detailed analysis on a provided block of raw text or multiple (retrieved) documents (e.g., summarization, sentiment analysis, entity extraction, processing multiple pieces of text). This is for analyzing *provided* text.\n",
    "- `code_math_writing`: Generating or executing code, solving mathematical problems, or performing complex computations.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze and Plan:** First, create a clear, direct and concise description of AI capabilities needed to answer this question, put the answer in the `analysis` field.\n",
    "2.  **Select Minimum Capabilities:** Based on your description/plan, set the corresponding boolean flags to `true`. Only activate the capabilities that are *absolutely necessary* for your plan. For example, a simple fact-lookup might not require `unstructured_data_processing` on top of `deep_web_search`.\n",
    "3.  **Ensure Consistency:** The capabilities mentioned in your `analysis` text MUST perfectly match the boolean flags set to `true`.\n",
    "4.  **Respond in JSON:** Your entire output must be in the exact JSON format specified below.\n",
    "\n",
    "**Better Examples:**\n",
    "\n",
    "Task: \"What is the boiling point of water at sea level?\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"analysis\": \"This is a direct factual query. It requires a single deep web search to look up a well-known scientific constant.\",\n",
    "    \"deep_web_search\": \"true\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Summarize the attached meeting notes for me.\" (with a .txt file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"analysis\": \"The user has provided a text document and wants a summary. This requires unstructured data processing to read the attached text and generate a concise summary of its key points.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"true\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Count the number of cars in this video and plot their positions on a heatmap.\" (with a video file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"analysis\": \"This is a multi-step task. First, it requires video processing to extract frames from the attached video file. Second, it needs image processing to be run on those frames to detect and count objects identified as 'cars' and log their coordinates. Finally, it requires code and mathematical computations to aggregate these coordinates and generate a heatmap visualization.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"true\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"true\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"true\"\n",
    "}}\n",
    "\n",
    "------------------\n",
    "**Begin Analysis**\n",
    "\n",
    "Question: \"{task}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"analysis\": \"Breakdown of what this question requires, including all necessary capabilities and processes\",\n",
    "    \"deep_web_search\": \"true\"/\"false\",\n",
    "    \"video_processing\": \"true\"/\"false\",\n",
    "    \"audio_prcessing\": \"true\"/\"false\",\n",
    "    \"image_processing\": \"true\"/\"false\",\n",
    "    \"structured_data_processing\": \"true\"/\"false\",\n",
    "    \"unstructered_data_processing\": \"true\"/\"false\",\n",
    "    \"code_math_writing\": \"true\"/\"false\"\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(classification_prompt)\n",
    "\n",
    "    response_text = response.text.strip()\n",
    "    result = json.loads(response_text)\n",
    "    return Capabilities(\n",
    "        analysis=result[\"analysis\"],\n",
    "        deep_web_search=result[\"deep_web_search\"],\n",
    "        video_processing=result[\"video_processing\"], \n",
    "        audio_prcessing=result[\"audio_prcessing\"], \n",
    "        image_processing=result[\"image_processing\"], \n",
    "        structured_data_processing=result[\"structured_data_processing\"],    \n",
    "        unstructered_data_processing=result[\"unstructered_data_processing\"],  \n",
    "        code_math_writing=result[\"code_math_writing\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web search defined as an information retrieval task.\n",
    "\n",
    "It deals with finding relevant resources on the web, downloading them and processing them into a usable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def parse_into_list(text:str, separator: str = \"|\") -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return text.strip().strip(\"[]\").split(separator)\n",
    "\n",
    "\n",
    "def question_to_query(question: str) -> list[str]:\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a list of general search engine queries for the following question: {question}. \n",
    "    \n",
    "    Make sure that:\n",
    "    - Your output is a list separated by \"|\" sing and nothing else\n",
    "    - Give MAXIMUM of TWO (2) search engine queries  \n",
    "    - Each query should be SHORT and precise\n",
    "\n",
    "    Example Output: \n",
    "    Large urban population areas in Europe | Biggest cities in Europe\n",
    "    Short History of Prussia | Origins and the story of East Germany    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"question_to_query prompt: \", prompt)\n",
    "    # Send this to an llm \n",
    "    query_responses = llm.complete(prompt)\n",
    "    \n",
    "    # Parse the response \n",
    "    return parse_into_list(query_responses.text)\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo search.\n",
    "    \n",
    "    Format of returning json:\n",
    "        {\"search_order\": i, \"web_page_title\": title, \"web_page_summary\": summary, WEB_SOURCE_URL_KEY: url}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No search results found for '{query}'\"\n",
    "\n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                title = result.get('title', 'No title')\n",
    "                body = result.get('body', 'No description')\n",
    "                href = result.get(WEB_SOURCE_URL_KEY, 'No URL')\n",
    "                \n",
    "                formatted_results.append({\"search_order\": i, \"web_page_title\": title, \"web_page_summary\": body, WEB_SOURCE_URL_KEY: href})\n",
    "            return formatted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {e}\"\n",
    "\n",
    "def drop_non_unique_dicts(lst:list, unique_key:str=WEB_SOURCE_URL_KEY):\n",
    "    seen_hrefs = set()\n",
    "    result_list = []\n",
    "    \n",
    "    for item in lst:\n",
    "        href = item.get(unique_key)\n",
    "        \n",
    "        if href not in seen_hrefs:\n",
    "            result_list.append(item)\n",
    "            seen_hrefs.add(href)\n",
    "            \n",
    "    return result_list\n",
    "\n",
    "def web_search_question(question: str, web_search_links_per_query: int=1) -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Disambiguate the question into a query\n",
    "    candidate_queries = question_to_query(question)\n",
    "    \n",
    "    print(\"Candidate queries for the question are: \", candidate_queries)\n",
    "\n",
    "    # Search for relevant sources \n",
    "    sources = list()\n",
    "    for query in candidate_queries:\n",
    "        sources_query = duckduckgo_search(query, web_search_links_per_query)\n",
    "        sources.extend(sources_query)\n",
    "    \n",
    "    return drop_non_unique_dicts(sources)\n",
    "\n",
    "\n",
    "def extract_text_with_links(raw_html: str, base_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, converting hyperlinks to Markdown format.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        base_url (str): Base URL to resolve relative links (e.g., Wikipedia base)\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with hyperlinks in Markdown format: [text](url)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert all <a> tags to Markdown-style links\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link_text = a_tag.get_text(separator=\" \", strip=True)\n",
    "            if not link_text:  # Skip if link has no text\n",
    "                a_tag.replace_with(\"\")\n",
    "                continue\n",
    "                \n",
    "            href = a_tag['href']\n",
    "            \n",
    "            # Resolve relative URLs\n",
    "            if href.startswith('/'):\n",
    "                full_url = urllib.parse.urljoin(base_url, href)\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            # Replace the <a> tag with Markdown link\n",
    "            markdown_link = f\"[{link_text}]({full_url})\"\n",
    "            a_tag.replace_with(markdown_link)\n",
    "        \n",
    "        # Extract all text (now with Markdown links)\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, removing hyperlinks and unwanted elements.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted plain text with no hyperlinks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "\n",
    "        # Extract all visible text\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_base_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract base URL (scheme + netloc) from a full URL.\n",
    "    Example:\n",
    "        Input:  https://en.wikipedia.org/wiki/Prussia\n",
    "        Output: https://en.wikipedia.org\n",
    "    \"\"\"                                                                                         \n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "def extract_text_from_wiki(raw_text: str) -> str:\n",
    "    import mwparserfromhell\n",
    "    wikicode = mwparserfromhell.parse(raw_text)\n",
    "    return str(wikicode) \n",
    "\n",
    "def download_text_content(url: str):\n",
    "    \"\"\"\n",
    "    Download and process content from approved sources.\n",
    "    \"\"\"\n",
    "    text_content = None\n",
    "    \n",
    "    try:\n",
    "        print(f\"⬇️ Starting to download: {url}\")\n",
    "        with urllib.request.urlopen(url, timeout=30) as response:\n",
    "            html_content = response.read()\n",
    "            # TODO: determine how to better choose between these options:\n",
    "            # - extract_text_from_wiki (LEAVES HTML TAGS)\n",
    "            # - extract_clean_text(html_content) \n",
    "            # - extract_text_with_links(html_content, base_url=get_base_url(url))       \n",
    "            text_content = extract_clean_text(html_content)   \n",
    "    # ToDo: create custom errors    \n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"❌ URL Error for {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error for {url}: {e}\")\n",
    "    \n",
    "    return text_content\n",
    "\n",
    "\n",
    "def preprocess_text_w_llm(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text to be in Markdown format by making a call to Ollama model and asking it to clean up the format.\n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): Raw text content to be preprocessed\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text formatted in Markdown with proper structure\n",
    "    \"\"\"\n",
    "    preprocessing_prompt = f\"\"\"\n",
    "You are an expert text formatter and technical writer. Your task is to convert the provided raw text into clean, well-structured Markdown format.\n",
    "\n",
    "**Instructions:**\n",
    "1. **General Formatting**: Convert the text to proper Markdown format with appropriate headers, lists, emphasis, and structure\n",
    "2. **Tables**: If you find any tabular data or information that looks like a table:\n",
    "    - Convert it to JSON format in a code block\n",
    "    - Add a descriptive annotation before the JSON explaining what the table contains\n",
    "    - Use this format:\n",
    "    ```\n",
    "    **Table Description: [Brief description of what this data represents]**\n",
    "    ```json\n",
    "    {{\n",
    "        \"data\": [your JSON structure here]\n",
    "    }}\n",
    "    ```\n",
    "3. **Equations**: If you find mathematical equations or formulas:\n",
    "    - Format them properly using Markdown/LaTeX syntax when possible\n",
    "    - Add annotations explaining what each equation represents\n",
    "    - Use this format:\n",
    "    ```\n",
    "    **Equation: [Brief description of what this equation represents]**\n",
    "    $$equation here$$\n",
    "    ```\n",
    "4. **Lists**: Convert any list-like content to proper Markdown lists\n",
    "5. **Headers**: Create appropriate heading hierarchy using # ## ### etc.\n",
    "6. **Emphasis**: Use **bold** and *italic* appropriately for important terms\n",
    "7. **Code**: Wrap any code snippets in appropriate code blocks with language specification\n",
    "8. **Links**: Preserve and properly format any URLs or references\n",
    "\n",
    "**Quality Standards:**\n",
    "- Maintain all original information and data\n",
    "- Ensure the output is readable and well-organized\n",
    "- Use consistent formatting throughout\n",
    "- Remove any formatting artifacts or noise from the original text\n",
    "- Preserve the logical structure and flow of information\n",
    "\n",
    "**Raw Text to Process:**\n",
    "{raw_text}\n",
    "\n",
    "**Output the cleaned Markdown version:**\n",
    "\"\"\"\n",
    "        \n",
    "    print(\"Processing text with Ollama for Markdown formatting...\")\n",
    "    response = llm.complete(preprocessing_prompt)\n",
    "    cleaned_text = response.text.strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify the format for retrieving information from the web  = THE RESOURCE FORMAT as  \n",
    "# {\n",
    "#     \"text\": text string,\n",
    "#     \"url\": web link\n",
    "#     \"metadata\": some info\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(question: str, web_search_links_per_query: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    search_results = web_search_question(question, web_search_links_per_query)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No search results found for the question.\"\n",
    "    \n",
    "    print(f\"Found search results:\\n{search_results} \")\n",
    "    resources = list()\n",
    "    for i, search_result in enumerate(search_results):  \n",
    "        page_content = download_text_content(search_result[WEB_SOURCE_URL_KEY])\n",
    "\n",
    "        resources.append({\n",
    "            \"text\": page_content,\n",
    "            WEB_SOURCE_URL_KEY: search_result[WEB_SOURCE_URL_KEY],\n",
    "            # Todo: clean up metadata\n",
    "            \"metadata\": search_result,\n",
    "        })\n",
    "\n",
    "    return resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_final_answer(task:str, context:str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    You are presented with a list of expert answers from different source that you need summarise.\n",
    "\n",
    "    LIST:\n",
    "    {context}\n",
    "    \n",
    "    Based **ONLY** on that list and without any addition assumptions from your side, perform the the task specified. \n",
    "    \n",
    "    TASK:\n",
    "    {task}\n",
    "    \n",
    "    Your answer should be in json format like so:\n",
    "    {{\n",
    "        \"answer\": <a single number, word of a phrase which si the answer to the question>,\n",
    "        \"clarification\": <very short mention of what the answer is based on>,\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "        - If the text contains the complete answer → put the exact answer in \"answer\".\n",
    "        - If the text contains no relevant information → put \"answer\": \"not found\".\n",
    "        - If the text contains some but not all information → put \"answer\": \"not found\".\n",
    "        - The \"clarification\" must mention the relevant part of the text and explain briefly.\n",
    "\n",
    "    Examples:\n",
    "    Q: \"Who won the 2022 FIFA World Cup?\"\n",
    "    {{\n",
    "    \"answer\": \"not found\",\n",
    "    \"clarification\": \"The text mentions the location of the tournament but not the winner.\"\n",
    "    }}\n",
    "    Q: \"How many colours there is in the rainbow\"\n",
    "    {{\n",
    "    \"answer\": \"12\",\n",
    "    \"clarification\": \"Red,Orange,Yellow,Chartreuse green,Green,Blue-green,Cyan,Azure,Violet,Purple,Magenta,Red\"\n",
    "    }}\n",
    "    Q:\"What's the name of Russian Santa?\"\n",
    "    {{\n",
    "    \"answer\": \"Ded Moroz\",\n",
    "    \"clarification\": \"Easter Slavic Father Frost\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    answer = llm.complete(prompt)\n",
    "\n",
    "    return answer.text\n",
    "\n",
    "def task_with_text_llm(task: str, text:str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Perform the instruction/task in the user's question. \n",
    "    Use only the information provided in the context. \n",
    "    \n",
    "    TASK \n",
    "    {task}\n",
    "\n",
    "    CONTEXT\n",
    "    {text}\n",
    "\n",
    "    **IMPORTANT** If the text does not include the SPECIFIC information about the task, output \"NOT FOUND\"\n",
    "    Output\n",
    "    \"\"\"\n",
    "\n",
    "    llm_result = llm.complete(prompt)\n",
    "    \n",
    "    return llm_result.text\n",
    "\n",
    "def text_process_llm(task: str, text:str):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE_TEXT,        \n",
    "        chunk_overlap=500,     \n",
    "        separator=\"\"            \n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    responses = list()\n",
    "    for chunk in chunks: \n",
    "        answer_response = task_with_text_llm(task, chunk)\n",
    "\n",
    "        responses.append(answer_response)\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\"\n",
    "\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())\n",
    "# web_resources = web_search(question)\n",
    "# llm_response = text_process_llm(task.plan[1] + task.plan[2], web_resources[0][\"text\"])\n",
    "# contruct_final_answer(task.plan[-1], context=llm_response[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '.rewsna eht sa \"tfel\" drow eht fo etisoppo eht etirw ,ecnetnes siht dnatsrednu uoy fI'\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who nominated the only Featured Article on English Wikipedia about a dinosaur that was promoted in November 2016?\"\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\"\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the surname of the equine veterinarian mentioned in 1.E Exercises from the chemistry materials licensed by Marisa Alviar-Agnew & Henry Agnew under the CK-12 license in LibreText's Introductory Chemistry materials as compiled 08/21/2023?\"\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"In the video https://www.youtube.com/watch?v=L1vXCYZAYYM, what is the highest number of bird species to be on camera simultaneously?\"\n",
    "task = define_task(question)\n",
    "pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store_document(text: str, url: str, metadata: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Embed document text and store it in Qdrant vector store.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text content to embed and store\n",
    "        url (str): Source URL of the document\n",
    "        metadata (dict): Additional metadata to store with the document\n",
    "        \n",
    "    Returns:\n",
    "        str: Document ID that was stored\n",
    "    \"\"\"\n",
    "    # Split text into chunks for better retrieval\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE_RAG,        \n",
    "        chunk_overlap=500,     \n",
    "        separator=\"\"            \n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Preprocess chunk\n",
    "        cleaned_chunk = extract_text_from_wiki(chunk) #preprocess_text_w_llm(chunk)\n",
    "        \n",
    "        # Generate embedding for the chunk\n",
    "        # TODO: trying out to embed also meta info\n",
    "        to_embed = cleaned_chunk + url\n",
    "        embedding = embedding_model.encode(to_embed).tolist()\n",
    "        \n",
    "        # Prepare metadata\n",
    "        chunk_metadata = {\n",
    "            WEB_SOURCE_URL_KEY: url,\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": chunk,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        }\n",
    "        if metadata:\n",
    "            chunk_metadata.update(metadata)\n",
    "        \n",
    "        # Store in Qdrant\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding,\n",
    "            payload=chunk_metadata\n",
    "        )\n",
    "        \n",
    "        qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=[point]\n",
    "        )\n",
    "    print(f\"Stored {len(chunks)} chunks from {url}\")\n",
    "\n",
    "\n",
    "def search_vector_store(query: str, limit: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search the vector store for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        limit (int): Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of relevant documents with metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search in Qdrant\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for result in search_results:\n",
    "        # Use `resource` format \n",
    "        resource = {\n",
    "            \"text\": result.payload.get(\"text\", \"\"),\n",
    "            WEB_SOURCE_URL_KEY: result.payload.get(WEB_SOURCE_URL_KEY, \"\"),\n",
    "            \"metadata\": result.payload\n",
    "        } \n",
    "        results.append(resource)\n",
    "\n",
    "        with open(\"text-dump-rag.txt\",\"a\") as file:\n",
    "            file.write(resource.__str__())\n",
    "            file.write(\"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def answer_from_rag(question: str, context_limit: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG approach - search vector store and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        context_limit (int): Maximum number of context chunks to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer based on retrieved context\n",
    "    \"\"\"\n",
    "    # Search for relevant documents\n",
    "    relevant_docs = search_vector_store(question, limit=context_limit)\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return \"No relevant information found in the vector store.\"\n",
    "    \n",
    "    # Combine context from relevant documents\n",
    "    context_pieces = []\n",
    "    urls_used = set()\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        context_pieces.append(f\"Source: {doc[WEB_SOURCE_URL_KEY]}\\nContent: {doc['text']}\")\n",
    "        urls_used.add(doc[WEB_SOURCE_URL_KEY])\n",
    "    \n",
    "    combined_context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "    \n",
    "    # Call to LLM to perform \"process unstructured data\" call       \n",
    "    response = text_process_llm(question,combined_context)\n",
    "    \n",
    "    return response.text \n",
    "        \n",
    "def create_rag_system_from_web_resources(web_resources: list[dict], web_search_links_per_query: int = 1):\n",
    "    \"\"\"\n",
    "    Enhanced web search that uses RAG system to store and query multiple web pages.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Final answer based on RAG analysis of multiple sources\n",
    "    \"\"\"\n",
    "    # Process each search result and store in vector store\n",
    "    for i, web_resource in enumerate(web_resources):  \n",
    "        print(f\"Processing result {i+1}: {web_resource[WEB_SOURCE_URL_KEY]}\")\n",
    "        if web_resource[\"text\"]:\n",
    "            embed_and_store_document(web_resource[\"text\"], web_resource[WEB_SOURCE_URL_KEY], web_resource[\"metadata\"])\n",
    "        else:\n",
    "            print(f\"Could not retrieve any content from {web_resource}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image analysis with Llava \n",
    "\n",
    "Llava does not usually provide accurate and correct results for specific queries related to images (e.g., tasks like counting object or describing what text is on the image). It can however describe in very very general terms what is on the image. \n",
    "\n",
    "**IT MAKES A LOT OF MISTAKES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Analyzer Tool with LLaVA Integration\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from typing import Literal\n",
    "\n",
    "def image_analyzer_llava(image_path: str, task: str = Literal[\"describe\", \"text\"]) -> str:\n",
    "    \"\"\"\n",
    "    Analyze images using local LLaVA instance - describe content, analyze chess positions, read text, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return f\"Image file not found: {image_path}\"\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get basic image information\n",
    "        width, height = image.size\n",
    "        mode = image.mode\n",
    "        format_type = image.format\n",
    "        file_size = os.path.getsize(image_path)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Convert image to base64 for API transmission\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Create task-specific prompts\n",
    "        if task == \"describe\":\n",
    "            prompt = \"Describe what you see in this image in detail.\"\n",
    "        elif task == \"text\":\n",
    "            prompt = \"Extract and read any text visible in this image.\"\n",
    "        else:\n",
    "            prompt = f\"Analyze this image for the following task: {task}\"\n",
    "        \n",
    "        # Placeholder for LLaVA API call\n",
    "        llava_response = send_to_llava(img_base64, prompt)\n",
    "        \n",
    "        basic_info = f\"Image: {file_name}\\nSize: {width}x{height}\\nMode: {mode}\\nFormat: {format_type}\\nFile size: {file_size} bytes\"\n",
    "        \n",
    "        return f\"{basic_info}\\n\\nLLaVA Analysis:\\n{llava_response}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image '{image_path}': {e}\"\n",
    "\n",
    "def send_to_llava(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder function to send image to local LLaVA instance.\n",
    "    Replace this with actual API call to your LLaVA server.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Placeholder for actual LLaVA API integration\n",
    "        # This would typically be a POST request to localhost:11434 or similar\n",
    "        \n",
    "        # Example of what the actual implementation might look like:\n",
    "        import requests\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_base64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", \n",
    "                               json=payload, \n",
    "                               timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No response from LLaVA\")\n",
    "        else:\n",
    "            return f\"LLaVA API error: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with LLaVA: {e}\"\n",
    "    \n",
    "# image_analyzer_llava(image_path=\"/Users/maksim.rostov/Desktop/vllama2.png\", task=\"describe\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured (tabular) data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Processor Tool for tables\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def peak_into_table(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a tabular file, options: \n",
    "     - Excel files \n",
    "     - CSV files \n",
    "     - Parquet files  \n",
    "    into memory and get some information about there contents.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    err = None    \n",
    "    try:\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "        \n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        \n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Process CSV files with automatic delimiter detection\n",
    "            def detect_delimiter(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sample = f.read(1024)\n",
    "                    sniffer = csv.Sniffer()\n",
    "                    delimiter = sniffer.sniff(sample).delimiter\n",
    "                    return delimiter\n",
    "            \n",
    "            delimiter = detect_delimiter(file_path)\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                        \n",
    "        elif file_path.endswith('.parquet'):\n",
    "            df = pd.read_parquet(file_path)\n",
    "        else:\n",
    "            err = \"ERROR: unrecognised file extension.\"\n",
    "            print(err)\n",
    "    \n",
    "    except Exception as e:\n",
    "        err = f\"ERROR: tried but could not load the file: {e}\"\n",
    "        print(err)\n",
    "    \n",
    "    if not df.empty:\n",
    "        shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "        columns_info = f\"Columns: {list(df.columns)}\"\n",
    "        sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        from io import StringIO\n",
    "        str_buffer = StringIO()\n",
    "        df.info(buf=str_buffer)\n",
    "        info_df = \"df.info = \" + str_buffer.getvalue()\n",
    "\n",
    "        return shape_info+\"\\n\"+columns_info+\"\\n\"+sample_data+\"\\n\"+info_df \n",
    "            \n",
    "    return f\"Failed to look into data: {err}\"\n",
    "\n",
    "# print(peak_into_table(\"7bd855d8-463d-4ed5-93ca-5fe35145f733.xls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_python_for_table_analysis(task:str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate python code with llm to analyse tablar data.     \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Perform the following task: {task}  \n",
    "    \n",
    "    Based on the following context\n",
    "    CONTEXT \n",
    "    {context}\n",
    "\n",
    "    You should output python code that would process and analyse data that is presented in the context to accomplish the task.\n",
    "    The code will have a single main function that would only have one argument which is the filename of a csv that that needs to be loaded.  \n",
    "    Output your results in the following format: \n",
    "    ```python\n",
    "    < imports necesary >\n",
    "    import pandas as pd \n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--filepath\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    def main(filepath):\n",
    "        < code here >\n",
    "        df = pd.read_csv(filepath)\n",
    "        return result \n",
    "\n",
    "    print(main(args.filepath))\n",
    "    ```\n",
    "    and should include all the necessary imports and finally the call to the function. \n",
    "    \n",
    "    Examples: \n",
    "\n",
    "    ```python\n",
    "        import matplotlib.pyplot as plt\n",
    "        from io import BytesIO\n",
    "        import pandas as pd \n",
    "        \n",
    "        import argparse\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "        parser.add_argument(\"--filepath\")\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        def main(filepath):\n",
    "            '''Plots a scatter plot of all the columns in the dataframe and return just the image''' \n",
    "            df = pd.read_csv(filepath) \n",
    "            fig, ax = plt.subplots()\n",
    "            column = \"apples\"\n",
    "            ax = df[column].plot(ax=ax)\n",
    "\n",
    "            buffer = BytesIO()\n",
    "            plt.savefig(buffer, format=\"png\")\n",
    "            buffer.seek(0)\n",
    "            img_png = buffer.getvalue()\n",
    "            img_png_encoded = base64.b64decode(img_png)\n",
    "\n",
    "            return img_png_encoded\n",
    "\n",
    "        print(main(args.filepath))\n",
    "    ```\n",
    "\n",
    "    ```python\n",
    "        import pandas as pd \n",
    "        import argparse\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "        parser.add_argument(\"--<argument>\")\n",
    "\n",
    "        args = parser.parse_args()        \n",
    "        def main(filepath):\n",
    "            '''Get average growth difference between April 2014 and 2013''' \n",
    "            df = pd.read_csv(filepath) \n",
    "            df_agg = df.groupby([\"year\", \"month\"]).mean()\n",
    "            diff_yield = df_agg[(df_agg.index.get_index_value(0)==2014) & ((df_agg.get_index_value(1)==\"April\"))] - df_agg[(df_agg.index.get_index_value(0)==2013) & ((df_agg.get_index_value(1)==\"April\"))] \n",
    "            return diff_yield \n",
    "\n",
    "        print(main(args.filepath))\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    code_response =  llm.complete(prompt)\n",
    "\n",
    "    return code_response.text\n",
    "\n",
    "def execute_code_on_dataframe(df: pd.DataFrame, code: str) -> str:\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "\n",
    "    filename = \"table_data.csv\"\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    open_file_handle = tempfile.NamedTemporaryFile(\"wt\")\n",
    "    with open_file_handle as f:\n",
    "        f.write(code)\n",
    "        f.flush()\n",
    "\n",
    "        call_result = subprocess.run([\"python3\", open_file_handle.name, \"--filepath\", filename], capture_output=True, text=True)\n",
    "\n",
    "    return  call_result.stdout, call_result.stderr\n",
    "\n",
    "\n",
    "############\n",
    "# EXAMPLE\n",
    "############\n",
    "\n",
    "# # The attached Excel file contains the sales of menu items for a local fast-food chain.\n",
    "# # What were the total sales that the chain made from food (not including drinks)? Express your answer in USD with two decimal places.\n",
    "\n",
    "# df = tabular_file_to_pandas(\"./7bd855d8-463d-4ed5-93ca-5fe35145f733.xls\")\n",
    "\n",
    "# # TODO: maybe makes sense to put these into preprocessing step so that we get some info about the data right away \n",
    "# shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "# columns_info = f\"Columns: {list(df.columns)}\"\n",
    "# sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "# print(df.info())\n",
    "\n",
    "# from io import StringIO\n",
    "\n",
    "# str_buffer = StringIO()\n",
    "# df.info(buf=str_buffer)\n",
    "# info_df = str_buffer.getvalue()\n",
    "\n",
    "# code = generate_python_for_table_analysis(\n",
    "#     \"Sum all the columns that are food (ONLY EXCLUDE THE DRINKS, DESERTS IS FOOD)\", \n",
    "#     context=shape_info+\"\\n\"+columns_info+\"\\n\"+info_df\n",
    "# )\n",
    "# print(code)\n",
    "\n",
    "# code_cleaned = code.strip(\"`\").strip(\"'\").strip(\"python\")\n",
    "# exec_stdout, exec_stderr = execute_code_on_dataframe(df, code_cleaned)\n",
    "# print(exec_stdout)\n",
    "# print(exec_stderr)\n",
    "\n",
    "# df[[\"Burgers\",\"Hot Dogs\",\"Salads\",\"Fries\",\"Ice Cream\"]].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_python_code(task:str, context: str):\n",
    "    \"\"\"\n",
    "    Generate python code for an arbitrary task and uses context for the information needed to accopmplish it. \n",
    "    Usually the context is the details where to take inputs/data for the task.     \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Perform the following task: {task}  \n",
    "    \n",
    "    Based on the following context:\n",
    "    {context}\n",
    "\n",
    "    You should output python code that would perform the task while using any neccessary information specified in the context.\n",
    "    Often context will include names of files/resources/name that you should use.\n",
    "\n",
    "    The code will have a single main function that would only have one argument which is the filename of a csv that that needs to be loaded.  \n",
    "    Output the code and nothing else. You should include all the necessary imports and the code should only perform steps needed to achieve the task. \n",
    "    \n",
    "    Example 1 (produce and save a line plot for 'apples' column of the table in 'table_data.csv'):\n",
    "    Output: \n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd \n",
    "    def main(inputfilepath, outputfile):\n",
    "        '''Plots a scatter plot of all the columns in the dataframe and return just the image''' \n",
    "        df = pd.read_csv(inputfilepath) \n",
    "        fig, ax = plt.subplots()\n",
    "        column = \"apples\"\n",
    "        ax = df[column].plot(ax=ax)\n",
    "        plt.savefig(outputfile, format=\"png\")\n",
    "    inputfilepath = 'table_data.csv'\n",
    "    outputfile = 'line_plot_apples.png'\n",
    "    main(inputfilepath, outputfile)\n",
    "\n",
    "    Example 2 (calculate the difference in yield btw april's of 2014 and 2013 data in 'table_data.csv'):\n",
    "    Output: \n",
    "    import pandas as pd \n",
    "    def main(inputfilepath, outputfile):\n",
    "        '''Get average growth difference between April 2014 and 2013''' \n",
    "        df = pd.read_csv(inputfilepath) \n",
    "        df_agg = df.groupby([\"year\", \"month\"]).mean()\n",
    "        diff_yield = df_agg[(df_agg.index.get_index_value(0)==2014) & ((df_agg.get_index_value(1)==\"April\"))] - df_agg[(df_agg.index.get_index_value(0)==2013) & ((df_agg.get_index_value(1)==\"April\"))] \n",
    "        with open(outputfile, 'wt') as f:\n",
    "             f.write(diff_yield) \n",
    "    inputfilepath = 'table_data.csv'\n",
    "    outputfile = 'yield_diff.txt'\n",
    "    main(inputfilepath, outputfile)\n",
    "\n",
    "    ------------------------------------------------\n",
    "    **IMPORTANT**: Output code and nothing else, no addition formatting needed, just code that could be executed by running `python3` command on it. \n",
    "    \"\"\"\n",
    "\n",
    "    code_reps = llm.complete(prompt)\n",
    "\n",
    "    return code_reps.text\n",
    "\n",
    "def execute_python_code(code: str):\n",
    "    \"\"\"\n",
    "    Function that executes arbitruary python code and outputs its stdout and stderr. \n",
    "\n",
    "    Args:\n",
    "        code (str): should always parse key-value arguments as would be given to subprocess command. \n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import tempfile\n",
    "\n",
    "    open_file_handle = tempfile.NamedTemporaryFile(\"wt\")\n",
    "    with open_file_handle as f:\n",
    "        f.write(code)\n",
    "        f.flush()\n",
    "\n",
    "        call_result = subprocess.run([\"python3\", open_file_handle.name], capture_output=True, text=True)\n",
    "\n",
    "    return  call_result.stdout, call_result.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"The attached Excel file contains the sales of menu items for a local fast-food chain.\" \\\n",
    "\"What were the total sales that the chain made from food (not including drinks)? \" \\\n",
    "\"Express your answer in USD with two decimal places.\"\n",
    "context = \"The table data is saved inside `7bd855d8-463d-4ed5-93ca-5fe35145f733.xls` that you can access from this folder.\"\n",
    "peak_result = peak_into_table(\"7bd855d8-463d-4ed5-93ca-5fe35145f733.xls\")\n",
    "context += \"Data in the table looks like this:\\n\"+peak_result\n",
    "\n",
    "code = write_python_code(task, context)\n",
    "execute_python_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
