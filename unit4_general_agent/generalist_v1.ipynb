{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Additional tools\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from PIL import Image\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "WEB_SOURCE_URL_KEY = \"href\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_RETRIES = 3\n",
    "TEMP_DIR = \"./temp_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalist Nodes\n",
    "\n",
    "The following code will be simple base functions that will describe capabilities of the generalist agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classification System\n",
    "@dataclass\n",
    "class QuestionAnalysis:\n",
    "    question_analysis: str\n",
    "    deep_web_search: bool  \n",
    "    video_processing: bool \n",
    "    audio_prcessing: bool \n",
    "    image_processing: bool \n",
    "    structured_data_processing: bool    \n",
    "    unstructered_data_processing: bool  \n",
    "    code_math_writing: bool \n",
    "\n",
    "def classify_question(question: str, attachments: List[str] = None) -> QuestionAnalysis:\n",
    "    \"\"\"\n",
    "    Use LLM to analyze a question and determine what capabilities/steps are needed.\n",
    "    \n",
    "    Args:\n",
    "     question (str): text of the question \n",
    "     attachments (str): list of files that are related to the question \n",
    "\n",
    "    Returns: \n",
    "        QuestionAnalysis: dataclass that describes what answering this question requires \n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments mentioned: {', '.join(attachments)}\"\n",
    "    \n",
    "    classification_prompt = f\"\"\"\n",
    "You are a highly intelligent routing agent. Your primary function is to analyze a user's question and determine the precise capabilities required to answer it accurately and efficiently.\n",
    "\n",
    "Question: {question}\n",
    "Attachments: {attachment_info}\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided question and determine the most logical and efficient plan to answer it using the capabilities listed below. Your analysis must be detailed in a step-by-step plan and then summarized in a series of boolean flags.\n",
    "\n",
    "**Capabilities:**\n",
    "- `deep_web_search`: Executing iterative search queries to find websources that answer the query the best, i.e., find and evaluate web content. This steps also often includes SIMPLE synthesize/understanding information from web pages. Use this for questions requiring up-to-date or niche knowledge.\n",
    "- `video_processing`: Processing a video file to analyze its content, typically by extracting frames for image analysis or audio for transcription/analysis.\n",
    "- `audio_processing`: Processing an audio file to transcribe speech, identify sounds, or analyze acoustic properties.\n",
    "- `image_processing`: Visually analyzing an image to identify objects, read text, or understand its content.\n",
    "- `structured_data_processing`: Analyzing, querying, or visualizing data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `unstructured_data_processing`: Performing detailed analysis on a provided block of raw text or multiple (retrieved) documents (e.g., summarization, sentiment analysis, entity extraction, processing multiple pieces of text). This is for analyzing *provided* text.\n",
    "- `code_math_writing`: Generating or executing code, solving mathematical problems, or performing complex computations.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze and Plan:** First, create a clear, direct and concise description of AI capabilities needed to answer this question, put the answer in the `question_analysis` field.\n",
    "2.  **Select Minimum Capabilities:** Based on your description/plan, set the corresponding boolean flags to `true`. Only activate the capabilities that are *absolutely necessary* for your plan. For example, a simple fact-lookup might not require `unstructured_data_processing` on top of `deep_web_search`.\n",
    "3.  **Ensure Consistency:** The capabilities mentioned in your `question_analysis` text MUST perfectly match the boolean flags set to `true`.\n",
    "4.  **Respond in JSON:** Your entire output must be in the exact JSON format specified below.\n",
    "\n",
    "**Better Examples:**\n",
    "\n",
    "Question: \"What is the boiling point of water at sea level?\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"This is a direct factual query. It requires a single deep web search to look up a well-known scientific constant.\",\n",
    "    \"deep_web_search\": \"true\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Summarize the attached meeting notes for me.\" (with a .txt file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"The user has provided a text document and wants a summary. This requires unstructured data processing to read the attached text and generate a concise summary of its key points.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"false\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"false\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"true\",\n",
    "    \"code_math_writing\": \"false\"\n",
    "}}\n",
    "\n",
    "Question: \"Count the number of cars in this video and plot their positions on a heatmap.\" (with a video file attached)\n",
    "Analysis:\n",
    "{{\n",
    "    \"question_analysis\": \"This is a multi-step task. First, it requires video processing to extract frames from the attached video file. Second, it needs image processing to be run on those frames to detect and count objects identified as 'cars' and log their coordinates. Finally, it requires code and mathematical computations to aggregate these coordinates and generate a heatmap visualization.\",\n",
    "    \"deep_web_search\": \"false\",\n",
    "    \"video_processing\": \"true\",\n",
    "    \"audio_prcessing\": \"false\",\n",
    "    \"image_processing\": \"true\",\n",
    "    \"structured_data_processing\": \"false\",\n",
    "    \"unstructered_data_processing\": \"false\",\n",
    "    \"code_math_writing\": \"true\"\n",
    "}}\n",
    "\n",
    "---\n",
    "**Begin Analysis**\n",
    "\n",
    "Question: \"{question}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"question_analysis\": \"Breakdown of what this question requires, including all necessary capabilities and processes\",\n",
    "    \"deep_web_search\": \"true\"/\"false\",\n",
    "    \"video_processing\": \"true\"/\"false\",\n",
    "    \"audio_prcessing\": \"true\"/\"false\",\n",
    "    \"image_processing\": \"true\"/\"false\",\n",
    "    \"structured_data_processing\": \"true\"/\"false\",\n",
    "    \"unstructered_data_processing\": \"true\"/\"false\",\n",
    "    \"code_math_writing\": \"true\"/\"false\"\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(classification_prompt)\n",
    "\n",
    "    response_text = response.text.strip()\n",
    "    result = json.loads(response_text)\n",
    "    return QuestionAnalysis(\n",
    "        question_analysis=result[\"question_analysis\"],\n",
    "        deep_web_search=result[\"deep_web_search\"],\n",
    "        video_processing=result[\"video_processing\"], \n",
    "        audio_prcessing=result[\"audio_prcessing\"], \n",
    "        image_processing=result[\"image_processing\"], \n",
    "        structured_data_processing=result[\"structured_data_processing\"],    \n",
    "        unstructered_data_processing=result[\"unstructered_data_processing\"],  \n",
    "        code_math_writing=result[\"code_math_writing\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def parse_into_list(text:str, separator: str = \"|\") -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return text.strip().strip(\"[]\").split(separator)\n",
    "\n",
    "\n",
    "def question_to_query(question: str) -> list[str]:\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a set of general search engine queries for the following question: \"{question}\". \n",
    "    \n",
    "    Make sure that:\n",
    "    - Your output is a list separated by \"|\" sing and nothing else\n",
    "    - Give maximum of two options where each query should be uniquely phrased \n",
    "    - Never use double or single quates anywhere in the answer\n",
    "    - Do not mention any specific website where the information should be searched\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"question_to_query prompt: \", prompt)\n",
    "    # Send this to an llm \n",
    "    query_responses = llm.complete(prompt)\n",
    "    \n",
    "    # Parse the response \n",
    "    return parse_into_list(query_responses.text)\n",
    "\n",
    "# @tool\n",
    "def duckduckgo_search(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No search results found for '{query}'\"\n",
    "\n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(results, 1):\n",
    "                title = result.get('title', 'No title')\n",
    "                body = result.get('body', 'No description')\n",
    "                href = result.get(WEB_SOURCE_URL_KEY, 'No URL')\n",
    "                \n",
    "                formatted_results.append({\"search_order\": i, \"web_page_title\": title, \"web_page_summary\": body, \"url\": href})\n",
    "            return formatted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {e}\"\n",
    "\n",
    "def drop_non_unique_dicts(lst:list, unique_key:str=WEB_SOURCE_URL_KEY):\n",
    "    seen_hrefs = set()\n",
    "    result_list = []\n",
    "    \n",
    "    for item in lst:\n",
    "        href = item.get(unique_key)\n",
    "        \n",
    "        if href not in seen_hrefs:\n",
    "            result_list.append(item)\n",
    "            seen_hrefs.add(href)\n",
    "            \n",
    "    return result_list\n",
    "\n",
    "def web_search_question(question: str):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Disambiguate the question into a query\n",
    "    candidate_queries = question_to_query(question)\n",
    "    \n",
    "    print(\"Candidate queries for the question are: \", candidate_queries)\n",
    "\n",
    "    # Search for relevant sources \n",
    "    sources = list()\n",
    "    for query in candidate_queries:\n",
    "        sources_query = duckduckgo_search(query)\n",
    "        sources.extend(sources_query)\n",
    "    \n",
    "    print(\"Found resources are: \", sources)\n",
    "    return drop_non_unique_dicts(sources)\n",
    "\n",
    "\n",
    "def extract_text_with_links(raw_html: str, base_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, converting hyperlinks to Markdown format.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        base_url (str): Base URL to resolve relative links (e.g., Wikipedia base)\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text with hyperlinks in Markdown format: [text](url)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert all <a> tags to Markdown-style links\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link_text = a_tag.get_text(separator=\" \", strip=True)\n",
    "            if not link_text:  # Skip if link has no text\n",
    "                a_tag.replace_with(\"\")\n",
    "                continue\n",
    "                \n",
    "            href = a_tag['href']\n",
    "            \n",
    "            # Resolve relative URLs\n",
    "            if href.startswith('/'):\n",
    "                full_url = urllib.parse.urljoin(base_url, href)\n",
    "            else:\n",
    "                full_url = href\n",
    "\n",
    "            # Replace the <a> tag with Markdown link\n",
    "            markdown_link = f\"[{link_text}]({full_url})\"\n",
    "            a_tag.replace_with(markdown_link)\n",
    "        \n",
    "        # Extract all text (now with Markdown links)\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text from raw HTML, removing hyperlinks and unwanted elements.\n",
    "    \n",
    "    Args:\n",
    "        raw_html (str): Raw HTML content\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted plain text with no hyperlinks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "\n",
    "        # Extract all visible text\n",
    "        text = soup.get_text(separator=\" \")\n",
    "\n",
    "        # Clean up whitespace: remove extra spaces and newlines\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_base_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract base URL (scheme + netloc) from a full URL.\n",
    "    Example:\n",
    "        Input:  https://en.wikipedia.org/wiki/Prussia\n",
    "        Output: https://en.wikipedia.org\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "def download_text_content(url: str):\n",
    "    \"\"\"\n",
    "    Download and process content from approved sources.\n",
    "    \"\"\"\n",
    "    text_content = None\n",
    "    \n",
    "    try:\n",
    "        print(f\"⬇️ Starting to download: {url}\")\n",
    "        with urllib.request.urlopen(url, timeout=30) as response:\n",
    "            html_content = response.read()\n",
    "            text_content = extract_clean_text(html_content) #extract_text_with_links(html_content, base_url=get_base_url(url))         \n",
    "    # ToDo: create custom errors    \n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"❌ URL Error for {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error for {url}: {e}\")\n",
    "    \n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_text_with_relation_to_question(question:str, text: str) -> str:\n",
    "    # Analyse the content and see if the query answer is in there \n",
    "\n",
    "    pprint(f\"==========================\\nPAGE CONTENT:  {text} \\n\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert AI research assistant performing a deep analysis of a webpage.\n",
    "    Your goal is to determine if the provided `page_content` conclusively answers the user's `query`.\n",
    "\n",
    "    --- USER QUERY ---\n",
    "    {question}\n",
    "\n",
    "    --- PAGE CONTENT ---\n",
    "    {text}\n",
    "\n",
    "    --- INSTRUCTIONS ---\n",
    "    Carefully analyze the `page_content` in relation to the `user_query` and follow this process:\n",
    "    1.  **Decision:** Based on your analysis, decide on one of two possible outcomes:\n",
    "        a. **ANSWER_FOUND:** The content provides a direct, complete, and trustworthy answer to the query.\n",
    "        b. **DEAD_END:** The content is irrelevant, low-quality, or does not contain any useful hyperlinks to continue the search.\n",
    "    2.  **Output:** Respond with a single JSON object with two keys: \"status\" and \"result\". The content of \"result\" depends on the status.\n",
    "\n",
    "        - If the status is **\"ANSWER_FOUND\"**:\n",
    "        The \"result\" key must contain a string with the final, extracted answer.\n",
    "        Example: {{\"status\": \"ANSWER_FOUND\", \"result\": \"The final answer is X.\"}}\n",
    "\n",
    "        - If the status is **\"DEAD_END\"**:\n",
    "        The \"result\" key must contain a string briefly explaining why the search has hit a dead end on this page.\n",
    "        Example: {{\"status\": \"DEAD_END\", \"result\": \"Page is a login form with no relevant information.\"}}\n",
    "\n",
    "    **CRITICAL RULE:** Your entire response must be a single, valid JSON object and nothing else. Do not add any text before or after the JSON.\n",
    "    \"\"\"\n",
    "    analysis_response  = llm.complete(prompt)\n",
    "    return    analysis_response.text\n",
    "\n",
    "def synthesize_answer(question: str, url: str) -> str:\n",
    "    \"\"\"Analyzes a webpage's content to find an answer or promising hyperlinks.\n",
    "\n",
    "    This function is a core reasoning step in an iterative web search agent.\n",
    "    It assesses if the text content of a given URL contains a conclusive answer\n",
    "    to the user's query. \n",
    "\n",
    "    Args:\n",
    "        query (str): The original, high-level question the user is trying to\n",
    "            answer. This provides the context for the analysis.\n",
    "        url (str): The URL of the specific webpage to download and analyze.\n",
    "\n",
    "    Returns:\n",
    "        List of str where str is a JSON-formatted string representing one of three outcomes:\n",
    "        1.  **Answer Found**: A JSON object containing the status \"ANSWER_FOUND\"\n",
    "            and the extracted answer.\n",
    "            Example: '{\"status\": \"ANSWER_FOUND\", \"result\": \"Mercedes Sosa released 2 studio albums between 2000 and 2009.\"}'\n",
    "\n",
    "        2.  **Dead End**: A JSON object with the status \"DEAD_END\" if the page\n",
    "            is irrelevant and offers no promising links for follow-up.\n",
    "            Example: '{\"status\": \"DEAD_END\", \"result\": \"The page content is not relevant to the query and contains no useful links.\"}'\n",
    "        each element of the (json) represents where the answer was found on a particular web page text chunk.\n",
    "    \"\"\"\n",
    "    # Download the content of the page \n",
    "    page_content = download_text_content(url)\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=4000,        \n",
    "        chunk_overlap=500,     \n",
    "        separator=\".\"            \n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(page_content)\n",
    "    print(f\"Splitted text in deep_web_analysis in {len(chunks)} chunks.\")\n",
    "\n",
    "    analysis_responses = list()\n",
    "    for i, chunk in enumerate(chunks):   \n",
    "        print(\"Starting chunk: \", i)     \n",
    "        result_inspection = inspect_text_with_relation_to_question(question, chunk)\n",
    "        analysis_responses.append(result_inspection)\n",
    "        print(\"Finished chunk: \", i) \n",
    "\n",
    "    return analysis_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\"\n",
    "\n",
    "search_results = web_search_question(question) \n",
    "search_results\n",
    "\n",
    "for search_result in search_results: \n",
    "    page_analysis_result = synthesize_answer(question, search_result[\"url\"])\n",
    "    print(page_analysis_result)\n",
    "    break\n",
    "# res = synthesize_answer(question, \"https://en.wikipedia.org/w/index.php?title=Mercedes_Sosa&action=edit&section=6\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image analysis with Llava \n",
    "\n",
    "Llava does not usually provide accurate and correct results for specific queries related to images (e.g., tasks like counting object or describing what text is on the image). It can however describe in very very general terms what is on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Analyzer Tool with LLaVA Integration\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from typing import Literal\n",
    "\n",
    "@tool\n",
    "def image_analyzer_llava(image_path: str, task: str = Literal[\"describe\", \"text\"]) -> str:\n",
    "    \"\"\"\n",
    "    Analyze images using local LLaVA instance - describe content, analyze chess positions, read text, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return f\"Image file not found: {image_path}\"\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get basic image information\n",
    "        width, height = image.size\n",
    "        mode = image.mode\n",
    "        format_type = image.format\n",
    "        file_size = os.path.getsize(image_path)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Convert image to base64 for API transmission\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Create task-specific prompts\n",
    "        if task == \"describe\":\n",
    "            prompt = \"Describe what you see in this image in detail.\"\n",
    "        elif task == \"text\":\n",
    "            prompt = \"Extract and read any text visible in this image.\"\n",
    "        else:\n",
    "            prompt = f\"Analyze this image for the following task: {task}\"\n",
    "        \n",
    "        # Placeholder for LLaVA API call\n",
    "        llava_response = send_to_llava(img_base64, prompt)\n",
    "        \n",
    "        basic_info = f\"Image: {file_name}\\nSize: {width}x{height}\\nMode: {mode}\\nFormat: {format_type}\\nFile size: {file_size} bytes\"\n",
    "        \n",
    "        return f\"{basic_info}\\n\\nLLaVA Analysis:\\n{llava_response}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image '{image_path}': {e}\"\n",
    "\n",
    "def send_to_llava(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder function to send image to local LLaVA instance.\n",
    "    Replace this with actual API call to your LLaVA server.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Placeholder for actual LLaVA API integration\n",
    "        # This would typically be a POST request to localhost:11434 or similar\n",
    "        \n",
    "        # Example of what the actual implementation might look like:\n",
    "        import requests\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_base64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", \n",
    "                               json=payload, \n",
    "                               timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No response from LLaVA\")\n",
    "        else:\n",
    "            return f\"LLaVA API error: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with LLaVA: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured (tabular) data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Processor Tool\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "@tool\n",
    "def file_processor(file_path: str, file_type: str = \"auto\") -> str:\n",
    "    \"\"\"\n",
    "    Process various file types - Excel files, CSV files, Parquet files, text files, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "        \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            # Process Excel files\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Excel file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Process CSV files with automatic delimiter detection\n",
    "            def detect_delimiter(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sample = f.read(1024)\n",
    "                    sniffer = csv.Sniffer()\n",
    "                    delimiter = sniffer.sniff(sample).delimiter\n",
    "                    return delimiter\n",
    "            \n",
    "            try:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                \n",
    "                # Basic analysis\n",
    "                shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "                columns_info = f\"Columns: {list(df.columns)}\"\n",
    "                delimiter_info = f\"Detected delimiter: '{delimiter}'\"\n",
    "                \n",
    "                # Calculate totals for numeric columns\n",
    "                numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                totals_info = \"\"\n",
    "                if len(numeric_cols) > 0:\n",
    "                    totals = df[numeric_cols].sum()\n",
    "                    totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "                \n",
    "                sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "                \n",
    "                return f\"CSV file: {file_name}\\nFile size: {file_size} bytes\\n{delimiter_info}\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "            \n",
    "            except Exception as csv_error:\n",
    "                # Fallback to text processing if CSV parsing fails\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                line_count = len(content.split('\\n'))\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                \n",
    "                return f\"CSV file (read as text due to parsing error): {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nError: {csv_error}\\n\\nContent preview:\\n{preview}\"\n",
    "        \n",
    "        elif file_path.endswith('.parquet'):\n",
    "            # Process Parquet files\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Parquet file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        else:\n",
    "            # Read as text file for all other formats (including Python files)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            \n",
    "            line_count = len(content.split('\\n'))\n",
    "            word_count = len(content.split())\n",
    "            char_count = len(content)\n",
    "            \n",
    "            preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "            \n",
    "            file_type_desc = \"Python file\" if file_path.endswith('.py') else \"Text file\"\n",
    "            \n",
    "            return f\"{file_type_desc}: {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nWords: {word_count}\\nCharacters: {char_count}\\n\\nContent preview:\\n{preview}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file '{file_path}': {e}\"\n",
    "\n",
    "print(\"File processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
