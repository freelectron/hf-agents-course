{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Additional tools\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "\n",
    "print(\"Generalist Agent v1 - Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_RETRIES = 3\n",
    "TEMP_DIR = \"./temp_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classification System\n",
    "@dataclass\n",
    "class QuestionAnalysis:\n",
    "    question_analysis: str  # Detailed breakdown of what the question requires\n",
    "    requires_files: bool\n",
    "    requires_web: bool\n",
    "    requires_computation: bool\n",
    "\n",
    "def classify_question(question: str, attachments: List[str] = None) -> QuestionAnalysis:\n",
    "    \"\"\"\n",
    "    Use LLM to analyze a question and determine what capabilities/steps are needed.\n",
    "    \"\"\"\n",
    "    if attachments is None:\n",
    "        attachments = []\n",
    "    \n",
    "    # Create attachment info string\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments mentioned: {', '.join(attachments)}\"\n",
    "    \n",
    "    classification_prompt = f\"\"\"You are an expert question analyzer for a generalist AI agent. Analyze the following question and break down exactly what capabilities and steps are needed to answer it.\n",
    "\n",
    "Question: \"{question}\"{attachment_info}\n",
    "\n",
    "Your task is to provide a detailed analysis of what this question requires. Consider these capabilities:\n",
    "- Web research (Wikipedia, general searches, specific databases)\n",
    "- Video analysis (YouTube download, content extraction, transcription)\n",
    "- Audio processing (MP3 transcription, voice analysis)\n",
    "- Image analysis (visual content, chess positions, diagrams)\n",
    "- File processing (Python execution, Excel analysis, data processing)\n",
    "- Text manipulation (string operations, linguistic analysis)\n",
    "- Mathematical computation (calculations, set theory, logic)\n",
    "- Specialized domain knowledge (botany, chemistry, sports statistics, etc.)\n",
    "- Multi-step research (cross-referencing, complex information gathering)\n",
    "\n",
    "Instructions:\n",
    "1. Provide a detailed question_analysis that breaks down EXACTLY what steps/capabilities are needed\n",
    "2. If multiple functionalities are required, describe them all in the analysis\n",
    "3. Determine if files, web access, or computation are needed\n",
    "\n",
    "Example:\n",
    "Question: \"What is the price trend of NVIDIA stock?\"\n",
    "Analysis: \"Looking up current and historical NVIDIA stock price data from financial websites, analyzing the price patterns over time, potentially creating visualizations of trends\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"question_analysis\": \"Detailed step-by-step breakdown of what this question requires, including all necessary capabilities and processes\",\n",
    "    \"requires_files\": true/false,\n",
    "    \"requires_web\": true/false, \n",
    "    \"requires_computation\": true/false\n",
    "}}\n",
    "\n",
    "Focus on being comprehensive - if a question needs multiple steps or capabilities, describe them all clearly.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.complete(classification_prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        response_text = response.text.strip()\n",
    "        if response_text.startswith('```json'):\n",
    "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        result = json.loads(response_text)\n",
    "        \n",
    "        return QuestionAnalysis(\n",
    "            question_analysis=result[\"question_analysis\"],\n",
    "            requires_files=result[\"requires_files\"],\n",
    "            requires_web=result[\"requires_web\"],\n",
    "            requires_computation=result[\"requires_computation\"]\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification: {e}\")\n",
    "        # Fallback to simple classification\n",
    "        return QuestionAnalysis(\n",
    "            question_analysis=f\"General research and analysis required for: {question}\",\n",
    "            requires_files=bool(attachments),\n",
    "            requires_web=True,\n",
    "            requires_computation=False\n",
    "        )\n",
    "\n",
    "print(\"LLM-based question analysis system implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Tool using DuckDuckGo\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No search results found for '{query}'\"\n",
    "            \n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(results, 1):\n",
    "                title = result.get('title', 'No title')\n",
    "                body = result.get('body', 'No description')\n",
    "                href = result.get('href', 'No URL')\n",
    "                \n",
    "                formatted_results.append(f\"{i}. {title}\\n   {body}\\n   URL: {href}\")\n",
    "            \n",
    "            return f\"Web search results for '{query}':\\n\\n\" + \"\\n\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {e}\"\n",
    "\n",
    "print(\"Web search tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Processor Tool\n",
    "import yt_dlp\n",
    "import whisper\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def video_processor(url: str, task: str = \"transcribe\") -> str:\n",
    "    \"\"\"\n",
    "    Process YouTube videos - download, extract audio, transcribe, or analyze content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure yt-dlp for audio extraction\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': f'{TEMP_DIR}/%(title)s.%(ext)s',\n",
    "            'extract_flat': False,\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'wav',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "        }\n",
    "        \n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            # Extract info first\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            video_title = info.get('title', 'Unknown')\n",
    "            duration = info.get('duration', 0)\n",
    "            \n",
    "            # Check if video is too long (limit to 10 minutes for transcription)\n",
    "            if task == \"transcribe\" and duration > 600:\n",
    "                return f\"Video '{video_title}' is too long ({duration}s) for transcription. Limit: 10 minutes.\"\n",
    "            \n",
    "            # Download the video\n",
    "            ydl.download([url])\n",
    "            \n",
    "        if task == \"transcribe\":\n",
    "            # Use whisper for transcription\n",
    "            model = whisper.load_model(\"base\")\n",
    "            \n",
    "            # Find the downloaded audio file\n",
    "            audio_file = None\n",
    "            for file in os.listdir(TEMP_DIR):\n",
    "                if video_title.replace('/', '_') in file and file.endswith('.wav'):\n",
    "                    audio_file = os.path.join(TEMP_DIR, file)\n",
    "                    break\n",
    "            \n",
    "            if not audio_file:\n",
    "                return f\"Could not find downloaded audio file for '{video_title}'\"\n",
    "            \n",
    "            result = model.transcribe(audio_file)\n",
    "            \n",
    "            # Clean up the audio file\n",
    "            os.remove(audio_file)\n",
    "            \n",
    "            return f\"Video: {video_title}\\nDuration: {duration}s\\nTranscription: {result['text']}\"\n",
    "        \n",
    "        return f\"Video '{video_title}' processed. Task: {task}, Duration: {duration}s\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing video: {e}\"\n",
    "\n",
    "print(\"Video processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Analyzer Tool with LLaVA Integration\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "@tool\n",
    "def image_analyzer(image_path: str, task: str = \"describe\") -> str:\n",
    "    \"\"\"\n",
    "    Analyze images using local LLaVA instance - describe content, analyze chess positions, read text, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return f\"Image file not found: {image_path}\"\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get basic image information\n",
    "        width, height = image.size\n",
    "        mode = image.mode\n",
    "        format_type = image.format\n",
    "        file_size = os.path.getsize(image_path)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Convert image to base64 for API transmission\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Create task-specific prompts\n",
    "        if task == \"chess\":\n",
    "            prompt = \"Analyze this chess position. What pieces are on the board? Whose turn is it? What would be the best next move?\"\n",
    "        elif task == \"describe\":\n",
    "            prompt = \"Describe what you see in this image in detail.\"\n",
    "        elif task == \"text\":\n",
    "            prompt = \"Extract and read any text visible in this image.\"\n",
    "        else:\n",
    "            prompt = f\"Analyze this image for the following task: {task}\"\n",
    "        \n",
    "        # Placeholder for LLaVA API call\n",
    "        llava_response = send_to_llava(img_base64, prompt)\n",
    "        \n",
    "        basic_info = f\"Image: {file_name}\\nSize: {width}x{height}\\nMode: {mode}\\nFormat: {format_type}\\nFile size: {file_size} bytes\"\n",
    "        \n",
    "        return f\"{basic_info}\\n\\nLLaVA Analysis:\\n{llava_response}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image '{image_path}': {e}\"\n",
    "\n",
    "def send_to_llava(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder function to send image to local LLaVA instance.\n",
    "    Replace this with actual API call to your LLaVA server.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Placeholder for actual LLaVA API integration\n",
    "        # This would typically be a POST request to localhost:11434 or similar\n",
    "        \n",
    "        # Example of what the actual implementation might look like:\n",
    "        \"\"\"\n",
    "        import requests\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_base64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", \n",
    "                               json=payload, \n",
    "                               timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No response from LLaVA\")\n",
    "        else:\n",
    "            return f\"LLaVA API error: {response.status_code}\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # For now, return placeholder response\n",
    "        return f\"[PLACEHOLDER] LLaVA would analyze the image with prompt: '{prompt}'. Image data size: {len(image_base64)} characters.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with LLaVA: {e}\"\n",
    "\n",
    "print(\"Image analyzer tool with LLaVA integration implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Processor Tool\n",
    "import whisper\n",
    "\n",
    "@tool\n",
    "def audio_processor(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process audio files - transcribe MP3 or other audio formats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Audio file not found: {file_path}\"\n",
    "        \n",
    "        # Load whisper model\n",
    "        model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # Transcribe the audio\n",
    "        result = model.transcribe(file_path)\n",
    "        \n",
    "        # Get file info\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        return f\"Audio file: {file_name}\\nFile size: {file_size} bytes\\nTranscription: {result['text']}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio file '{file_path}': {e}\"\n",
    "\n",
    "print(\"Audio processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Processor Tool\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "@tool\n",
    "def file_processor(file_path: str, file_type: str = \"auto\") -> str:\n",
    "    \"\"\"\n",
    "    Process various file types - Excel files, CSV files, Parquet files, text files, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "        \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            # Process Excel files\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Excel file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Process CSV files with automatic delimiter detection\n",
    "            def detect_delimiter(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sample = f.read(1024)\n",
    "                    sniffer = csv.Sniffer()\n",
    "                    delimiter = sniffer.sniff(sample).delimiter\n",
    "                    return delimiter\n",
    "            \n",
    "            try:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                \n",
    "                # Basic analysis\n",
    "                shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "                columns_info = f\"Columns: {list(df.columns)}\"\n",
    "                delimiter_info = f\"Detected delimiter: '{delimiter}'\"\n",
    "                \n",
    "                # Calculate totals for numeric columns\n",
    "                numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                totals_info = \"\"\n",
    "                if len(numeric_cols) > 0:\n",
    "                    totals = df[numeric_cols].sum()\n",
    "                    totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "                \n",
    "                sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "                \n",
    "                return f\"CSV file: {file_name}\\nFile size: {file_size} bytes\\n{delimiter_info}\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "            \n",
    "            except Exception as csv_error:\n",
    "                # Fallback to text processing if CSV parsing fails\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                line_count = len(content.split('\\n'))\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                \n",
    "                return f\"CSV file (read as text due to parsing error): {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nError: {csv_error}\\n\\nContent preview:\\n{preview}\"\n",
    "        \n",
    "        elif file_path.endswith('.parquet'):\n",
    "            # Process Parquet files\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Parquet file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        else:\n",
    "            # Read as text file for all other formats (including Python files)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            \n",
    "            line_count = len(content.split('\\n'))\n",
    "            word_count = len(content.split())\n",
    "            char_count = len(content)\n",
    "            \n",
    "            preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "            \n",
    "            file_type_desc = \"Python file\" if file_path.endswith('.py') else \"Text file\"\n",
    "            \n",
    "            return f\"{file_type_desc}: {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nWords: {word_count}\\nCharacters: {char_count}\\n\\nContent preview:\\n{preview}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file '{file_path}': {e}\"\n",
    "\n",
    "print(\"File processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
