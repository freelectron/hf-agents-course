{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Additional tools\n",
    "import yt_dlp\n",
    "import whisper\n",
    "from PIL import Image\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'whisper' from '/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/whisper.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 300\n",
    "CONTEXT_WINDOW = 80000\n",
    "MODEL_NAME = \"qwen2:7b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_RETRIES = 3\n",
    "TEMP_DIR = \"./temp_files\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classification System\n",
    "@dataclass\n",
    "class QuestionAnalysis:\n",
    "    question_analysis: str\n",
    "    deep_web_search: bool  \n",
    "    video_processing: bool \n",
    "    audio_prcessing: bool \n",
    "    image_processing: bool \n",
    "    structured_data_processing: bool    \n",
    "    unstructered_data_processing: bool  \n",
    "    code_math_writing: bool \n",
    "\n",
    "def classify_question(question: str, attachments: List[str] = None) -> QuestionAnalysis:\n",
    "    \"\"\"\n",
    "    Use LLM to analyze a question and determine what capabilities/steps are needed.\n",
    "    \n",
    "    Args:\n",
    "     question (str): text of the question \n",
    "     attachments (str): list of files that are related to the question \n",
    "\n",
    "    Returns: \n",
    "        QuestionAnalysis: dataclass that describes what answering this question requires \n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments mentioned: {', '.join(attachments)}\"\n",
    "    \n",
    "    classification_prompt = f\"\"\"You are an expert question analyzer for a generalist AI agent. Analyze the following question and break down exactly what capabilities are needed to answer it.\n",
    "\n",
    "Question: {question}\n",
    "Attachments: {attachment_info}\n",
    "\n",
    "Your task is to provide a detailed analysis of what this question requires. Consider these capabilities:\n",
    "- Deep web research: executing search queries on the web, iterating over results and trying to find the answer in multiple web search steps\n",
    "- Video analysis: downloading and processing a video, extracting its content in the form of image frames or audio, and answering queries about the video \n",
    "- Audio processing: dowloading and processing audio, analysing the voice and the content\n",
    "- Image analysis: visual analysis of an image \n",
    "- Structured data processing: analysis of table data or json documents, answering queries against them and/or visualising the data  \n",
    "- Unstructered data processing: raw text analys, summarisation, entity or sentiment extraction  \n",
    "- Code and mathematical computations: creating or executing code, processing and/or analysing code bits, creating and/or computing mathematical expressions\n",
    "\n",
    "Instructions:\n",
    "1. Provide what capabilities would need to be triggered to answer the question\n",
    "2. If multiple functionalities are required, describe them briefly and suggest an order\n",
    "3. Determine if files, web access, or computation are needed\n",
    "\n",
    "Example:\n",
    "Question: \"What is the price trend of NVIDIA stock?\"\n",
    "Analysis: \"Deep we research: looking up current and historical NVIDIA stock price data from financial websites, finding financial articles that describe the trand. Unstructered data processing: analyzing the text data found before.\"\n",
    "\n",
    "Question: \"What is the artistic style of this video https://www.youtube.com/watch?v=fLu080UX25o\"?\n",
    "Analysis: \"Process the video: download it, capture fragments of it, evaluate the fragments\"  \n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"question_analysis\": \"Breakdown of what this question requires, including all necessary capabilities and processes\",\n",
    "    \"deep_web_search\": True/False,  \n",
    "    \"video_processing\": True/False, \n",
    "    \"audio_prcessing\": True/False, \n",
    "    \"image_processing\": True/False, \n",
    "    \"structured_data_processing\": True/False,    \n",
    "    \"unstructered_data_processing\": True/False,  \n",
    "    \"code_math_writing\": True/False, \n",
    "}}\n",
    "\n",
    "Focus on being precise and concise - if a question needs multiple steps or capabilities, describe them all clearly without being verbose.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.complete(classification_prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        response_text = response.text.strip()\n",
    "        if response_text.startswith('```json'):\n",
    "            response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        result = json.loads(response_text)\n",
    "        \n",
    "        return QuestionAnalysis(\n",
    "            question_analysis=result[\"question_analysis\"],\n",
    "            requires_files=result[\"requires_files\"],\n",
    "            requires_web=result[\"requires_web\"],\n",
    "            requires_computation=result[\"requires_computation\"]\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification: {e}\")\n",
    "        # Fallback to simple classification\n",
    "        return QuestionAnalysis(\n",
    "            question_analysis=f\"General research and analysis required for: {question}\",\n",
    "            requires_files=bool(attachments),\n",
    "            requires_web=True,\n",
    "            requires_computation=False\n",
    "        )\n",
    "\n",
    "print(\"LLM-based question analysis system implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Tool using DuckDuckGo\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "@tool\n",
    "def web_search(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using DuckDuckGo search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No search results found for '{query}'\"\n",
    "            \n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(results, 1):\n",
    "                title = result.get('title', 'No title')\n",
    "                body = result.get('body', 'No description')\n",
    "                href = result.get('href', 'No URL')\n",
    "                \n",
    "                formatted_results.append(f\"{i}. {title}\\n   {body}\\n   URL: {href}\")\n",
    "            \n",
    "            return f\"Web search results for '{query}':\\n\\n\" + \"\\n\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {e}\"\n",
    "\n",
    "print(\"Web search tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Analyzer Tool with LLaVA Integration\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from typing import Literal\n",
    "\n",
    "@tool\n",
    "def image_analyzer_llava(image_path: str, task: str = Literal[\"describe\", \"text\"]) -> str:\n",
    "    \"\"\"\n",
    "    Analyze images using local LLaVA instance - describe content, analyze chess positions, read text, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return f\"Image file not found: {image_path}\"\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get basic image information\n",
    "        width, height = image.size\n",
    "        mode = image.mode\n",
    "        format_type = image.format\n",
    "        file_size = os.path.getsize(image_path)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Convert image to base64 for API transmission\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        # Create task-specific prompts\n",
    "        if task == \"describe\":\n",
    "            prompt = \"Describe what you see in this image in detail.\"\n",
    "        elif task == \"text\":\n",
    "            prompt = \"Extract and read any text visible in this image.\"\n",
    "        else:\n",
    "            prompt = f\"Analyze this image for the following task: {task}\"\n",
    "        \n",
    "        # Placeholder for LLaVA API call\n",
    "        llava_response = send_to_llava(img_base64, prompt)\n",
    "        \n",
    "        basic_info = f\"Image: {file_name}\\nSize: {width}x{height}\\nMode: {mode}\\nFormat: {format_type}\\nFile size: {file_size} bytes\"\n",
    "        \n",
    "        return f\"{basic_info}\\n\\nLLaVA Analysis:\\n{llava_response}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing image '{image_path}': {e}\"\n",
    "\n",
    "def send_to_llava(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder function to send image to local LLaVA instance.\n",
    "    Replace this with actual API call to your LLaVA server.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Placeholder for actual LLaVA API integration\n",
    "        # This would typically be a POST request to localhost:11434 or similar\n",
    "        \n",
    "        # Example of what the actual implementation might look like:\n",
    "        import requests\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llava\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_base64],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", \n",
    "                               json=payload, \n",
    "                               timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No response from LLaVA\")\n",
    "        else:\n",
    "            return f\"LLaVA API error: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with LLaVA: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Processor Tool\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "@tool\n",
    "def file_processor(file_path: str, file_type: str = \"auto\") -> str:\n",
    "    \"\"\"\n",
    "    Process various file types - Excel files, CSV files, Parquet files, text files, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"File not found: {file_path}\"\n",
    "        \n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        if file_path.endswith(('.xlsx', '.xls')):\n",
    "            # Process Excel files\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Excel file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        elif file_path.endswith('.csv'):\n",
    "            # Process CSV files with automatic delimiter detection\n",
    "            def detect_delimiter(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    sample = f.read(1024)\n",
    "                    sniffer = csv.Sniffer()\n",
    "                    delimiter = sniffer.sniff(sample).delimiter\n",
    "                    return delimiter\n",
    "            \n",
    "            try:\n",
    "                delimiter = detect_delimiter(file_path)\n",
    "                df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                \n",
    "                # Basic analysis\n",
    "                shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "                columns_info = f\"Columns: {list(df.columns)}\"\n",
    "                delimiter_info = f\"Detected delimiter: '{delimiter}'\"\n",
    "                \n",
    "                # Calculate totals for numeric columns\n",
    "                numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                totals_info = \"\"\n",
    "                if len(numeric_cols) > 0:\n",
    "                    totals = df[numeric_cols].sum()\n",
    "                    totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "                \n",
    "                sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "                \n",
    "                return f\"CSV file: {file_name}\\nFile size: {file_size} bytes\\n{delimiter_info}\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "            \n",
    "            except Exception as csv_error:\n",
    "                # Fallback to text processing if CSV parsing fails\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                line_count = len(content.split('\\n'))\n",
    "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "                \n",
    "                return f\"CSV file (read as text due to parsing error): {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nError: {csv_error}\\n\\nContent preview:\\n{preview}\"\n",
    "        \n",
    "        elif file_path.endswith('.parquet'):\n",
    "            # Process Parquet files\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Basic analysis\n",
    "            shape_info = f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\"\n",
    "            columns_info = f\"Columns: {list(df.columns)}\"\n",
    "            \n",
    "            # Calculate totals for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            totals_info = \"\"\n",
    "            if len(numeric_cols) > 0:\n",
    "                totals = df[numeric_cols].sum()\n",
    "                totals_info = f\"Column totals: {totals.to_dict()}\"\n",
    "            \n",
    "            sample_data = f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "            \n",
    "            return f\"Parquet file: {file_name}\\nFile size: {file_size} bytes\\n{shape_info}\\n{columns_info}\\n{totals_info}\\n\\n{sample_data}\"\n",
    "        \n",
    "        else:\n",
    "            # Read as text file for all other formats (including Python files)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Try with different encoding if UTF-8 fails\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            \n",
    "            line_count = len(content.split('\\n'))\n",
    "            word_count = len(content.split())\n",
    "            char_count = len(content)\n",
    "            \n",
    "            preview = content[:500] + \"...\" if len(content) > 500 else content\n",
    "            \n",
    "            file_type_desc = \"Python file\" if file_path.endswith('.py') else \"Text file\"\n",
    "            \n",
    "            return f\"{file_type_desc}: {file_name}\\nFile size: {file_size} bytes\\nLines: {line_count}\\nWords: {word_count}\\nCharacters: {char_count}\\n\\nContent preview:\\n{preview}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file '{file_path}': {e}\"\n",
    "\n",
    "print(\"File processor tool implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
