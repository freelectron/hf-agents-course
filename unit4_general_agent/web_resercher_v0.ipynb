{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d93e9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "269717c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: Hello! How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "\n",
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46bac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a task, determine a step-by-step action plan of what needs to be done to accomplish this task and output the answer/result. \n",
    "    The most important actions that are taken: \n",
    "     1. Define the goal: what result is asked to be produced.\n",
    "     2. List the steps: provide a short explanation for each action that needs to be taken.       \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert project planner. Your task is to create a concise, step-by-step action plan to accomplish the user's goal.\n",
    "\n",
    "User's Goal:\n",
    "---\n",
    "{task}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "1. Clarify the Core Objective: Start by rephrasing the user's goal as a single, clear, and specific objective.\n",
    "2. Develop a Chronological Action Plan: Break down the objective into a logical sequence of high-level steps.\n",
    "\n",
    "Guiding Principles for the Plan:\n",
    "- Tool-Agnostic: Focus on the action required, not the specific tool to perform it (e.g., use \"Gather data on market trends\" instead of \"Search Google for market trends\").\n",
    "- Information First: The initial step should almost always be to gather and analyze the necessary information before taking further action.\n",
    "- S.M.A.R. Steps: Each step must be Specific, Measurable, Achievable, and Relevant. The focus is on the logical sequence, not specific deadlines.\n",
    "- Concise: Include only the critical steps needed to reach the objective.\n",
    "\n",
    "Example Output Format (ALWAS **JSON** ):\n",
    "{{\n",
    "  \"objective\": \"Plan and execute a one-day offsite event for a team of 10 people focused on team building and strategic planning.\",\n",
    "  \"plan\": [\n",
    "    \"Gather requirements including budget, potential dates, and key goals for the offsite from team leadership\",\n",
    "    \"Research and shortlist suitable venues and activity options that fit the budget and goals\",\n",
    "    \"Create a detailed agenda and budget proposal for approval\",\n",
    "    \"Book the selected venue, catering, and activities upon approval\",\n",
    "    \"Send out official invitations and manage attendee confirmations and dietary requirements\",\n",
    "    \"Finalize all logistical details and communicate the full itinerary to the team\"\n",
    "  ]\n",
    "}}\n",
    "where\n",
    "  \"objective\" 's value in the json is a clear, one-sentence summary of the end goal,\n",
    "  \"plan\" 's value in the json is a list **ALWAYS SEPARATED BY PYTHON NEWLINE CHARCTER** like \n",
    "  [\n",
    "    A short explanation of the first logical step\", \n",
    "    A short explanation of the next step that follows from the first\",\n",
    "    And so on...\"\n",
    "  ]\n",
    "\"\"\"\n",
    "    task_response = llm.complete(prompt)\n",
    "\n",
    "    return task_response.text\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    question: str\n",
    "    objective: str\n",
    "    plan: list[str]\n",
    "\n",
    "\n",
    "def define_task(task:str) -> str: \n",
    "    task_plan_response = create_plan(task)\n",
    "\n",
    "    # Assume llm outputs smth json-like with the correct keys.\n",
    "    result = json.loads(task_plan_response)\n",
    "\n",
    "    return Task(\n",
    "      question=task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97e53eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Literal\n",
    "from typing import Literal, NewType\n",
    "from enum import Enum\n",
    "\n",
    "class AgentCapability(str, Enum):\n",
    "    DEEP_WEB_SEARCHER = \"deep_web_search\"\n",
    "    VIDEO_PROCESSOR = \"video_processing\"  \n",
    "    AUDIO_PROCESSOR = \"audio_processing\"\n",
    "    IMAGE_PROCESSOR = \"image_processing\"\n",
    "    STRUCTURED_DATA_PROCESSOR = \"structured_data_processing\"\n",
    "    UNSTRUCTURED_DATA_PROCESSOR = \"unstructured_data_processing\"\n",
    "    CODE_MATH_WRITTER = \"code_math_writing\"\n",
    "\n",
    "@dataclass\n",
    "class CapabilityPlan:\n",
    "    \"\"\"A structured plan outlining the sequence of capabilities and actions.\"\"\"\n",
    "    subplan: List[Dict[str, str | str]]\n",
    "\n",
    "def create_capability_plan(task: str, attachments: List[str] = None) -> CapabilityPlan:\n",
    "    \"\"\"\n",
    "    Analyzes a task and generates a sequential execution plan using available capabilities.\n",
    "\n",
    "    Args:\n",
    "        task (str): The description of the task to be performed.\n",
    "        attachments (list[str]): A list of file names related to the task.\n",
    "\n",
    "    Returns:\n",
    "        CapabilityPlan: A dataclass containing the ordered list of sub-tasks.\n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments provided: {', '.join(attachments)}\"\n",
    "\n",
    "    # TODO: is this fine to map capability to an agent one-to-one? \n",
    "    planning_prompt = f\"\"\"\n",
    "You are a highly intelligent planning agent. Your primary function is to analyze a user's task and create a precise, step-by-step execution plan using a predefined set of capabilities.\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided task and create a sequential plan to accomplish it. The plan should be a list of steps, where each step defines the capability to use and the specific activity to perform.\n",
    "\n",
    "**Capabilities:**\n",
    "- `{AgentCapability.DEEP_WEB_SEARCHER.value}`: Find, evaluate, and download web content (e.g., articles, documents). This capability is for search and downloading web resources only, not for processing the content or getting any answers on the content.\n",
    "- `{AgentCapability.VIDEO_PROCESSOR.value}`: Download video, extract frames or audio from a video file for further analysis.\n",
    "- `{AgentCapability.AUDIO_PROCESSOR.value}`: Download audio, transcribe speech, identify sounds, or analyze properties of an audio file.\n",
    "- `{AgentCapability.IMAGE_PROCESSOR.value}`: Download image, analyze an image to identify objects, read text (OCR), or understand its content.\n",
    "- `{AgentCapability.STRUCTURED_DATA_PROCESSOR.value}`: Analyze, query, or visualize data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}`: Analyze, summarize, extract information from, or answer questions about raw text or documents (e.g., PDFs, TXT files, retrieved web content).\n",
    "- `{AgentCapability.CODE_MATH_WRITTER.value}`: Generate or execute code, solve mathematical problems, or perform complex logical operations and computations.\n",
    "\n",
    "Instructions:\n",
    "Deconstruct the Task -> Assign Capabilities for each step -> Define the Activity for each step (i.e.,write a clear and concise description of the specific action to be performed using the chosen capability)\n",
    "\n",
    "Example 1: Simple Fact Lookup\n",
    "Task: \"What is the boiling point of water at sea level?\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.DEEP_WEB_SEARCHER.value}\",\n",
    "      \"activity\": \"Search for the boiling point of water at sea level\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}\",\n",
    "      \"activity\": \"Analyze the downloaded web resources and find the reference to the boiling point temperature.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Example 2: Multi-step Information Retrieval and Analysis\n",
    "\n",
    "Task: \"Find the Q2 2025 earnings report for NVIDIA and tell me what their 'Gaming' division revenue was.\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.DEEP_WEB_SEARCHER.value}\",\n",
    "      \"activity\": \"Search for and download NVIDIA's official Q2 2025 earnings report document and download it.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}\",\n",
    "      \"activity\": \"Analyze the downloaded earnings report to find and extract the revenue figure for the 'Gaming' division.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "---\n",
    "Begin Plan Generation\n",
    "\n",
    "Task: \"{task}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(planning_prompt)\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    result = json.loads(response_text)\n",
    "    return CapabilityPlan(subplan=result[\"subplan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e36bb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(question='Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.',\n",
      "objective=\"Identify the actor who played Ray in the Polish-language version of 'Everybody Loves Raymond' and determine their role\n",
      "in the film 'Magda M.'\", plan=[\"Gather information about the Polish-language cast of 'Everybody Loves Raymond'\", 'Identify the\n",
      "actor who played Ray in this adaptation', \"Research the actor's roles in other films, focusing on 'Magda M.'\"])\n"
     ]
    }
   ],
   "source": [
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "task = define_task(question)\n",
    "pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a126e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapabilityPlan(subplan=[{'capability': 'deep_web_search', 'activity': \"Search for and download information about the Polish-\n",
      "language cast of 'Everybody Loves Raymond'.\"}, {'capability': 'unstructured_data_processing', 'activity': 'Analyze the downloaded\n",
      "information to gather details about the Polish-language cast members.'}])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the plan\n",
    "# Get a single task of the plan together with the objective \n",
    "current_plan_step = task.plan[0]\n",
    "task_step = f\"You need to FOCUS ON: {current_plan_step}. When your global objective which you SHOULD NOT FOCUS ON: {task.objective}.\"  \n",
    "\n",
    "# What is required for this to be executed?\n",
    "capability_plan = create_capability_plan(task=task_step)\n",
    "pprint(capability_plan.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2cebb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# web_search_agent \n",
    "##################\n",
    "\n",
    "LINK_KEY = \"link\"\n",
    "\n",
    "@dataclass\n",
    "class WebResource:\n",
    "    \"\"\"A unified dataclass for handling a web resource.\n",
    "\n",
    "    Attributes:\n",
    "        content: The main text content of the web page. Can be None if not yet downloaded.\n",
    "        link: The unique URL for the resource.\n",
    "        metadata: A dictionary containing additional information, such as search result data.\n",
    "    \"\"\"\n",
    "    content: Optional[str]\n",
    "    link: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "\n",
    "def question_to_queries(question: str, max_queries: int = 2) -> List[str]:\n",
    "    \"\"\"Converts a user question into a list of optimized search engine queries.\n",
    "\n",
    "    Note:\n",
    "        This function requires a Large Language Model (LLM) to generate queries.\n",
    "        The `llm.complete()` call is a placeholder for your model's inference logic.\n",
    "\n",
    "    Args:\n",
    "        question: The user's input question.\n",
    "        max_queries: The maximum number of search queries to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of string queries optimized for a search engine.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a list of general search engine queries for the following question: \"{question}\".\n",
    "\n",
    "    Make sure that:\n",
    "    - Your output is a list separated by a \"|\" character and nothing else.\n",
    "    - You provide a MAXIMUM of {max_queries} search engine queries.\n",
    "    - Each query is SHORT and precise.\n",
    "\n",
    "    Example Output:\n",
    "    Large urban population areas in Europe|Biggest cities in Europe\n",
    "    \"\"\"\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "\n",
    "    return response_text.strip().split(\"|\")\n",
    "\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 2) -> List[WebResource]:\n",
    "    \"\"\"Performs a DuckDuckGo search and returns results as WebResource objects.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string.\n",
    "        max_results: The maximum number of search results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, where 'content' is None and 'metadata'\n",
    "        contains the search result details.\n",
    "    \"\"\"\n",
    "    found_resources = []\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=max_results))\n",
    "        if not results:\n",
    "            print(f\"⚠️ No search results found for '{query}'\")\n",
    "            return []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            resource = WebResource(\n",
    "                content=None,\n",
    "                link=result.get('href', 'No URL'),\n",
    "                metadata={\n",
    "                    \"search_order\": i,\n",
    "                    \"web_page_title\": result.get('title', 'No title'),\n",
    "                    \"web_page_summary\": result.get('body', 'No description'),\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            found_resources.append(resource)\n",
    "\n",
    "    return found_resources\n",
    "\n",
    "\n",
    "def drop_non_unique_resources(resources: List[WebResource]) -> List[WebResource]:\n",
    "    \"\"\"Removes duplicate WebResource objects based on their 'link' attribute.\n",
    "\n",
    "    Args:\n",
    "        resources: A list of WebResource objects.\n",
    "\n",
    "    Returns:\n",
    "        A new list of WebResource objects with duplicates removed.\n",
    "    \"\"\"\n",
    "    seen_links = set()\n",
    "    unique_resources = []\n",
    "    for resource in resources:\n",
    "        if resource.link and resource.link not in seen_links:\n",
    "            unique_resources.append(resource)\n",
    "            seen_links.add(resource.link)\n",
    "    return unique_resources\n",
    "\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"Extracts clean, readable text from raw HTML content.\n",
    "\n",
    "    This function removes scripts, styles, navigation, and other non-content\n",
    "    elements, then cleans up whitespace.\n",
    "\n",
    "    Args:\n",
    "        raw_html: The raw HTML content of a webpage.\n",
    "\n",
    "    Returns:\n",
    "        The extracted and cleaned plain text.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    # Remove elements that typically do not contain main content\n",
    "    for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        element.decompose()\n",
    "\n",
    "    # Extract text and clean up whitespace\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    return ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "\n",
    "# This is a new helper function to safely encode URLs\n",
    "def _encode_url_path(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Encodes the path and query components of a URL to be URL-safe.\n",
    "    \n",
    "    This prevents errors when a URL contains non-ASCII characters.\n",
    "    \"\"\"\n",
    "    # Split the URL into its components\n",
    "    scheme, netloc, path, query, fragment = urlsplit(url)\n",
    "    \n",
    "    # Encode the path and query parts\n",
    "    path = quote(path)\n",
    "    query = quote(query)\n",
    "    \n",
    "    # Reassemble the URL\n",
    "    return urlunsplit((scheme, netloc, path, query, fragment))\n",
    "\n",
    "\n",
    "import httpx\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "\n",
    "def download_content(resource: WebResource) -> WebResource:\n",
    "    \"\"\"\n",
    "    Downloads the HTML from a resource's link and populates its 'content' field.\n",
    "    This version includes robust URL encoding and safe error printing.\n",
    "    \"\"\"\n",
    "    if not resource.link or not resource.link.startswith('http'):\n",
    "        return resource\n",
    "\n",
    "    # For this example, let's assume _encode_url_path is defined elsewhere\n",
    "    # encoded_link = _encode_url_path(resource.link)\n",
    "    response = httpx.get(resource.link, timeout=15)\n",
    "    charset = response.encoding or 'utf-8'\n",
    "    html_bytes = response.content\n",
    "    html_content = html_bytes.decode(charset)\n",
    "    resource.content = extract_clean_text(html_content)\n",
    "\n",
    "    return resource\n",
    "\n",
    "def web_search(question: str, links_per_query: int = 2) -> List[WebResource]:\n",
    "    \"\"\"Orchestrates the full web search process for a given question.\n",
    "\n",
    "    This process includes:\n",
    "    1. Converting the question into search queries.\n",
    "    2. Searching the web to find resources.\n",
    "    3. Downloading and extracting text content from each resource.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        links_per_query: The number of web links to retrieve for each search query.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, with their 'content' field populated.\n",
    "    \"\"\"\n",
    "    # 1. Generate search queries from the question\n",
    "    candidate_queries = question_to_queries(question)\n",
    "    print(f\"\\nGenerated queries: {candidate_queries}\")\n",
    "\n",
    "    # 2. Search for relevant sources for each query\n",
    "    all_sources = []\n",
    "    for query in candidate_queries:\n",
    "        sources_for_query = duckduckgo_search(query, links_per_query)\n",
    "        all_sources.extend(sources_for_query)\n",
    "\n",
    "    # 3. Filter out any duplicate resources found by different queries\n",
    "    unique_sources = drop_non_unique_resources(all_sources)\n",
    "    print(f\"\\nFound {len(unique_sources)} unique web resources.\")\n",
    "\n",
    "    # 4. Download content for each unique resource\n",
    "    final_resources = []\n",
    "    for source in unique_sources:\n",
    "        populated_resource = download_content(source)\n",
    "        if populated_resource.content: # Only keep resources where content was successfully downloaded\n",
    "            final_resources.append(populated_resource)\n",
    "\n",
    "    return final_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dfdb6877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated queries: ['Polish cast Everybody Loves Raymond', 'Everybody Loves Raymond Polish voices']\n",
      "\n",
      "Found 3 unique web resources.\n",
      "Finished processing\n",
      "https://en.wikipedia.org/wiki/Everybody_Loves_Raymond\n",
      "https://myflixer-to.tube/tv/watch-everybody-loves-raymond-movies-free-myflixer-38914\n",
      "https://en.wikipedia.org/wiki/Wszyscy_kochają_Romana\n"
     ]
    }
   ],
   "source": [
    "for capability_step in capability_plan.subplan:\n",
    "    match capability_step[\"capability\"]:\n",
    "        case AgentCapability.DEEP_WEB_SEARCHER:\n",
    "            web_resources = web_search(capability_step[\"activity\"])\n",
    "        case _:\n",
    "            print(\"Finished processing\")\n",
    "\n",
    "for web_resource in web_resources:\n",
    "    print(web_resource.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c67a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# unstructered_text_processor \n",
    "##################################\n",
    "\n",
    "def contruct_final_answer(task:str, context:str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    You are presented with a list of expert answers from different source that you need summarise.\n",
    "\n",
    "    LIST:\n",
    "    {context}\n",
    "    \n",
    "    Based **ONLY** on that list and without any addition assumptions from your side, perform the the task specified. \n",
    "    \n",
    "    TASK:\n",
    "    {task}\n",
    "    \n",
    "    Your answer should be in json format like so:\n",
    "    {{\n",
    "        \"answer\": <a single number, word of a phrase which si the answer to the question>,\n",
    "        \"clarification\": <very short mention of what the answer is based on>,\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "        - If the text contains the complete answer → put the exact answer in \"answer\".\n",
    "        - If the text contains no relevant information → put \"answer\": \"not found\".\n",
    "        - If the text contains some but not all information → put \"answer\": \"not found\".\n",
    "        - The \"clarification\" must mention the relevant part of the text and explain briefly.\n",
    "\n",
    "    Examples:\n",
    "    Q: \"Who won the 2022 FIFA World Cup?\"\n",
    "    {{\n",
    "    \"answer\": \"not found\",\n",
    "    \"clarification\": \"The text mentions the location of the tournament but not the winner.\"\n",
    "    }}\n",
    "    Q: \"How many colours there is in the rainbow\"\n",
    "    {{\n",
    "    \"answer\": \"12\",\n",
    "    \"clarification\": \"Red,Orange,Yellow,Chartreuse green,Green,Blue-green,Cyan,Azure,Violet,Purple,Magenta,Red\"\n",
    "    }}\n",
    "    Q:\"What's the name of Russian Santa?\"\n",
    "    {{\n",
    "    \"answer\": \"Ded Moroz\",\n",
    "    \"clarification\": \"Easter Slavic Father Frost\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    answer = llm.complete(prompt)\n",
    "\n",
    "    return answer.text\n",
    "\n",
    "def task_with_text_llm(task: str, text:str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Perform the instruction/task in the user's question. \n",
    "    Use only the information provided in the context. \n",
    "    \n",
    "    TASK \n",
    "    {task}\n",
    "\n",
    "    CONTEXT\n",
    "    {text}\n",
    "\n",
    "    **IMPORTANT** If the text does not include the SPECIFIC information about the task, output \"NOT FOUND\"\n",
    "    Output\n",
    "    \"\"\"\n",
    "\n",
    "    llm_result = llm.complete(prompt)\n",
    "    \n",
    "    return llm_result.text\n",
    "\n",
    "def text_process_llm(task: str, text:str):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=10000,        \n",
    "        chunk_overlap=500,     \n",
    "        separator=\"\"            \n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    responses = list()\n",
    "    for chunk in chunks: \n",
    "        answer_response = task_with_text_llm(task, chunk)\n",
    "\n",
    "        responses.append(answer_response)\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe325eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 47551, which is longer than the specified 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NOT FOUND',\n",
       " 'The Polish-language cast members of the TV series \"Wszyscy kochają Romana\" are:\\n\\n- Bartłomiej Kasprzykowski as Roman (Ray)\\n- Aneta Todorczuk-Perchuć as Dorota (Debra)\\n- Anna Seniuk as Maryla (Marie)\\n- Joachim Lamża as Zygmunt (Frank)\\n- Tede as Robert (Robert)']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [web_resource.content for web_resource in web_resources]\n",
    "text_process_llm(task=capability_plan.subplan[1][\"activity\"], text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df604f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on capabilities let an agent do the task \n",
    "\n",
    "# Combine task and objective and hand over the execution to the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's work flow:\n",
    "#  1. Compose query \n",
    "#  2. Do search & collect results \n",
    "#  3. Download and process results\n",
    "#  4. Save results \n",
    "#  5. Produce summery of how the data looks like and where to find it.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77346936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define agents \n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b058fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
