{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93e9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269717c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: Hello! How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)\n",
    "\n",
    "## TODO: Use ChatEngine\n",
    "# chat_model = SimpleChatEngine.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a task, determine a step-by-step action plan of what needs to be done to accomplish this task and output the answer/result. \n",
    "    The most important actions that are taken: \n",
    "     1. Define the goal: what result is asked to be produced.\n",
    "     2. List the steps: provide a short explanation for each action that needs to be taken.       \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert project planner. Your task is to create a concise, step-by-step action plan to accomplish the user's goal.\n",
    "\n",
    "User's Goal:\n",
    "---\n",
    "{task}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "1. Clarify the Core Objective: Start by rephrasing the user's goal as a single, clear, and specific objective.\n",
    "2. Develop a Chronological Action Plan: Break down the objective into a logical sequence of high-level steps.\n",
    "\n",
    "Guiding Principles for the Plan:\n",
    "- Tool-Agnostic: Focus on the action required, not the specific tool to perform it (e.g., use \"Gather data on market trends\" instead of \"Search Google for market trends\").\n",
    "- Information First: The initial step should almost always be to gather and analyze the necessary information before taking further action.\n",
    "- S.M.A.R. Steps: Each step must be Specific, Measurable, Achievable, and Relevant. The focus is on the logical sequence, not specific deadlines.\n",
    "- Concise: Include only the critical steps needed to reach the objective.\n",
    "\n",
    "Example Output Format (ALWAS **JSON** ):\n",
    "{{\n",
    "  \"objective\": \"Plan and execute a one-day offsite event for a team of 10 people focused on team building and strategic planning.\",\n",
    "  \"plan\": [\n",
    "    \"Gather requirements including budget, potential dates, and key goals for the offsite from team leadership\",\n",
    "    \"Research and shortlist suitable venues and activity options that fit the budget and goals\",\n",
    "    \"Create a detailed agenda and budget proposal for approval\",\n",
    "    \"Book the selected venue, catering, and activities upon approval\",\n",
    "    \"Send out official invitations and manage attendee confirmations and dietary requirements\",\n",
    "    \"Finalize all logistical details and communicate the full itinerary to the team\"\n",
    "  ]\n",
    "}}\n",
    "where\n",
    "  \"objective\" 's value in the json is a clear, one-sentence summary of the end goal,\n",
    "  \"plan\" 's value in the json is a list **ALWAYS SEPARATED BY PYTHON NEWLINE CHARCTER** like \n",
    "  [\n",
    "    A short explanation of the first logical step\", \n",
    "    A short explanation of the next step that follows from the first\",\n",
    "    And so on...\"\n",
    "  ]\n",
    "\"\"\"\n",
    "    task_response = llm.complete(prompt)\n",
    "\n",
    "    return task_response.text\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    question: str\n",
    "    objective: str\n",
    "    plan: list[str]\n",
    "\n",
    "\n",
    "def define_task(task:str) -> str: \n",
    "    task_plan_response = create_plan(task)\n",
    "\n",
    "    # Assume llm outputs smth json-like with the correct keys.\n",
    "    result = json.loads(task_plan_response)\n",
    "\n",
    "    return Task(\n",
    "      question=task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e53eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Literal\n",
    "from typing import Literal, NewType\n",
    "from enum import Enum\n",
    "\n",
    "class AgentCapability(str, Enum):\n",
    "    DEEP_WEB_SEARCHER = \"deep_web_search\"\n",
    "    VIDEO_PROCESSOR = \"video_processing\"  \n",
    "    AUDIO_PROCESSOR = \"audio_processing\"\n",
    "    IMAGE_PROCESSOR = \"image_processing\"\n",
    "    STRUCTURED_DATA_PROCESSOR = \"structured_data_processing\"\n",
    "    UNSTRUCTURED_DATA_PROCESSOR = \"unstructured_data_processing\"\n",
    "    CODE_MATH_WRITTER = \"code_math_writing\"\n",
    "\n",
    "@dataclass\n",
    "class CapabilityPlan:\n",
    "    \"\"\"A structured plan outlining the sequence of capabilities and actions.\"\"\"\n",
    "    subplan: List[Dict[str, str | str]]\n",
    "\n",
    "def create_capability_plan(task: str, attachments: List[str] = None) -> CapabilityPlan:\n",
    "    \"\"\"\n",
    "    Analyzes a task and generates a sequential execution plan using available capabilities.\n",
    "\n",
    "    Args:\n",
    "        task (str): The description of the task to be performed.\n",
    "        attachments (list[str]): A list of file names related to the task.\n",
    "\n",
    "    Returns:\n",
    "        CapabilityPlan: A dataclass containing the ordered list of sub-tasks.\n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments provided: {', '.join(attachments)}\"\n",
    "\n",
    "    # TODO: is this fine to map capability to an agent one-to-one? \n",
    "    planning_prompt = f\"\"\"\n",
    "You are a highly intelligent planning agent. Your primary function is to analyze a user's task and create a precise, step-by-step execution plan using a predefined set of capabilities.\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided task and create a sequential plan to accomplish it. The plan should be a list of steps, where each step defines the capability to use and the specific activity to perform.\n",
    "\n",
    "**Capabilities:**\n",
    "- `{AgentCapability.DEEP_WEB_SEARCHER.value}`: Find, evaluate, and download web content (e.g., articles, documents). This capability is for search and downloading web resources only, not for processing the content or getting any answers on the content.\n",
    "- `{AgentCapability.VIDEO_PROCESSOR.value}`: Download video, extract frames or audio from a video file for further analysis.\n",
    "- `{AgentCapability.AUDIO_PROCESSOR.value}`: Download audio, transcribe speech, identify sounds, or analyze properties of an audio file.\n",
    "- `{AgentCapability.IMAGE_PROCESSOR.value}`: Download image, analyze an image to identify objects, read text (OCR), or understand its content.\n",
    "- `{AgentCapability.STRUCTURED_DATA_PROCESSOR.value}`: Analyze, query, or visualize data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}`: Analyze, summarize, extract information from, or answer questions about raw text or documents (e.g., PDFs, TXT files, retrieved web content).\n",
    "- `{AgentCapability.CODE_MATH_WRITTER.value}`: Generate or execute code, solve mathematical problems, or perform complex logical operations and computations.\n",
    "\n",
    "Instructions:\n",
    "Deconstruct the Task -> Assign Capabilities for each step -> Define the Activity for each step (i.e.,write a clear and concise description of the specific action to be performed using the chosen capability)\n",
    "\n",
    "Example 1: Simple Fact Lookup\n",
    "Task: \"What is the boiling point of water at sea level?\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.DEEP_WEB_SEARCHER.value}\",\n",
    "      \"activity\": \"Search for the boiling point of water at sea level\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}\",\n",
    "      \"activity\": \"Analyze the downloaded web resources and find the reference to the boiling point temperature.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Example 2: Multi-step Information Retrieval and Analysis\n",
    "\n",
    "Task: \"Find the Q2 2025 earnings report for NVIDIA and tell me what their 'Gaming' division revenue was.\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.DEEP_WEB_SEARCHER.value}\",\n",
    "      \"activity\": \"Search for and download NVIDIA's official Q2 2025 earnings report document and download it.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapability.UNSTRUCTURED_DATA_PROCESSOR.value}\",\n",
    "      \"activity\": \"Analyze the downloaded earnings report to find and extract the revenue figure for the 'Gaming' division.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "---\n",
    "Begin Plan Generation\n",
    "\n",
    "Task: \"{task}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(planning_prompt)\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    result = json.loads(response_text)\n",
    "    return CapabilityPlan(subplan=result[\"subplan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36bb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(question='Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.',\n",
      "objective=\"Identify and confirm the role played by the actor who portrayed Ray in the Polish-language version of 'Everybody Loves\n",
      "Raymond' in the film 'Magda M.'\", plan=[\"Gather information about the cast of the Polish-language adaptation of 'Everybody Loves\n",
      "Raymond'\", 'Identify the lead actor who played Ray in this adaptation', \"Research the filmography of this identified actor,\n",
      "focusing on their roles and projects after 'Everybody Loves Raymond'\", \"Locate specific details regarding the actor's involvement\n",
      "in 'Magda M.'\", 'Verify the role information from multiple reliable sources to ensure accuracy'])\n"
     ]
    }
   ],
   "source": [
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "task = define_task(question)\n",
    "pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a126e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapabilityPlan(subplan=[{'capability': 'deep_web_search', 'activity': \"Search for the cast information of the Polish-language\n",
      "adaptation of 'Everybody Loves Raymond'.\"}, {'capability': 'unstructured_data_processing', 'activity': 'Analyze the gathered web\n",
      "content to compile a list of actors and their roles in the Polish-language adaptation.'}])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the plan\n",
    "# Get a single task of the plan together with the objective \n",
    "current_plan_step = task.plan[0]\n",
    "task_step = f\"You need to FOCUS ON: {current_plan_step}. When your global objective which you SHOULD NOT FOCUS ON: {task.objective}.\"  \n",
    "\n",
    "# What is required for this to be executed?\n",
    "capability_plan = create_capability_plan(task=task_step)\n",
    "pprint(capability_plan.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# web_search_agent \n",
    "##################\n",
    "\n",
    "@dataclass\n",
    "class ContentResource:\n",
    "    \"\"\"A unified dataclass for handling any type of resource that is being used for a task (web document, pdf file ).\n",
    "\n",
    "    Attributes:\n",
    "        provided_by: who supplied the resource (user or an agent/tool call) # <=> as_answer_to\n",
    "        content: The main text content of the web page. Can be None if not yet downloaded.\n",
    "        link: The unique URL or file path for the resource.\n",
    "        metadata: A dictionary containing additional information, such as search result data.\n",
    "    \"\"\"\n",
    "    provided_by: str \n",
    "    content: Optional[str]\n",
    "    link: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class WebSearchResult: \n",
    "    link: str \n",
    "    metadata: dict\n",
    "\n",
    "def question_to_queries(question: str, max_queries: int = 2) -> List[str]:\n",
    "    \"\"\"Converts a user question into a list of optimized search engine queries.\n",
    "\n",
    "    Note:\n",
    "        This function requires a Large Language Model (LLM) to generate queries.\n",
    "        The `llm.complete()` call is a placeholder for your model's inference logic.\n",
    "\n",
    "    Args:\n",
    "        question: The user's input question.\n",
    "        max_queries: The maximum number of search queries to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of string queries optimized for a search engine.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a list of general search engine queries for the following question: \"{question}\".\n",
    "\n",
    "    Make sure that:\n",
    "    - Your output is a list separated by a \"|\" character and nothing else.\n",
    "    - You provide a MAXIMUM of {max_queries} search engine queries.\n",
    "    - Each query is SHORT and precise.\n",
    "\n",
    "    Example Output:\n",
    "    Large urban population areas in Europe|Biggest cities in Europe\n",
    "    \"\"\"\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "\n",
    "    return response_text.strip().split(\"|\")\n",
    "\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 2) -> List[WebSearchResult]:\n",
    "    \"\"\"Performs a DuckDuckGo search and returns results as WebResource objects.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string.\n",
    "        max_results: The maximum number of search results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, where 'content' is None and 'metadata'\n",
    "        contains the search result details.\n",
    "    \"\"\"\n",
    "    found_resources = []\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=max_results))\n",
    "        if not results:\n",
    "            print(f\"⚠️ No search results found for '{query}'\")\n",
    "            return []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            resource = WebSearchResult(\n",
    "                link=result.get('href', 'No URL'),\n",
    "                metadata={\n",
    "                    \"search_order\": i,\n",
    "                    \"web_page_title\": result.get('title', 'No title'),\n",
    "                    \"web_page_summary\": result.get('body', 'No description'),\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            found_resources.append(resource)\n",
    "\n",
    "    return found_resources\n",
    "\n",
    "\n",
    "def drop_non_unique_link(resources: List[ContentResource|WebSearchResult]) -> List[ContentResource|WebSearchResult]:\n",
    "    \"\"\"Removes duplicate WebResource objects based on their 'link' attribute.\n",
    "\n",
    "    Args:\n",
    "        resources: A list of WebResource objects.\n",
    "\n",
    "    Returns:\n",
    "        A new list of WebResource objects with duplicates removed.\n",
    "    \"\"\"\n",
    "    seen_links = set()\n",
    "    unique_resources = []\n",
    "    for resource in resources:\n",
    "        if resource.link and resource.link not in seen_links:\n",
    "            unique_resources.append(resource)\n",
    "            seen_links.add(resource.link)\n",
    "    return unique_resources\n",
    "\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"Extracts clean, readable text from raw HTML content.\n",
    "\n",
    "    This function removes scripts, styles, navigation, and other non-content\n",
    "    elements, then cleans up whitespace.\n",
    "\n",
    "    Args:\n",
    "        raw_html: The raw HTML content of a webpage.\n",
    "\n",
    "    Returns:\n",
    "        The extracted and cleaned plain text.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    # Remove elements that typically do not contain main content\n",
    "    for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        element.decompose()\n",
    "\n",
    "    # Extract text and clean up whitespace\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    return ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "\n",
    "import httpx\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "\n",
    "def download_content(resource: WebSearchResult) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the HTML from a resource's link and populates its 'content' field.\n",
    "    This version includes robust URL encoding and safe error printing.\n",
    "    \"\"\"\n",
    "    if not resource.link or not resource.link.startswith('http'):\n",
    "        return resource\n",
    "\n",
    "    # For this example, let's assume _encode_url_path is defined elsewhere\n",
    "    # encoded_link = _encode_url_path(resource.link)\n",
    "    response = httpx.get(resource.link, timeout=15)\n",
    "    charset = response.encoding or 'utf-8'\n",
    "    html_bytes = response.content\n",
    "    html_content = html_bytes.decode(charset)\n",
    "\n",
    "    return extract_clean_text(html_content)\n",
    "\n",
    "\n",
    "def web_search(question: str, links_per_query: int = 2) -> List[ContentResource]:\n",
    "    \"\"\"Orchestrates the full web search process for a given question.\n",
    "\n",
    "    This process includes:\n",
    "    1. Converting the question into search queries.\n",
    "    2. Searching the web to find resources.\n",
    "    3. Downloading and extracting text content from each resource.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        links_per_query: The number of web links to retrieve for each search query.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, with their 'content' field populated.\n",
    "    \"\"\"\n",
    "    # 1. Generate search queries from the question\n",
    "    candidate_queries = question_to_queries(question)\n",
    "    print(f\"\\nGenerated queries: {candidate_queries}\")\n",
    "\n",
    "    # 2. Search for relevant sources for each query\n",
    "    all_sources = []\n",
    "    for query in candidate_queries:\n",
    "        search_results_for_query = duckduckgo_search(query, links_per_query)\n",
    "        all_sources.extend(search_results_for_query)\n",
    "\n",
    "    # 3. Filter out any duplicate resources found by different queries\n",
    "    unique_search_results = drop_non_unique_link(all_sources)\n",
    "    print(f\"\\nFound {len(unique_search_results)} unique web resources.\")\n",
    "\n",
    "    # 4. Download content for each unique resource\n",
    "    final_resources = []\n",
    "    for search in unique_search_results:\n",
    "        content = download_content(search)\n",
    "        populated_resource = ContentResource(\n",
    "            provided_by=web_search.__name__,\n",
    "            content=content,\n",
    "            link=search.link,\n",
    "            metadata=search.metadata\n",
    "        )\n",
    "        if populated_resource.content: # Only keep resources where content was successfully downloaded\n",
    "            final_resources.append(populated_resource)\n",
    "\n",
    "    return final_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# unstructered_text_processor \n",
    "##################################\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShortAnswer:\n",
    "    \"\"\"A dataclass to hold a structured answer from the LLM.\n",
    "\n",
    "    Attributes:\n",
    "        answer: The direct answer to the task, or \"not found\" if unavailable.\n",
    "        clarification: A brief explanation of the context or reason for the answer.\n",
    "    \"\"\"\n",
    "    answer: str = \"not found\"\n",
    "    clarification: str = \"No information processed yet.\"\n",
    "\n",
    "\n",
    "def construct_short_answer(task: str, context: str) -> ShortAnswer:\n",
    "    \"\"\"Summarizes a context and provides a structured short answer to a task.\n",
    "\n",
    "    This function sends the context and a task to an LLM, asking it to synthesize\n",
    "    the information and return the result in a specific JSON format.\n",
    "\n",
    "    Note:\n",
    "        This function requires a configured LLM client, represented here as `llm`.\n",
    "\n",
    "    Args:\n",
    "        task: The specific question or instruction to be performed.\n",
    "        context: A string containing the text/information to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A ShortAnswer dataclass instance containing the answer and clarification.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are presented with a list of expert answers from different sources that you need to summarize.\n",
    "\n",
    "    LIST:\n",
    "    {context}\n",
    "\n",
    "    Based **ONLY** on that list and without any additional assumptions from your side, perform the task specified.\n",
    "\n",
    "    TASK:\n",
    "    {task}\n",
    "\n",
    "    Your answer should be in a valid JSON format like so:\n",
    "    {{\n",
    "        \"answer\": \"<a single number, word, or phrase which is the answer to the question>\",\n",
    "        \"clarification\": \"<a very short mention of what the answer is based on>\"\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "        - If the text contains the complete answer → put the exact answer in \"answer\".\n",
    "        - If the text contains no relevant information → put \"answer\": \"not found\".\n",
    "        - If the text contains some but not all information → put \"answer\": \"not found\".\n",
    "        - The \"clarification\" must mention the relevant part of the text and explain briefly.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    data = json.loads(response_text)\n",
    "    return ShortAnswer(\n",
    "        answer=data.get(\"answer\", \"did-not-parse\"),\n",
    "        clarification=data.get(\"clarification\", \"did-not-parse.\")\n",
    "    )\n",
    "\n",
    "def task_with_text_llm(task: str, text: str) -> str:\n",
    "    \"\"\"Performs a task on a single block of text using an LLM.\n",
    "\n",
    "    This function is a general-purpose processor that asks an LLM to execute\n",
    "    an instruction based only on the provided context.\n",
    "\n",
    "    Note:\n",
    "        This function requires a configured LLM client, represented here as `llm`.\n",
    "\n",
    "    Args:\n",
    "        task: The instruction to be performed (e.g., \"Summarize this text\").\n",
    "        text: The context text for the LLM to work with.\n",
    "\n",
    "    Returns:\n",
    "        The raw string response from the LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Perform the instruction/task in the user's question.\n",
    "    Use only the information provided in the context.\n",
    "\n",
    "    TASK:\n",
    "    {task}\n",
    "\n",
    "    CONTEXT:\n",
    "    {text}\n",
    "\n",
    "    **IMPORTANT**: If the text does not include the SPECIFIC information required for the task, output \"NOT FOUND\".\n",
    "    Otherwise, provide the direct answer.\n",
    "    \"\"\"\n",
    "    llm_result = llm.complete(prompt)\n",
    "    return llm_result.text\n",
    "\n",
    "\n",
    "def text_process_llm(task: str, text: str, chunk_size: int = 10000, chunk_overlap: int = 500) -> List[str]:\n",
    "    \"\"\"Splits a large text into chunks and processes each chunk with an LLM.\n",
    "\n",
    "    This is useful for analyzing documents that are too large to fit into a\n",
    "    single LLM context window. Each chunk is processed independently.\n",
    "\n",
    "    Args:\n",
    "        task: The task to perform on each chunk of text.\n",
    "        text: The entire body of text to be processed.\n",
    "        chunk_size: The maximum number of characters in each chunk.\n",
    "        chunk_overlap: The number of characters to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of string responses, with one response for each processed chunk.\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separator=\"\"  # An empty separator splits by character\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    responses = []\n",
    "    for chunk in chunks:\n",
    "        chunk_response = task_with_text_llm(task, chunk)\n",
    "        if \"NOT FOUND\" not in chunk_response:\n",
    "            responses.append(chunk_response)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "197af377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated queries: ['Polish Raymond cast', 'Everybody Loves Raymond Polish cast']\n",
      "\n",
      "Found 4 unique web resources.\n",
      "https://en.wikipedia.org/wiki/Wszyscy_kochają_Romana\n",
      "https://fandubdb.fandom.com/wiki/Raymond_(Polish)\n",
      "https://en.wikipedia.org/wiki/Everybody_Loves_Raymond\n",
      "https://myflixer-to.tube/tv/watch-everybody-loves-raymond-movies-free-myflixer-38914\n"
     ]
    }
   ],
   "source": [
    "for capability_step in capability_plan.subplan:\n",
    "    match capability_step[\"capability\"]:\n",
    "        case AgentCapability.DEEP_WEB_SEARCHER:\n",
    "            web_resources = web_search(capability_step[\"activity\"])\n",
    "        case AgentCapability.UNSTRUCTURED_DATA_PROCESSOR:\n",
    "            # TODO: how to pass things around? \n",
    "            pass\n",
    "        case _:\n",
    "            print(\"Finished processing\")\n",
    "\n",
    "for web_resource in web_resources:\n",
    "    print(web_resource.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe325eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [web_resource.content for web_resource in web_resources]\n",
    "text_process_llm(task=capability_plan.subplan[1][\"activity\"], text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state Object \n",
    "\n",
    "@dataclass\n",
    "class Attachments:\n",
    "    # provided_by: who supplied the resource (user or an agent/tool call) # <=> as_answer_to\n",
    "    provided_by: str \n",
    "    filepath: str \n",
    "    description: str\n",
    "\n",
    "class ExecuteState(TypedDict):\n",
    "    task: Task\n",
    "    attachments: list[Attachments]\n",
    "    resources: list[ContentResource]\n",
    "    tools_called: str  # tools called already to produce resources and attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "646fc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Create agents from tools \n",
    "web_search_tool  = FunctionTool.from_defaults(\n",
    "    fn=web_search,\n",
    "    name=\"web_search\",\n",
    "    description=web_search.__doc__,\n",
    ")\n",
    "\n",
    "web_searcher = FunctionAgent(\n",
    "    name=\"web_searcher\",\n",
    "    description=\"Searches the web for reources and downloads the contents of the search results\", \n",
    "    system_prompt=\"\",\n",
    "    tools=[web_search_tool],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be66a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n"
     ]
    }
   ],
   "source": [
    "#  \"How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\"\n",
    "resp = await web_searcher.run(\"Who of Yankees had the most walks in 1977 regular season?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(resp.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First node: create task \n",
    "\n",
    "# Second node: create capability plan for the first step \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df604f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on capabilities let an agent do the task \n",
    "\n",
    "# Combine task and objective and hand over the execution to the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's work flow:\n",
    "#  1. Compose query \n",
    "#  2. Do search & collect results \n",
    "#  3. Download and process results\n",
    "#  4. Save results \n",
    "#  5. Produce summery of how the data looks like and where to find it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77346936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define agents \n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b058fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
