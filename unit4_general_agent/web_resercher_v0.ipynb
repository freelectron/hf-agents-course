{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93e9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union, Type\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269717c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: Hello! How can I assist you today? Feel free to as...\n"
     ]
    }
   ],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)\n",
    "\n",
    "## TODO: Use ChatEngine\n",
    "# chat_model = SimpleChatEngine.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a task, determine a step-by-step action plan of what needs to be done to accomplish this task and output the answer/result. \n",
    "    The most important actions that are taken: \n",
    "     1. Define the goal: what result is asked to be produced.\n",
    "     2. List the steps: provide a short explanation for each action that needs to be taken.       \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert project planner. Your task is to create a concise, step-by-step action plan to accomplish the user's goal.\n",
    "\n",
    "User's Goal:\n",
    "---\n",
    "{task}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "1. Clarify the Core Objective: Start by rephrasing the user's goal as a single, clear, and specific objective.\n",
    "2. Develop a Chronological Action Plan: Break down the objective into a logical sequence of high-level steps.\n",
    "\n",
    "Guiding Principles for the Plan:\n",
    "- Tool-Agnostic: Focus on the action required, not the specific tool to perform it (e.g., use \"Gather data on market trends\" instead of \"Search Google for market trends\").\n",
    "- Information First: The initial step should almost always be to gather and analyze the necessary information before taking further action.\n",
    "- S.M.A.R. Steps: Each step must be Specific, Measurable, Achievable, and Relevant. The focus is on the logical sequence, not specific deadlines.\n",
    "- Concise: Include only the critical steps needed to reach the objective.\n",
    "\n",
    "Example Output Format (ALWAS **JSON** ):\n",
    "{{\n",
    "  \"objective\": \"Plan and execute a one-day offsite event for a team of 10 people focused on team building and strategic planning.\",\n",
    "  \"plan\": [\n",
    "    \"Gather requirements including budget, potential dates, and key goals for the offsite from team leadership\",\n",
    "    \"Research and shortlist suitable venues and activity options that fit the budget and goals\",\n",
    "    \"Create a detailed agenda and budget proposal for approval\",\n",
    "    \"Book the selected venue, catering, and activities upon approval\",\n",
    "    \"Send out official invitations and manage attendee confirmations and dietary requirements\",\n",
    "    \"Finalize all logistical details and communicate the full itinerary to the team\"\n",
    "  ]\n",
    "}}\n",
    "where\n",
    "  \"objective\" 's value in the json is a clear, one-sentence summary of the end goal,\n",
    "  \"plan\" 's value in the json is a list **ALWAYS SEPARATED BY PYTHON NEWLINE CHARCTER** like \n",
    "  [\n",
    "    A short explanation of the first logical step\", \n",
    "    A short explanation of the next step that follows from the first\",\n",
    "    And so on...\"\n",
    "  ]\n",
    "\"\"\"\n",
    "    task_response = llm.complete(prompt)\n",
    "\n",
    "    return task_response.text\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    question: str\n",
    "    objective: str\n",
    "    plan: list[str]\n",
    "\n",
    "\n",
    "def define_task(task:str) -> str: \n",
    "    task_plan_response = create_plan(task)\n",
    "\n",
    "    # Assume llm outputs smth json-like with the correct keys.\n",
    "    result = json.loads(task_plan_response)\n",
    "\n",
    "    return Task(\n",
    "      question=task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e53eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class BaseAgentCapability:\n",
    "    \"\"\"Base class for all agent capabilities.\"\"\"\n",
    "    name: str  # Defines that all subclasses should have a 'name' class attribute\n",
    "\n",
    "    def __init__(self, activity: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initializes the capability with a specific activity.\n",
    "\n",
    "        Args:\n",
    "            activity: The specific action to be performed.\n",
    "        \"\"\"\n",
    "        self.activity = activity\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Provides a clean string representation of the object.\"\"\"\n",
    "        # 'self.name' correctly accesses the class attribute from the instance\n",
    "        return f\"{self.__class__.__name__}(name='{self.name}', activity='{self.activity}')\"\n",
    "\n",
    "class AgentCapabilityDeepWebSearch(BaseAgentCapability):\n",
    "    \"\"\"Capability for performing a deep web search.\"\"\"\n",
    "    name = \"deep_web_search\"\n",
    "\n",
    "    def run(state: ExecutionState):\n",
    "        resource = web_search(stat)\n",
    "\n",
    "\n",
    "class AgentCapabilityVideoProcessor(BaseAgentCapability):\n",
    "    \"\"\"Capability for processing video files.\"\"\"\n",
    "    name = \"video_processing\"\n",
    "\n",
    "class AgentCapabilityAudioProcessor(BaseAgentCapability):\n",
    "    \"\"\"Capability for processing audio files.\"\"\"\n",
    "    name = \"audio_processing\"\n",
    "\n",
    "class AgentCapabilityImageProcessor(BaseAgentCapability):\n",
    "    \"\"\"Capability for processing image files.\"\"\"\n",
    "    name = \"image_processing\"\n",
    "\n",
    "class AgentCapabilityStructuredDataProcessor(BaseAgentCapability):\n",
    "    \"\"\"Capability for processing structured data.\"\"\"\n",
    "    name = \"structured_data_processing\"\n",
    "\n",
    "class AgentCapabilityUnstructuredDataProcessor(BaseAgentCapability):\n",
    "    \"\"\"Capability for processing unstructured text.\"\"\"\n",
    "    name = \"unstructured_data_processing\"\n",
    "\n",
    "class AgentCapabilityCodeMathWritter(BaseAgentCapability):\n",
    "    \"\"\"Capability for writing code or solving math problems.\"\"\"\n",
    "    name = \"code_math_writing\"\n",
    "\n",
    "class CapabilityPlan:\n",
    "    \"\"\"A structured plan outlining the sequence of capabilities and actions.\"\"\"\n",
    "    subplan: List[BaseAgentCapability]\n",
    "\n",
    "\n",
    "CAPABILITY_MAP: Dict[str, Type[BaseAgentCapability]] = {\n",
    "    AgentCapabilityDeepWebSearch.name: AgentCapabilityDeepWebSearch,\n",
    "    AgentCapabilityVideoProcessor.name: AgentCapabilityVideoProcessor,\n",
    "    AgentCapabilityAudioProcessor.name: AgentCapabilityAudioProcessor,\n",
    "    AgentCapabilityImageProcessor.name: AgentCapabilityImageProcessor,\n",
    "    AgentCapabilityStructuredDataProcessor.name: AgentCapabilityStructuredDataProcessor,\n",
    "    AgentCapabilityUnstructuredDataProcessor.name: AgentCapabilityUnstructuredDataProcessor,\n",
    "    AgentCapabilityCodeMathWritter.name: AgentCapabilityCodeMathWritter,\n",
    "}\n",
    "\n",
    "def json_to_capability_plan(json_data: dict) -> CapabilityPlan:\n",
    "    \"\"\"\n",
    "    Convert JSON response into a CapabilityPlan with proper capability objects.\n",
    "    \"\"\"\n",
    "    subplan = []\n",
    "    for step in json_data[\"subplan\"]:\n",
    "        cap_name = step[\"capability\"]\n",
    "        activity = step[\"activity\"]\n",
    "\n",
    "        if cap_name not in CAPABILITY_MAP:\n",
    "            raise ValueError(f\"Unknown capability: {cap_name}\")\n",
    "\n",
    "        cap_class = CAPABILITY_MAP[cap_name]\n",
    "        capability = cap_class(activity=activity)\n",
    "        subplan.append(capability)\n",
    "\n",
    "    return CapabilityPlan(subplan=subplan)\n",
    "\n",
    "@dataclass\n",
    "class CapabilityPlan:\n",
    "    \"\"\"A structured plan outlining the sequence of capabilities and actions.\"\"\"\n",
    "    subplan: List[Dict[str, str | str]]\n",
    "\n",
    "\n",
    "def determine_capabilities(task: str, attachments: List[str] = None) -> CapabilityPlan:\n",
    "    \"\"\"\n",
    "    Analyzes a task and generates a sequential execution plan using available capabilities.\n",
    "\n",
    "    Args:\n",
    "        task (str): The description of the task to be performed.\n",
    "        attachments (list[str]): A list of file names related to the task.\n",
    "\n",
    "    Returns:\n",
    "        CapabilityPlan: A dataclass containing the ordered list of sub-tasks.\n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments provided: {', '.join(attachments)}\"\n",
    "\n",
    "    # TODO: is this fine to map capability to an agent one-to-one? \n",
    "    planning_prompt = f\"\"\"\n",
    "You are a highly intelligent planning agent. Your primary function is to analyze a user's task and create a precise, step-by-step execution plan using a predefined set of capabilities.\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided task and create a sequential plan to accomplish it. The plan should be a list of steps, where each step defines the capability to use and the specific activity to perform.\n",
    "\n",
    "**Capabilities:**\n",
    "- `{AgentCapabilityDeepWebSearch.name}`: Find, evaluate, and download web content (e.g., articles, documents). This capability is for search and downloading web resources only, not for processing the content or getting any answers on the content.\n",
    "- `{AgentCapabilityVideoProcessor.name}`: Download video, extract frames or audio from a video file for further analysis.\n",
    "- `{AgentCapabilityAudioProcessor.name}`: Download audio, transcribe speech, identify sounds, or analyze properties of an audio file.\n",
    "- `{AgentCapabilityImageProcessor.name}`: Download image, analyze an image to identify objects, read text (OCR), or understand its content.\n",
    "- `{AgentCapabilityStructuredDataProcessor.name}`: Analyze, query, or visualize data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `{AgentCapabilityUnstructuredDataProcessor.name}`: Analyze, summarize, extract information from, or answer questions about raw text or documents (e.g., PDFs, TXT files, retrieved web content).\n",
    "- `{AgentCapabilityCodeMathWritter.name}`: Generate or execute code, solve mathematical problems, or perform complex logical operations and computations.\n",
    "\n",
    "Instructions:\n",
    "Deconstruct the Task -> Assign Capabilities for each step -> Define the Activity for each step (i.e.,write a clear and concise description of the specific action to be performed using the chosen capability)\n",
    "\n",
    "Example 1: Simple Fact Lookup\n",
    "Task: \"What is the boiling point of water at sea level?\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapabilityDeepWebSearch.name}\",\n",
    "      \"activity\": \"Search for the boiling point of water at sea level\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapabilityUnstructuredDataProcessor.name}\",\n",
    "      \"activity\": \"Analyze the downloaded web resources and find the reference to the boiling point temperature.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Example 2: Multi-step Information Retrieval and Analysis\n",
    "\n",
    "Task: \"Find the Q2 2025 earnings report for NVIDIA and tell me what their 'Gaming' division revenue was.\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapabilityDeepWebSearch.name}\",\n",
    "      \"activity\": \"Search for and download NVIDIA's official Q2 2025 earnings report document and download it.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"{AgentCapabilityUnstructuredDataProcessor.name}\",\n",
    "      \"activity\": \"Analyze the downloaded earnings report to find and extract the revenue figure for the 'Gaming' division.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "---\n",
    "Begin Plan Generation\n",
    "\n",
    "Task: \"{task}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(planning_prompt)\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    result = json.loads(response_text)\n",
    "\n",
    "    return json_to_capability_plan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e36bb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(question='Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.',\n",
      "objective=\"Identify and confirm the role played by the actor who portrayed Ray in the Polish-language version of 'Everybody Loves\n",
      "Raymond' in the film 'Magda M'.\", plan=[\"Gather information on the Polish cast of the TV show 'Everybody Loves Raymond'\",\n",
      "'Identify the actor who played Ray in the Polish adaptation', \"Research the filmography of the identified actor to find their role\n",
      "in 'Magda M'\", 'Verify the accuracy of the gathered information through reliable sources'])\n",
      "CapabilityPlan(subplan=[AgentCapabilityDeepWebSearch(name='deep_web_search', activity=\"Search for and download information about\n",
      "the Polish cast of the TV show 'Everybody Loves Raymond'.\"),\n",
      "AgentCapabilityUnstructuredDataProcessor(name='unstructured_data_processing', activity='Analyze the downloaded web resources to\n",
      "gather detailed information on the Polish cast members.')])\n"
     ]
    }
   ],
   "source": [
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "task = define_task(question)\n",
    "pprint(task.__str__())\n",
    "\n",
    "# Iterate over the plan: get a single task of the plan together with the objective \n",
    "current_plan_step = task.plan[0]\n",
    "task_step = f\"You need to FOCUS ON: {current_plan_step}. When your global objective which you SHOULD NOT FOCUS ON: {task.objective}.\"  \n",
    "capability_plan = determine_capabilities(task=task_step)\n",
    "pprint(capability_plan.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# web_search_agent \n",
    "##################\n",
    "\n",
    "@dataclass\n",
    "class ContentResource:\n",
    "    \"\"\"A unified dataclass for handling any type of resource that is being used for a task (web document, pdf file ).\n",
    "\n",
    "    Attributes:\n",
    "        provided_by: who supplied the resource (user or an agent/tool call) # <=> as_answer_to\n",
    "        content: The main text content of the web page. Can be None if not yet downloaded.\n",
    "        link: The unique URL or file path for the resource.\n",
    "        metadata: A dictionary containing additional information, such as search result data.\n",
    "    \"\"\"\n",
    "    provided_by: str \n",
    "    content: Optional[str]\n",
    "    link: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class WebSearchResult: \n",
    "    link: str \n",
    "    metadata: dict\n",
    "\n",
    "def question_to_queries(question: str, max_queries: int = 2) -> List[str]:\n",
    "    \"\"\"Converts a user question into a list of optimized search engine queries.\n",
    "\n",
    "    Note:\n",
    "        This function requires a Large Language Model (LLM) to generate queries.\n",
    "        The `llm.complete()` call is a placeholder for your model's inference logic.\n",
    "\n",
    "    Args:\n",
    "        question: The user's input question.\n",
    "        max_queries: The maximum number of search queries to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of string queries optimized for a search engine.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a list of general search engine queries for the following question: \"{question}\".\n",
    "\n",
    "    Make sure that:\n",
    "    - Your output is a list separated by a \"|\" character and nothing else.\n",
    "    - You provide a MAXIMUM of {max_queries} search engine queries.\n",
    "    - Each query is SHORT and precise.\n",
    "\n",
    "    Example Output:\n",
    "    Large urban population areas in Europe|Biggest cities in Europe\n",
    "    \"\"\"\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "\n",
    "    return response_text.strip().split(\"|\")\n",
    "\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 2) -> List[WebSearchResult]:\n",
    "    \"\"\"Performs a DuckDuckGo search and returns results as WebResource objects.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string.\n",
    "        max_results: The maximum number of search results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, where 'content' is None and 'metadata'\n",
    "        contains the search result details.\n",
    "    \"\"\"\n",
    "    found_resources = []\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=max_results))\n",
    "        if not results:\n",
    "            print(f\"⚠️ No search results found for '{query}'\")\n",
    "            return []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            resource = WebSearchResult(\n",
    "                link=result.get('href', 'No URL'),\n",
    "                metadata={\n",
    "                    \"search_order\": i,\n",
    "                    \"web_page_title\": result.get('title', 'No title'),\n",
    "                    \"web_page_summary\": result.get('body', 'No description'),\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            found_resources.append(resource)\n",
    "\n",
    "    return found_resources\n",
    "\n",
    "\n",
    "def drop_non_unique_link(resources: List[ContentResource|WebSearchResult]) -> List[ContentResource|WebSearchResult]:\n",
    "    \"\"\"Removes duplicate WebResource objects based on their 'link' attribute.\n",
    "\n",
    "    Args:\n",
    "        resources: A list of WebResource objects.\n",
    "\n",
    "    Returns:\n",
    "        A new list of WebResource objects with duplicates removed.\n",
    "    \"\"\"\n",
    "    seen_links = set()\n",
    "    unique_resources = []\n",
    "    for resource in resources:\n",
    "        if resource.link and resource.link not in seen_links:\n",
    "            unique_resources.append(resource)\n",
    "            seen_links.add(resource.link)\n",
    "    return unique_resources\n",
    "\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"Extracts clean, readable text from raw HTML content.\n",
    "\n",
    "    This function removes scripts, styles, navigation, and other non-content\n",
    "    elements, then cleans up whitespace.\n",
    "\n",
    "    Args:\n",
    "        raw_html: The raw HTML content of a webpage.\n",
    "\n",
    "    Returns:\n",
    "        The extracted and cleaned plain text.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    # Remove elements that typically do not contain main content\n",
    "    for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        element.decompose()\n",
    "\n",
    "    # Extract text and clean up whitespace\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    return ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "\n",
    "import httpx\n",
    "from urllib.parse import quote, urlsplit, urlunsplit\n",
    "\n",
    "def download_content(resource: WebSearchResult) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the HTML from a resource's link and populates its 'content' field.\n",
    "    This version includes robust URL encoding and safe error printing.\n",
    "    \"\"\"\n",
    "    if not resource.link or not resource.link.startswith('http'):\n",
    "        return resource\n",
    "\n",
    "    # For this example, let's assume _encode_url_path is defined elsewhere\n",
    "    # encoded_link = _encode_url_path(resource.link)\n",
    "    response = httpx.get(resource.link, timeout=15)\n",
    "    charset = response.encoding or 'utf-8'\n",
    "    html_bytes = response.content\n",
    "    html_content = html_bytes.decode(charset)\n",
    "\n",
    "    return extract_clean_text(html_content)\n",
    "\n",
    "def web_search(question: str, links_per_query: int = 2) -> List[ContentResource]:\n",
    "    \"\"\"Orchestrates the full web search process for a given question.\n",
    "\n",
    "    This process includes:\n",
    "    1. Converting the question into search queries.\n",
    "    2. Searching the web to find resources.\n",
    "    3. Downloading and extracting text content from each resource.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        links_per_query: The number of web links to retrieve for each search query.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, with their 'content' field populated.\n",
    "    \"\"\"\n",
    "    # 1. Generate search queries from the question\n",
    "    candidate_queries = question_to_queries(question)\n",
    "    print(f\"\\nGenerated queries: {candidate_queries}\")\n",
    "\n",
    "    # 2. Search for relevant sources for each query\n",
    "    all_sources = []\n",
    "    for query in candidate_queries:\n",
    "        search_results_for_query = duckduckgo_search(query, links_per_query)\n",
    "        all_sources.extend(search_results_for_query)\n",
    "\n",
    "    # 3. Filter out any duplicate resources found by different queries\n",
    "    unique_search_results = drop_non_unique_link(all_sources)\n",
    "    print(f\"\\nFound {len(unique_search_results)} unique web resources.\")\n",
    "\n",
    "    # 4. Download content for each unique resource\n",
    "    final_resources = []\n",
    "    for search in unique_search_results:\n",
    "        content = download_content(search)\n",
    "        populated_resource = ContentResource(\n",
    "            provided_by=web_search.__name__,\n",
    "            content=content,\n",
    "            link=search.link,\n",
    "            metadata=search.metadata\n",
    "        )\n",
    "        if populated_resource.content: # Only keep resources where content was successfully downloaded\n",
    "            final_resources.append(populated_resource)\n",
    "\n",
    "    return final_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# unstructered_text_processor \n",
    "##################################\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShortAnswer:\n",
    "    \"\"\"A dataclass to hold a structured answer from the LLM.\n",
    "\n",
    "    Attributes:\n",
    "        answer: The direct answer to the task, or \"not found\" if unavailable.\n",
    "        clarification: A brief explanation of the context or reason for the answer.\n",
    "    \"\"\"\n",
    "    answer: str = \"not found\"\n",
    "    clarification: str = \"No information processed yet.\"\n",
    "\n",
    "\n",
    "def construct_short_answer(task: str, context: str) -> ShortAnswer:\n",
    "    \"\"\"Summarizes a context and provides a structured short answer to a task.\n",
    "\n",
    "    This function sends the context and a task to an LLM, asking it to synthesize\n",
    "    the information and return the result in a specific JSON format.\n",
    "\n",
    "    Note:\n",
    "        This function requires a configured LLM client, represented here as `llm`.\n",
    "\n",
    "    Args:\n",
    "        task: The specific question or instruction to be performed.\n",
    "        context: A string containing the text/information to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A ShortAnswer dataclass instance containing the answer and clarification.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are presented with a list of expert answers from different sources that you need to summarize.\n",
    "\n",
    "    LIST:\n",
    "    {context}\n",
    "\n",
    "    Based **ONLY** on that list and without any additional assumptions from your side, perform the task specified.\n",
    "\n",
    "    TASK:\n",
    "    {task}\n",
    "\n",
    "    Your answer should be in a valid JSON format like so:\n",
    "    {{\n",
    "        \"answer\": \"<a single number, word, or phrase which is the answer to the question>\",\n",
    "        \"clarification\": \"<a very short mention of what the answer is based on>\"\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "        - If the text contains the complete answer → put the exact answer in \"answer\".\n",
    "        - If the text contains no relevant information → put \"answer\": \"not found\".\n",
    "        - If the text contains some but not all information → put \"answer\": \"not found\".\n",
    "        - The \"clarification\" must mention the relevant part of the text and explain briefly.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    data = json.loads(response_text)\n",
    "    return ShortAnswer(\n",
    "        answer=data.get(\"answer\", \"did-not-parse\"),\n",
    "        clarification=data.get(\"clarification\", \"did-not-parse.\")\n",
    "    )\n",
    "\n",
    "def task_with_text_llm(task: str, text: str) -> str:\n",
    "    \"\"\"Performs a task on a single block of text using an LLM.\n",
    "\n",
    "    This function is a general-purpose processor that asks an LLM to execute\n",
    "    an instruction based only on the provided context.\n",
    "\n",
    "    Note:\n",
    "        This function requires a configured LLM client, represented here as `llm`.\n",
    "\n",
    "    Args:\n",
    "        task: The instruction to be performed (e.g., \"Summarize this text\").\n",
    "        text: The context text for the LLM to work with.\n",
    "\n",
    "    Returns:\n",
    "        The raw string response from the LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Perform the instruction/task in the user's question.\n",
    "    Use only the information provided in the context.\n",
    "\n",
    "    TASK:\n",
    "    {task}\n",
    "\n",
    "    CONTEXT:\n",
    "    {text}\n",
    "\n",
    "    **IMPORTANT**: If the text does not include the SPECIFIC information required for the task, output \"NOT FOUND\".\n",
    "    Otherwise, provide the direct answer.\n",
    "    \"\"\"\n",
    "    llm_result = llm.complete(prompt)\n",
    "    return llm_result.text\n",
    "\n",
    "\n",
    "def text_process_llm(task: str, text: str, chunk_size: int = 10000, chunk_overlap: int = 500) -> List[str]:\n",
    "    \"\"\"Splits a large text into chunks and processes each chunk with an LLM.\n",
    "\n",
    "    This is useful for analyzing documents that are too large to fit into a\n",
    "    single LLM context window. Each chunk is processed independently.\n",
    "\n",
    "    Args:\n",
    "        task: The task to perform on each chunk of text.\n",
    "        text: The entire body of text to be processed.\n",
    "        chunk_size: The maximum number of characters in each chunk.\n",
    "        chunk_overlap: The number of characters to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of string responses, with one response for each processed chunk.\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separator=\"\"  # An empty separator splits by character\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    responses = []\n",
    "    for chunk in chunks:\n",
    "        chunk_response = task_with_text_llm(task, chunk)\n",
    "        if \"NOT FOUND\" not in chunk_response:\n",
    "            responses.append(chunk_response)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197af377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for capability_step in capability_plan.subplan:\n",
    "#     match capability_step[\"capability\"]:\n",
    "#         case AgentCapability.DEEP_WEB_SEARCHER:\n",
    "#             web_resources = web_search(capability_step[\"activity\"])\n",
    "#         case AgentCapability.UNSTRUCTURED_DATA_PROCESSOR:\n",
    "#             # TODO: how to pass things around? \n",
    "#             pass\n",
    "#         case _:\n",
    "#             print(\"Finished processing\")\n",
    "\n",
    "# for web_resource in web_resources:\n",
    "#     print(web_resource.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe325eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [web_resource.content for web_resource in web_resources]\n",
    "# text_process_llm(task=capability_plan.subplan[1][\"activity\"], text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Attachments:\n",
    "    # provided_by: who supplied the resource (user or an agent/tool call) # <=> as_answer_to\n",
    "    provided_by: str \n",
    "    filepath: str \n",
    "    description: str\n",
    "\n",
    "class ExecutionState(TypedDict):\n",
    "    # Identifies what the original question/task given, which objective it got transferred to, what the plan to get an answer is\n",
    "    task: Task\n",
    "    # status of whether it is completed or not \n",
    "    completed: bool\n",
    "    # order index of the step of the task's plan that is being executed \n",
    "    step: int\n",
    "    # the step of the plan that is being executed \n",
    "    subtask: str \n",
    "    # capability plan for this task \n",
    "    capability_plan: CapabilityPlan\n",
    "    # capability plan step order \n",
    "    capability_plan_step: int\n",
    "    # all files that might be needed to execute the task \n",
    "    attachments: list[Attachments]\n",
    "    # all text resources that might be needed to execute the task\n",
    "    resources: list[ContentResource]\n",
    "    # tools that already got called\n",
    "    # TODO: see if this is needed \n",
    "    tools_called: str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fc609",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At 'identify_plan_step' node, 'check_completion' branch found unknown target '<function run_determine_capabilities at 0x11d8a56c0>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     84\u001b[39m workflow.add_conditional_edges(\n\u001b[32m     85\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetermine_capability_step_execution\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     86\u001b[39m     run_capability,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     }\n\u001b[32m     91\u001b[39m )\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m research_graph = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Display the graph\u001b[39;00m\n\u001b[32m     97\u001b[39m display(Image(research_graph.get_graph().draw_mermaid_png()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.venv/lib/python3.11/site-packages/langgraph/graph/state.py:829\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    826\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    838\u001b[39m output_channels = (\n\u001b[32m    839\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output_schema]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    846\u001b[39m     ]\n\u001b[32m    847\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pdev/freestyling/agents/hf-course/.venv/lib/python3.11/site-packages/langgraph/graph/state.py:770\u001b[39m, in \u001b[36mStateGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m branch.ends.values():\n\u001b[32m    769\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m end != END:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    771\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m node, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m branch found unknown target \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m             )\n\u001b[32m    773\u001b[39m         all_targets.add(end)\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: At 'identify_plan_step' node, 'check_completion' branch found unknown target '<function run_determine_capabilities at 0x11d8a56c0>'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(state_schema=ExecutionState)\n",
    "\n",
    "def init_state(ask: str, attachments: str = None, resources: str = None) -> ExecutionState:\n",
    "    # TODO: should I be using LLM to convert attachments/resources to acceptable format?\n",
    "    # TODO: implement proper handling of attachments and resources \n",
    "    return ExecutionState(\n",
    "        ask=ask,\n",
    "        completed=False,\n",
    "        task=None,\n",
    "        step=None,\n",
    "        attachments=None\n",
    "    )\n",
    "\n",
    "def identify_plan_step(state: ExecutionState) -> ExecutionState: \n",
    "    \"\"\"  \n",
    "    Determine which state is being executed. \n",
    "    \"\"\"\n",
    "    state[\"step\"] = 0 if None else state[\"step\"] + 1\n",
    "         \n",
    "    return state\n",
    "\n",
    "def check_completion(state: ExecutionState) -> str:\n",
    "    plan_length = len(state[\"task\"].plan)\n",
    "    if state[\"step\"] < plan_length:\n",
    "        return \"proceed\"\n",
    "    else:\n",
    "        return \"stop\"\n",
    "\n",
    "def run_determine_capabilities(state: ExecutionState) -> ExecutionState: \n",
    "    state[\"capability_plan\"] = determine_capabilities(state[\"task\"].plan[state[\"step\"]])\n",
    "    state[\"capability_plan_step\"] = None\n",
    "\n",
    "    return state\n",
    "\n",
    "def determine_capability_step_execution(state: ExecutionState) -> ExecutionState:\n",
    "    if state[\"capability_plan_step\"] is None:\n",
    "        state[\"capability_plan_step\"] = 0 \n",
    "    else:\n",
    "        state[\"capability_plan_step\"] += 1 \n",
    "\n",
    "    return state\n",
    "\n",
    "def run_capability(state: ExecutionState) -> str: \n",
    "    capability_plan_length = len(state[\"capability_plan\"])\n",
    "    if state[\"capability_plan_step\"] < capability_plan:\n",
    "        # run capability \n",
    "        ask = state[\"capability_plan\"][state[\"capability_plan_step\"]]\n",
    "        resources, attachments = state[\"capability_plan\"][state[\"capability_plan_step\"]].run(ask)\n",
    "        state[\"attachments\"].extend(attachments)\n",
    "        state[\"resources\"].extend(resources)\n",
    "\n",
    "        return \"iterate\"        \n",
    "    else: \n",
    "        # signal that we need to move over to the next state\n",
    "        return \"stop\" \n",
    "\n",
    "def run_web_search(state: ExecutionState) -> ExecutionState:\n",
    "    resources = web_search(state[\"subtask\"])\n",
    "    state[\"resources\"].extend(resources)\n",
    "\n",
    "    return state \n",
    "\n",
    "workflow.add_node(\"define_task\", define_task)\n",
    "workflow.add_node(\"identify_plan_step\", identify_plan_step)\n",
    "workflow.add_node(\"run_determine_capabilities\", run_determine_capabilities)\n",
    "workflow.add_node(\"determine_capability_step_execution\", determine_capability_step_execution)\n",
    "\n",
    "workflow.add_edge(START, \"define_task\")\n",
    "workflow.add_edge(\"define_task\", \"identify_plan_step\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"identify_plan_step\",\n",
    "    check_completion,\n",
    "    {\n",
    "        \"proceed\": \"run_determine_capabilities\",\n",
    "        \"stop\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"run_determine_capabilities\", \"determine_capability_step_execution\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"determine_capability_step_execution\", \n",
    "    run_capability,\n",
    "    {\n",
    "        \"iterate\":\"determine_capability_step_execution\",\n",
    "        \"stop\": \"identify_plan_step\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "research_graph = workflow.compile()\n",
    "\n",
    "# Display the graph\n",
    "display(Image(research_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be66a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n"
     ]
    }
   ],
   "source": [
    "# Q: \"How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\"\n",
    "# Take the first step of the plan \n",
    "# resp = web_searcher.run(\"Who of Yankees had the most walks in 1977 regular season?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4d2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidStateError",
     "evalue": "Result is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidStateError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(\"Agent is running... waiting for the result...\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_result = \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInvalidStateError\u001b[39m: Result is not set."
     ]
    }
   ],
   "source": [
    "# print(\"Agent is running... waiting for the result...\")\n",
    "final_result = resp.result()\n",
    "pprint(resp.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First node: create task \n",
    "\n",
    "# Second node: create capability plan for the first step \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df604f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on capabilities let an agent do the task \n",
    "\n",
    "# Combine task and objective and hand over the execution to the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's work flow:\n",
    "#  1. Compose query \n",
    "#  2. Do search & collect results \n",
    "#  3. Download and process results\n",
    "#  4. Save results \n",
    "#  5. Produce summery of how the data looks like and where to find it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77346936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define agents \n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "# task = define_task(question)\n",
    "# pprint(task.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b058fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
