{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93e9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import textwrap\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LLM imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from ddgs import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269717c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: Hello! How can I assist you today? Feel free to as...\n"
     ]
    }
   ],
   "source": [
    "# Configuration and LLM Setup\n",
    "REQUEST_TIMEOUT = 180\n",
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=MODEL_NAME, \n",
    "    request_timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.complete(\"Hello\")\n",
    "print(f\"LLM initialized: {test_response.text[:50]}...\")\n",
    "\n",
    "\n",
    "def pprint(text):\n",
    "    wrapped_lines = textwrap.wrap(text, width=130)\n",
    "    for line in wrapped_lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a task, determine a step-by-step action plan of what needs to be done to accomplish this task and output the answer/result. \n",
    "    The most important actions that are taken: \n",
    "     1. Define the goal: what result is asked to be produced.\n",
    "     2. List the steps: provide a short explanation for each action that needs to be taken.       \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert project planner. Your task is to create a concise, step-by-step action plan to accomplish the user's goal.\n",
    "\n",
    "User's Goal:\n",
    "---\n",
    "{task}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "1. Clarify the Core Objective: Start by rephrasing the user's goal as a single, clear, and specific objective.\n",
    "2. Develop a Chronological Action Plan: Break down the objective into a logical sequence of high-level steps.\n",
    "\n",
    "Guiding Principles for the Plan:\n",
    "- Tool-Agnostic: Focus on the action required, not the specific tool to perform it (e.g., use \"Gather data on market trends\" instead of \"Search Google for market trends\").\n",
    "- Information First: The initial step should almost always be to gather and analyze the necessary information before taking further action.\n",
    "- S.M.A.R. Steps: Each step must be Specific, Measurable, Achievable, and Relevant. The focus is on the logical sequence, not specific deadlines.\n",
    "- Concise: Include only the critical steps needed to reach the objective.\n",
    "\n",
    "Example Output Format (ALWAS **JSON** ):\n",
    "{{\n",
    "  \"objective\": \"Plan and execute a one-day offsite event for a team of 10 people focused on team building and strategic planning.\",\n",
    "  \"plan\": [\n",
    "    \"Gather requirements including budget, potential dates, and key goals for the offsite from team leadership\",\n",
    "    \"Research and shortlist suitable venues and activity options that fit the budget and goals\",\n",
    "    \"Create a detailed agenda and budget proposal for approval\",\n",
    "    \"Book the selected venue, catering, and activities upon approval\",\n",
    "    \"Send out official invitations and manage attendee confirmations and dietary requirements\",\n",
    "    \"Finalize all logistical details and communicate the full itinerary to the team\"\n",
    "  ]\n",
    "}}\n",
    "where\n",
    "  \"objective\" 's value in the json is a clear, one-sentence summary of the end goal,\n",
    "  \"plan\" 's value in the json is a list **ALWAYS SEPARATED BY PYTHON NEWLINE CHARCTER** like \n",
    "  [\n",
    "    A short explanation of the first logical step\", \n",
    "    A short explanation of the next step that follows from the first\",\n",
    "    And so on...\"\n",
    "  ]\n",
    "\"\"\"\n",
    "    task_response = llm.complete(prompt)\n",
    "\n",
    "    return task_response.text\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    question: str\n",
    "    objective: str\n",
    "    plan: list[str]\n",
    "\n",
    "\n",
    "def define_task(task:str) -> str: \n",
    "    task_plan_response = create_plan(task)\n",
    "\n",
    "    # Assume llm outputs smth json-like with the correct keys.\n",
    "    result = json.loads(task_plan_response)\n",
    "\n",
    "    return Task(\n",
    "      question=task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e53eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Literal\n",
    "\n",
    "# Define the available capabilities as a type for better validation\n",
    "CapabilityAgents = Literal[\n",
    "    \"deep_web_search\",\n",
    "    \"video_processing\",\n",
    "    \"audio_processing\",\n",
    "    \"image_processing\",\n",
    "    \"structured_data_processing\",\n",
    "    \"unstructured_data_processing\",\n",
    "    \"code_math_writing\"\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class CapabilityPlan:\n",
    "    \"\"\"A structured plan outlining the sequence of capabilities and actions.\"\"\"\n",
    "    subplan: List[Dict[str, CapabilityAgents | str]]\n",
    "\n",
    "def create_capability_plan(task: str, attachments: List[str] = None) -> CapabilityPlan:\n",
    "    \"\"\"\n",
    "    Analyzes a task and generates a sequential execution plan using available capabilities.\n",
    "\n",
    "    Args:\n",
    "        task (str): The description of the task to be performed.\n",
    "        attachments (list[str]): A list of file names related to the task.\n",
    "\n",
    "    Returns:\n",
    "        CapabilityPlan: A dataclass containing the ordered list of sub-tasks.\n",
    "    \"\"\"\n",
    "    attachment_info = \"\"\n",
    "    if attachments:\n",
    "        attachment_info = f\"\\n\\nAttachments provided: {', '.join(attachments)}\"\n",
    "\n",
    "    planning_prompt = f\"\"\"\n",
    "You are a highly intelligent planning agent. Your primary function is to analyze a user's task and create a precise, step-by-step execution plan using a predefined set of capabilities.\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided task and create a sequential plan to accomplish it. The plan should be a list of steps, where each step defines the capability to use and the specific activity to perform.\n",
    "\n",
    "**Capabilities:**\n",
    "- `deep_web_search`: Find, evaluate, and download web content (e.g., articles, documents). This capability is for search and downloading web resources only, not for processing the content or getting any answers on the content.\n",
    "- `video_processing`: Download video, extract frames or audio from a video file for further analysis.\n",
    "- `audio_processing`: Download audio, transcribe speech, identify sounds, or analyze properties of an audio file.\n",
    "- `image_processing`: Download image, analyze an image to identify objects, read text (OCR), or understand its content.\n",
    "- `structured_data_processing`: Analyze, query, or visualize data from structured files like Parquet, CSV, JSON, or databases.\n",
    "- `unstructured_data_processing`: Analyze, summarize, extract information from, or answer questions about raw text or documents (e.g., PDFs, TXT files, retrieved web content).\n",
    "- `code_math_writing`: Generate or execute code, solve mathematical problems, or perform complex logical operations and computations.\n",
    "\n",
    "Instructions:\n",
    "Deconstruct the Task -> Assign Capabilities for each step -> Define the Activity for each step (i.e.,write a clear and concise description of the specific action to be performed using the chosen capability)\n",
    "\n",
    "Example 1: Simple Fact Lookup\n",
    "Task: \"What is the boiling point of water at sea level?\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"deep_web_search\",\n",
    "      \"activity\": \"Search for the boiling point of water at sea level\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"unstructured_data_processing\",\n",
    "      \"activity\": \"Analyze the downloaded web resources and find the reference to the boiling point temperature.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Example 2: Multi-step Information Retrieval and Analysis\n",
    "\n",
    "Task: \"Find the Q2 2025 earnings report for NVIDIA and tell me what their 'Gaming' division revenue was.\"\n",
    "Output:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"deep_web_search\",\n",
    "      \"activity\": \"Search for and download NVIDIA's official Q2 2025 earnings report document and download it.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"unstructured_data_processing\",\n",
    "      \"activity\": \"Analyze the downloaded earnings report to find and extract the revenue figure for the 'Gaming' division.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "---\n",
    "Begin Plan Generation\n",
    "\n",
    "Task: \"{task}\"\n",
    "Attachments: \"{attachment_info}\"\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"subplan\": [\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"capability\": \"...\",\n",
    "      \"activity\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.complete(planning_prompt)\n",
    "    response_text = response.text.strip()\n",
    "\n",
    "    result = json.loads(response_text)\n",
    "    return CapabilityPlan(subplan=result[\"subplan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36bb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(question='Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.',\n",
      "objective=\"Identify the actor who played Ray in the Polish-language version of 'Everybody Loves Raymond' and determine their role\n",
      "in the movie 'Magda M.'\", plan=[\"Research to identify the title or details of the Polish-language adaptation of 'Everybody Loves\n",
      "Raymond'\", 'Find the name of the actor who played Ray in this adaptation', \"Gather information on the filmography of the\n",
      "identified actor, focusing on their role in 'Magda M.'\"])\n"
     ]
    }
   ],
   "source": [
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "task = define_task(question)\n",
    "pprint(task.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a126e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapabilityPlan(subplan=[{'capability': 'deep_web_search', 'activity': \"Search for the Polish-language adaptation of 'Everybody\n",
      "Loves Raymond' and download relevant web pages or documents.\"}, {'capability': 'unstructured_data_processing', 'activity':\n",
      "\"Analyze the downloaded content to extract information about the title or production details of the Polish-language version of\n",
      "'Everybody Loves Raymond'.\"}])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the plan\n",
    "# Get a single task of the plan together with the objective \n",
    "current_plan_step = task.plan[0]\n",
    "task_step = f\"You need to FOCUS ON: {current_plan_step}. When your global objective which you SHOULD NOT FOCUS ON: {task.objective}.\"  \n",
    "\n",
    "# What is required for this to be executed?\n",
    "capabilities = create_capability_plan(task=task_step)\n",
    "pprint(capabilities.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cebb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# web_search_agent \n",
    "##################\n",
    "\n",
    "LINK_KEY = \"link\"\n",
    "\n",
    "@dataclass\n",
    "class WebResource:\n",
    "    \"\"\"A unified dataclass for handling a web resource.\n",
    "\n",
    "    Attributes:\n",
    "        content: The main text content of the web page. Can be None if not yet downloaded.\n",
    "        link: The unique URL for the resource.\n",
    "        metadata: A dictionary containing additional information, such as search result data.\n",
    "    \"\"\"\n",
    "    content: Optional[str]\n",
    "    link: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "\n",
    "def question_to_queries(question: str, max_queries: int = 2) -> List[str]:\n",
    "    \"\"\"Converts a user question into a list of optimized search engine queries.\n",
    "\n",
    "    Note:\n",
    "        This function requires a Large Language Model (LLM) to generate queries.\n",
    "        The `llm.complete()` call is a placeholder for your model's inference logic.\n",
    "\n",
    "    Args:\n",
    "        question: The user's input question.\n",
    "        max_queries: The maximum number of search queries to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of string queries optimized for a search engine.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Create a list of general search engine queries for the following question: \"{question}\".\n",
    "\n",
    "    Make sure that:\n",
    "    - Your output is a list separated by a \"|\" character and nothing else.\n",
    "    - You provide a MAXIMUM of {max_queries} search engine queries.\n",
    "    - Each query is SHORT and precise.\n",
    "\n",
    "    Example Output:\n",
    "    Large urban population areas in Europe|Biggest cities in Europe\n",
    "    \"\"\"\n",
    "    llm_response = llm.complete(prompt)\n",
    "    response_text = llm_response.text\n",
    "\n",
    "    return response_text.strip().split(\"|\")\n",
    "\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 2) -> List[WebResource]:\n",
    "    \"\"\"Performs a DuckDuckGo search and returns results as WebResource objects.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string.\n",
    "        max_results: The maximum number of search results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, where 'content' is None and 'metadata'\n",
    "        contains the search result details.\n",
    "    \"\"\"\n",
    "    found_resources = []\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=max_results))\n",
    "        if not results:\n",
    "            print(f\"⚠️ No search results found for '{query}'\")\n",
    "            return []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            resource = WebResource(\n",
    "                content=None,\n",
    "                link=result.get('href', 'No URL'),\n",
    "                metadata={\n",
    "                    \"search_order\": i,\n",
    "                    \"web_page_title\": result.get('title', 'No title'),\n",
    "                    \"web_page_summary\": result.get('body', 'No description'),\n",
    "                    \"query\": query\n",
    "                }\n",
    "            )\n",
    "            found_resources.append(resource)\n",
    "\n",
    "    return found_resources\n",
    "\n",
    "\n",
    "def drop_non_unique_resources(resources: List[WebResource]) -> List[WebResource]:\n",
    "    \"\"\"Removes duplicate WebResource objects based on their 'link' attribute.\n",
    "\n",
    "    Args:\n",
    "        resources: A list of WebResource objects.\n",
    "\n",
    "    Returns:\n",
    "        A new list of WebResource objects with duplicates removed.\n",
    "    \"\"\"\n",
    "    seen_links = set()\n",
    "    unique_resources = []\n",
    "    for resource in resources:\n",
    "        if resource.link and resource.link not in seen_links:\n",
    "            unique_resources.append(resource)\n",
    "            seen_links.add(resource.link)\n",
    "    return unique_resources\n",
    "\n",
    "\n",
    "def extract_clean_text(raw_html: str) -> str:\n",
    "    \"\"\"Extracts clean, readable text from raw HTML content.\n",
    "\n",
    "    This function removes scripts, styles, navigation, and other non-content\n",
    "    elements, then cleans up whitespace.\n",
    "\n",
    "    Args:\n",
    "        raw_html: The raw HTML content of a webpage.\n",
    "\n",
    "    Returns:\n",
    "        The extracted and cleaned plain text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        # Remove elements that typically do not contain main content\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            element.decompose()\n",
    "\n",
    "        # Extract text and clean up whitespace\n",
    "        text = soup.get_text(separator=\" \")\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        return ' '.join(chunk for chunk in chunks if chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def download_content(resource: WebResource) -> WebResource:\n",
    "    \"\"\"Downloads the HTML from a resource's link and populates its 'content' field.\n",
    "\n",
    "    Args:\n",
    "        resource: A WebResource object with a valid 'link'.\n",
    "\n",
    "    Returns:\n",
    "        The same WebResource object with its 'content' field updated with\n",
    "        the cleaned text from the webpage. Returns the original object if\n",
    "        download fails.\n",
    "    \"\"\"\n",
    "    if not resource.link:\n",
    "        return resource\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(resource.link, timeout=15) as response:\n",
    "            html_content = response.read()\n",
    "            resource.content = extract_clean_text(html_content)\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"URL Error for {resource.link}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {resource.link}: {e}\")\n",
    "\n",
    "    return resource\n",
    "\n",
    "\n",
    "def web_search(question: str, links_per_query: int = 2) -> List[WebResource]:\n",
    "    \"\"\"Orchestrates the full web search process for a given question.\n",
    "\n",
    "    This process includes:\n",
    "    1. Converting the question into search queries.\n",
    "    2. Searching the web to find resources.\n",
    "    3. Downloading and extracting text content from each resource.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        links_per_query: The number of web links to retrieve for each search query.\n",
    "\n",
    "    Returns:\n",
    "        A list of WebResource objects, with their 'content' field populated.\n",
    "    \"\"\"\n",
    "    # 1. Generate search queries from the question\n",
    "    candidate_queries = question_to_queries(question)\n",
    "    print(f\"\\nGenerated queries: {candidate_queries}\")\n",
    "\n",
    "    # 2. Search for relevant sources for each query\n",
    "    all_sources = []\n",
    "    for query in candidate_queries:\n",
    "        sources_for_query = duckduckgo_search(query, links_per_query)\n",
    "        all_sources.extend(sources_for_query)\n",
    "\n",
    "    # 3. Filter out any duplicate resources found by different queries\n",
    "    unique_sources = drop_non_unique_resources(all_sources)\n",
    "    print(f\"\\nFound {len(unique_sources)} unique web resources.\")\n",
    "\n",
    "    # 4. Download content for each unique resource\n",
    "    final_resources = []\n",
    "    for source in unique_sources:\n",
    "        populated_resource = download_content(source)\n",
    "        if populated_resource.content: # Only keep resources where content was successfully downloaded\n",
    "            final_resources.append(populated_resource)\n",
    "\n",
    "    return final_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df604f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on capabilities let an agent do the task \n",
    "\n",
    "\n",
    "\n",
    "# Combine task and objective and hand over the execution to the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29edd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent's work flow:\n",
    "#  1. Compose query \n",
    "#  2. Do search & collect results \n",
    "#  3. Download and process results\n",
    "#  4. Save results \n",
    "#  5. Produce summery of how the data looks like and where to find it.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77346936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define agents \n",
    "from llama_index.core.agent import ReActAgent, FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f3216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b058fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
