{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "emb_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama  \n",
        "\n",
        "chat_model = Ollama(model=\"qwen2:7b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnsDEuS1wHh-"
      },
      "source": [
        "## Researcher: An Agent Workflow\n",
        "\n",
        "Let's create an agent workflow that would: \n",
        " 1) Take a topic to write a blogpost about and subdivide it into sections\n",
        " 2) The first section should include variables, definitions, concepts and terminology that are going to be used for explaining (mathematicaly) the problem   \n",
        "    - Briefly explain those concepts \n",
        " 3) Look up websites that explain the topic  \n",
        "    - Download the selected websites and process their data (make a note that you need to refence them)\n",
        "    - Select websites that best fit the subsections identified before  \n",
        "    - Decide which website is the best at explaining the topic based on how it covers the subsections\n",
        "1) Analyse the website and change rearrange its content to fit the desired subsection structure and the variables and defintions  \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.error\n",
        "import urllib.error\n",
        "import urllib.request\n",
        "\n",
        "from llama_index.core.workflow import (\n",
        "    InputRequiredEvent,\n",
        "    HumanResponseEvent,\n",
        ")\n",
        "from llama_index.core.agent.workflow import ReActAgent, AgentWorkflow\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.workflow import Context\n",
        "from duckduckgo_search import DDGS\n",
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment(experiment_name=\"Test-agentic-workflow\")\n",
        "mlflow.set_tracking_uri('http://localhost:5000')\n",
        "mlflow.llama_index.autolog()\n",
        "\n",
        "\n",
        "def get_relevant_webpages(ctx: Context, query: str) -> tuple[list[dict[str, str]], int]:\n",
        "    \"\"\"\n",
        "    Gets the relevant webpages urls for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): what to search online on www.\n",
        "\n",
        "    Returns:\n",
        "        tuple: first element is list of url links (websites) related to the query and second is integer number that is the lenght of that list.  \n",
        "    \"\"\"\n",
        "    search_ggg = DDGS()\n",
        "    results = search_ggg.text(query)\n",
        "    return results, len(results)\n",
        "\n",
        "def download_webpage(ctx: Context, url: str, num_) -> str:\n",
        "    \"\"\"\n",
        "    Load the raw webpage of the url. Store it in the context.\n",
        "    \n",
        "    Args:\n",
        "        url (str): www url of the page.\n",
        "    \n",
        "    Returns: \n",
        "        str: html string of the whole webpage.\n",
        "    \"\"\"\n",
        "    try: \n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            html_text = response.read()\n",
        "            ctx.store.set(\"html_resource\", html_text)\n",
        "            return html_text\n",
        "    except urllib.error.URLError as e:\n",
        "        print(\"Error getting the page.\")\n",
        "    except Exception as e:\n",
        "        print(\"Something happened.\")\n",
        "\n",
        "def generate_blogpost(ctx: Context, text: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a blogpost in a markdown format based on the raw of a resource. \n",
        "    \"\"\"\n",
        "    task = f\"Based on the this resource html {text}, generate a blogpost about the topic in markdown format.\"\n",
        "    resp = chat_model.complete(task)\n",
        "\n",
        "    print(resp.text)\n",
        "\n",
        "    while True: \n",
        "        question = \"do you like what you see?\"\n",
        "        human_feedback = ctx.wait_for_event(\n",
        "            HumanResponseEvent,\n",
        "            waiter_id=question,\n",
        "            waiter_event=InputRequiredEvent(\n",
        "                prefix=question\n",
        "            )\n",
        "        )\n",
        "        if len(human_feedback) == 0:\n",
        "            return resp.text\n",
        "        else:\n",
        "            task = f\"Based on the this resource html {text}, generate a blogpost about the topic in markdown format. Take into account previous feedback which was: {human_feedback}.\"\n",
        "            resp = chat_model.complete(task)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/22 22:49:43 WARNING mlflow.tracing.fluent: Failed to start span Workflow.run: API request to http://localhost:5000/api/2.0/mlflow/traces failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x30f749c70>: Failed to establish a new connection: [Errno 61] Connection refused')). For full traceback, set logging level to debug.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history of Prussia dates back to around 1075 when it was established as a duchy under the Kingdom of Denmark. It went through various transformations and had numerous rulers over time, including the Hohenzollern dynasty who were the dominant power in Prussia.\n",
            "\n",
            "In 1871, Prussia played a pivotal role in unifying Germany after their victory against France during the Franco-Prussian War. The outcome led to the creation of the German Empire with the House of Hohenzollern as its ruling family.\n",
            "\n",
            "From then on, Prussia became part of Germany and was instrumental in shaping modern European politics until World War II when it lost territories including East Prussia due to post-war redrawing of borders by the Allies. \n",
            "\n",
            "After World War II, Berlin (Prussian capital) split into four zones occupied by the allies, leading to its division into today's East and West Berlin.\n",
            "\n",
            "The history of Prussia is rich with significant contributions in fields such as military tactics, literature, science, music and philosophy that have influenced European culture profoundly throughout the centuries.\n"
          ]
        }
      ],
      "source": [
        "web_search_tool = FunctionTool.from_defaults(\n",
        "    fn=get_relevant_webpages,\n",
        "    name=\"get_relevant_webpages\",\n",
        "    description=\"Useful for getting a list of relevant webpages (url links) for a particular query. \" \\\n",
        "    \"Together with the web links the list also include a short information about the answer for the query.\"\n",
        ")\n",
        "\n",
        "page_download_tool = FunctionTool.from_defaults(\n",
        "    fn=download_webpage,\n",
        "    name=\"download_webpage\",\n",
        "    description=\"Useful for dowloading raw html of the page, storing it in the agent's context and returning the content of that page.\"\n",
        ")\n",
        "\n",
        "blogpost_write_tool = FunctionTool.from_defaults(\n",
        "    fn=generate_blogpost,\n",
        "    name=\"generate_blogpost\",\n",
        "    description=\"Generate a blogpost based on the provided resource's html webpage.\"\n",
        ")\n",
        "\n",
        "\n",
        "blogpost_writer_agent = ReActAgent(\n",
        "    name = \"BlogpostWriter\", \n",
        "    description = \"Write a blogpost in markdown based on a resource's web page html\",\n",
        "    system_prompt = \"You receive a raw html of a webpage that describes a topic at interest, you need to generate a blogpost in markdown format that would be modern, factual and not too verbose.\",\n",
        "    tools=[blogpost_write_tool],\n",
        "    verbose=True,\n",
        "    llm=chat_model,\n",
        ")\n",
        "\n",
        "download_page_agent = ReActAgent(\n",
        "    name = \"WebpageDownloader\", \n",
        "    description = \"Download a web page's html\",\n",
        "    system_prompt = \"You receive a url link and the order number that you need to pass to the 'page_download_tool' \",\n",
        "    tools=[page_download_tool],\n",
        "    verbose=True,\n",
        "    llm=chat_model,\n",
        "    can_handoff_to=[\"BlogpostWriter\"],\n",
        ")\n",
        "\n",
        "search_agent = ReActAgent(\n",
        "    name = \"WebSearcher\", \n",
        "    description = \"Search the web give links to the relevant pages found. For each element in the returned list call\",\n",
        "    system_prompt = \"You will receive a list of webpages on a topic in the format [{'title': '<....>', 'href': 'https://<....>', 'body': '<....>']. For each element of the list run the \",\n",
        "    tools=[web_search_tool],\n",
        "    verbose=True,\n",
        "    llm=chat_model,\n",
        "    can_handoff_to=[\"WebpageDownloader\"],\n",
        ")\n",
        "\n",
        "workflow = AgentWorkflow(agents=[search_agent,download_page_agent,blogpost_writer_agent], root_agent=\"BlogpostWriter\")\n",
        "\n",
        "ctx = Context(workflow=workflow)\n",
        "handler = workflow.run(\"History of Prussia\", ctx=ctx)\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, InputRequiredEvent):\n",
        "        response = input(event.prefix)\n",
        "        handler.ctx.send_event(\n",
        "            HumanResponseEvent(\n",
        "                response=response\n",
        "            )\n",
        "        )\n",
        "\n",
        "answer = await handler        \n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_state': DictState(),\n",
              " '_lock': <asyncio.locks.Lock object at 0x309eab650 [unlocked]>}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
