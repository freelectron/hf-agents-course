{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pprint(text):\n",
        "    import textwrap\n",
        "    wrapped_text = textwrap.fill(text, width=100) \n",
        "    print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4IWFIkdwHh9"
      },
      "source": [
        "## Creating a FunctionTool\n",
        "\n",
        "Let's create a basic `FunctionTool` and call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tEv1yzCwHh9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting weather for New York\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in New York is sunny')], tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
        "    print(f\"Getting weather for {location}\")\n",
        "    return f\"The weather in {location} is sunny\"\n",
        "\n",
        "\n",
        "tool = FunctionTool.from_defaults(\n",
        "    get_weather,\n",
        "    name=\"my_weather_tool\",\n",
        "    description=\"Useful for getting the weather for a given location.\",\n",
        ")\n",
        "tool.call(\"New York\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "emb_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama  \n",
        "\n",
        "chat_model = Ollama(model=\"qwen2:7b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nLlvvWwHh9"
      },
      "source": [
        "## Creating a QueryEngineTool\n",
        "\n",
        "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Documents: \n",
            " 4\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"~/pdev/yaub/frontend/simple-yaub/public/\", recursive=True)\n",
        "documents_all = reader.load_data()\n",
        "documents = list()\n",
        "for doc in documents_all:\n",
        "    if \"text\" in doc.metadata[\"file_type\"]:\n",
        "        documents.append(doc) \n",
        "print(\"# Documents: s\", len(documents))\n",
        "\n",
        "# Splits the documents into sentences, pushes each sentence through the embed_model, \n",
        "# groups sentences into nodes based on how far the distance between their embeddings is,\n",
        "# threshold distance is determined based on breakpoint_percentile_threshold    \n",
        "splitter = SemanticSplitterNodeParser(embed_model=emb_model)\n",
        "nodes = splitter.get_nodes_from_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/llama_index/vector_stores/qdrant/base.py:709: UserWarning: Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
            "  self._client.create_payload_index(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "qdrant_client = QdrantClient(\":memory:\")      \n",
        "vector_store = QdrantVectorStore(collection_name=\"blog\", client=qdrant_client)\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model=emb_model)\n",
        "query_engine = index.as_query_engine(llm=chat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pQcfenWKwHh9",
        "outputId": "cb016174-ff17-4680-879e-d609fc42998c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolOutput(blocks=[TextBlock(block_type='text', text='The author mentions Bishop\\'s Section 6.4.5 from his book \"Pattern Recognition and Machine Learning\" (2006) as a reference for understanding different approximation methods used in Gaussian processes. Specifically, he refers to this section to describe the Laplace approximation method which is used to approximate $p(t^* \\\\mid \\\\vec{t})$.')], tool_name='Blogposts Accessor', raw_input={'input': 'What references in the end does the author mention for Kernel methods post?'}, raw_output=Response(response='The author mentions Bishop\\'s Section 6.4.5 from his book \"Pattern Recognition and Machine Learning\" (2006) as a reference for understanding different approximation methods used in Gaussian processes. Specifically, he refers to this section to describe the Laplace approximation method which is used to approximate $p(t^* \\\\mid \\\\vec{t})$.', source_nodes=[NodeWithScore(node=TextNode(id_='8f3762d5-2064-4b97-a08d-00d1d6debb6a', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f9573ec-2379-47c2-9951-6e0189692a38', node_type='4', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, hash='58fed88fa163949aaaf3de13bcc19cbab78d11749e65619d2da25680b1adea99'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7a9679db-dfd2-4335-b7b8-8d7d3dee0395', node_type='1', metadata={}, hash='f603c23345e1c5f5cab686b733262e00df57910e7bbe2c52ae2a31a14ea4b022')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='<center>\\n    <h1> Kernel Methods  </h1>\\n</center>\\n\\n$~$\\n\\n$~$\\n## Why do we care? 🤔\\n$~$\\n\\nA previous blog post on Linear Models explained a\\nbeautiful connection between linear models and kernels. It has also set\\nthe stage for us to progress towards what more commonly referred to\\nas Kernel Methods. Probabilistic techniques that utilise kernel\\nfunctions to model complex behaviours in the data.\\n\\n$~$\\n## Gaussian Processes for Regression ﮩ٨ـﮩﮩ٨ـ♡ﮩ٨ـﮩﮩ٨ـ\\n$~$\\n\\nIn Linear models, we saw a bunch of linear models that\\nassumed some functional form. Most of them, if not all, can be\\nrepresented by some (linear) combination of the targets. Now, consider\\nthat instead of finding optimal function parameters, we would try to\\nfind functions (function mappings). This way, we can directly sample a\\nfunction from a governing (multivariate) distribution. However, a question arirse: \\nhow could one parameterise a distribution over functions? Well, we do not parameterise\\nfunctions directly, but model the joint marginal probabilities of their\\nvalues. We then reason about the entire function space by evaluating the\\njoint probabilities at any set of input points.\\n\\n$~$\\n\\nLet $X = \\\\{\\\\vec{x}_1, \\\\vec{x}_2 ... \\\\vec{x}_N\\\\}$ be a finite set of\\nelements and consider a set $\\\\mathcal{H}$ of all possible functions\\n$f$ mapping from $\\\\mathcal{X}$ to $\\\\mathbb{R}$. If we\\nassume that the domain of any $f(\\\\cdot) \\\\in \\\\mathcal{H}$ has a limited\\n($N$) number of elements, we can say that $f$ is \"represented\" by its\\nvector $\\\\vec{f} = [f(\\\\vec{x}_1), f(\\\\vec{x}_2)... f(\\\\vec{x}_N)]$. Note\\nthat $f(\\\\vec{x}_i)$\\'s by themselves can be seen as random\\nvariables/functions if the underlying function $f(\\\\cdot)$ is also\\nrandom. Thus, there is a one-to-one correspondence between a function\\n$f(\\\\cdot) \\\\in \\\\mathcal{H}$ and its \"vector form\\\\\" representation.\\nConsequently, we can specify a governing probability distribution where\\n$\\\\vec{f} \\\\sim N(\\\\vec{\\\\mu}, \\\\Lambda)$. This implies a (joint) probability\\ndistribution over random variables/functions that could have produced\\nthe values we are trying to predict, i.e., $p(\\\\vec{f}|X) = N(\\\\vec{\\\\mu}, K))$. \\n\\n$~$\\n\\nIn most of the cases, we need to assume that our stream of data is\\ninfinite and we can always make new predictions based on the new\\nobservations $\\\\vec{x}^{*}$ , i.e., a continuing process. Gaussian\\nprocess (GP) is a stochastic process - collection of random variables -\\nsuch that any finite sub-collection of random variables from the process\\nhas a joint multivariate distribution. By assuming a Gaussian process,\\nwe are pretty much saying: there were these values $\\\\vec{f}$ from\\nunderlying $f$, now any new collection $\\\\vec{f}^{*}$ must come from the\\nsame $f$. \\n\\n', mimetype='text/plain', start_char_idx=0, end_char_idx=2678, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7886392157698461), NodeWithScore(node=TextNode(id_='4a6746a1-1060-405b-a33d-aedcb0d860b5', embedding=None, metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8f9573ec-2379-47c2-9951-6e0189692a38', node_type='4', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, hash='58fed88fa163949aaaf3de13bcc19cbab78d11749e65619d2da25680b1adea99'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7a9679db-dfd2-4335-b7b8-8d7d3dee0395', node_type='1', metadata={'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, hash='562560c6088c5f3ad6a461e234be233a5e86b5433ece5ad4d00cf985928656dd'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d54a7643-2a30-40d0-9dc1-1132548e3cac', node_type='1', metadata={}, hash='07dcedc6387fbb8693b8b15c4103f98d42d7fc6c7bad4ea023cc6706f1ee4c49')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='That is\\nwhy the real-world implementations of GP\\'s usually do not look at the\\nwhole covariance matrix of observations, i.e., evaluating the kernel\\nfunction for all data points, instead they use a sample of the data for\\neach new set of observations. Those optimised methods are often called\\nsparse kernel methods or sparse kernel machines. Second, the choice of\\nkernels is another aspect to consider. Many kernel function comes with\\nits own behaviour and hyperparameters. As a result, a practitioner can\\nspend a considerable amount of time in choosing the right kernel and\\noptimising its hyperparameters.\\n\\n$~$\\n\\n$~$\\n## Gaussian Processes for Classification 🔡\\n$~$\\n\\nThe solution for GP\\'s in the case of a regression task, though\\ncomputationally heavy, is analytical. If we put GP\\'s into the framework\\nof classification tasks, we end up with a less behaved solution.\\n\\n$~$\\n\\nNow, we are predicting a variable that can only take either of two value\\n$t_i \\\\in\\\\{0 ; 1\\\\}$. Use a linear separator function $a(\\\\cdot)$ and put\\nit into a sigmoid to map a value to the probability range of $[0, 1]$\\n\\n$$\\n\\\\begin{align} \\na(\\\\vec{x}) : \\\\vec{x} \\\\in \\\\mathbb{R}^M \\\\mapsto a \\\\in \\\\mathbb{R}^1 \\\\\\\\ \\n\\\\sigma(a) = \\\\frac{1}{1 + e^{-a}} \\\\\\\\ \\n\\\\sigma(a): a \\\\in \\\\mathbb{R}^1 \\\\mapsto [0,1] \\\\ .\\n\\\\end{align}\\n$$\\n\\nThen, define the probability for the target to follow a Bernoulli\\ndistribution \\n\\n$$\\n\\\\begin{align} \\np(t \\\\mid a)=\\\\sigma(a)^t (1-\\\\sigma(a))^{1-t} . \\\\ \\n\\\\end{align}\\n$$\\n\\nUnlike the regression case, variable $t$ is defined with a non-Gaussian\\ndistribution. This prevents us from entering the GP framework right away\\nto model the target. However, the values produced by $a(\\\\cdot)$ are\\ncontinuous and can be modeled with a Gaussian process just like in the\\nregression case. Consequently, we can define a zero-meaned prior with\\nthe covariance function produced by a chosen kernel function\\n\\n$$\\n\\\\begin{align} \\np(\\\\vec{a}) = N(\\\\vec{a} | \\\\vec{0}, C_N) \\\\ .\\n\\\\end{align}\\n$$\\n\\nThe final predictive probability distribution, the probability of\\n$\\\\vec{t}^*$ given $\\\\vec{t}, X, \\\\vec{x}^{*}$, is predictably\\n\\n$$\\n\\\\begin{align} \\np(t^* \\\\mid \\\\vec{t}, X, \\\\vec{x}^{*}) \\\\text{\\\\ or, simply, \\\\ \\\\ } p(t^* \\\\mid \\\\vec{t}) = ?\\n\\\\end{align}\\n$$\\n\\nWe can rewrite it as \\n\\n$$\\n\\\\begin{align} \\np\\\\left(t^* \\\\mid \\\\vec{t}\\\\right) = \\\\int p\\\\left(t^* \\\\mid a^{*}\\\\right) p\\\\left(a^{*} | \\\\vec{t}\\\\right) d a^{*}\\n\\\\end{align}\\n$$\\n\\nwhere\\n$p\\\\left(t^* \\\\mid a^{*}\\\\right) = p\\\\left(t^* = 1 | a^{*}\\\\right) = \\\\sigma\\\\left(a^{*}\\\\right)$.\\nWe see that the first term $p\\\\left(t^* \\\\mid a^{*}\\\\right)$ is a sigmoid\\nand the posterior $p\\\\left(a^{*} | \\\\vec{t}\\\\right)$ can be rewritten in\\nthe following form \\n\\n$$\\n\\\\begin{align} \\np\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right) &= \\\\int p\\\\left(a^{*}, \\\\vec{a} \\\\mid \\\\vec{t} \\\\right) d \\\\vec{a} = \\\\\\\\\\n& = \\\\text{\\\\{ following Bayes formula \\\\}} = \\\\\\\\\\n& = \\\\int \\\\frac{ p\\\\left(\\\\vec{t} | a^{*}, \\\\vec{a} ) \\\\cdot p\\\\left(a^{*}, \\\\vec{a} \\\\right)\\\\right.} {p\\\\left(\\\\vec{t} \\\\right)} d \\\\vec{a} = \\\\\\\\\\n& = \\\\frac{1}{p (\\\\vec{t} )}  \\\\int p\\\\left(a^{*}, \\\\vec{a} \\\\right) p\\\\left(\\\\vec{t} \\\\mid a^{*}, \\\\vec{a} \\\\right) d \\\\vec{a} = \\\\\\\\ \\n& = \\\\left\\\\{P\\\\left(\\\\vec{t} \\\\mid a^{*}, \\\\vec{a} \\\\right) = p\\\\left(\\\\vec{t} \\\\left(\\\\vec{a} \\\\right)\\\\right) \\\\text{\\\\ since $\\\\vec{t} $ is independent of } a^{*} \\\\right\\\\} = \\\\\\\\ \\n& = \\\\frac{1}{p (\\\\vec{t} )} \\\\int p\\\\left(a^{*} \\\\mid \\\\vec{a}\\\\right) p\\\\left(\\\\vec{a} \\\\right) p\\\\left(\\\\vec{t} | \\\\vec{a} \\\\right) d \\\\vec{a} \\\\\\\\\\n& = \\\\text{\\\\{ using Bayes formula again \\\\}} = \\\\\\\\\\n& = \\\\int p\\\\left(a^{*} | \\\\vec{a} \\\\right) p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right) d \\\\vec{a} \\\\ .\\n\\\\end{align}\\n$$\\n\\nThe integral $p\\\\left(t^* \\\\mid \\\\vec{t}\\\\right)$ can be approximate using multiple methods, see [Bishop\\'s Section\\n6.4.5](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\\nto get an idea of those methods. He himself - maybe seeking for an\\neasier to write solution - goes for the Laplace approximation method\\nsince $p\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right)$ can be so neatly reconstructed. The term\\n$p\\\\left(a^{*} | \\\\vec{a} \\\\right)$ is the prior and\\ncan be found by using the GP for regressions. What is left is to notice\\nthat $p\\\\left(a_{*} \\\\mid \\\\vec{t} \\\\right)$ gives us a convolution of two Gaussian\\'s if\\nwe were two approximate $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ as such.\\nThen, one can the solution for the posterior and predictive distribution\\nby using [the\\nformula](https://math.stackexchange.com/questions/1745174/convolution-of-two-gaussian-functions)\\nfor convolving two Gaussian\\'s.\\n\\n\\n<br>\\n<br>\\n\\n<style>\\n  .intermezzo1 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 2px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo1::before {\\n    content: \"★ Intuition: Convolutions ★\"; \\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    padding: 10px 5px;\\n    color: #4e3737;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo1\">\\n\\nThe general formula for a convolution is:\\n\\n$$\\n(f * g)(x) = \\\\int_{-\\\\infty}^{\\\\infty} f(t) g(x - t) \\\\, dt\\n\\\\tag{I.1}\\n$$\\n\\nWe can see $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ as $ f(t) $ because a zero mean was assumed for the $ a(\\\\cdot) $ \\'s GP. Thus, the formula in the exponent of $p\\\\left(\\\\vec{a} \\\\mid \\\\vec{t} \\\\right)$ is something like:\\n\\n$$\\n-\\\\frac{1}{(2\\\\pi \\\\sigma^2)^{1/2}} a({\\\\vec{x}})^2 \\\\tag{I.2}\\n$$\\n\\nSimilarly, $ p\\\\left(a^{*} \\\\mid \\\\vec{a} \\\\right) $ corresponds to $ g(x - t) $, since conditioning on $ \\\\vec{a} $ will lead to something along the lines of:\\n\\n$$\\n-\\\\frac{1}{(2\\\\pi \\\\sigma^2)^{1/2}} \\\\left( a(\\\\vec{x}^{*}) - a(\\\\vec{x}) \\\\right)^2  \\\\tag{I.3}\\n$$\\n\\nVery roughly speaking, you can interpret this convolution as marginalizing out $\\\\vec{a}$ and determining the probability of $ a^{*} $.\\n\\n<style>\\n  .intermezzo12 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 40px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo12::before {\\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    background: white;\\n    padding: 10px 10px;\\n    color: #666;\\n    font-size: 1.2em;\\n  }\\n</style>\\n\\n<br>\\n\\n[//]: # (<hr class=\"intermezzo12\">)\\n\\n\\n<style>\\n  .intermezzo21 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 2px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo21::before {\\n    content: \"★ Intuition: Laplace approximation ★\"; \\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    padding: 10px 5px;\\n    color: #4e3737;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo21\">\\n\\nLaplace\\'s approximation takes a uni-modal function where most of the probability mass is concentrated around its mode and approximates it with a Normal density function.\\n\\nHow do we know that $p\\\\left(\\\\vec{a}_N \\\\mid \\\\vec{t}_N\\\\right)$ is a function that can be approximated with the Laplace method? We can see that\\n\\n$$\\np(\\\\vec{a}|\\\\vec{t}) \\\\propto p(\\\\vec{a}) \\\\cdot p(\\\\vec{t} | \\\\vec{a}) \\\\text{ where}\\n$$\\n\\n$$\\np(\\\\vec{a}) = N(\\\\vec{a} | \\\\vec{0}, {C}_N) \\\\text{ , and }\\n$$\\n\\n$$\\np\\\\left(\\\\vec{t} \\\\mid \\\\vec{a} \\\\right) = \\\\prod_{n=1}^N \\\\sigma\\\\left(a_n\\\\right)^{t_n}\\\\left(1-\\\\sigma\\\\left(a_n\\\\right)\\\\right)^{1-t_n}=\\\\prod_{n=1}^N e^{a_n t_n} \\\\sigma\\\\left(-a_n\\\\right).\\n$$\\n\\nThe first term, $p(\\\\vec{a}_{N})$, is a normal distribution by (previous) construction, and therefore uni-modal. The second term, $p\\\\left(\\\\vec{t}_N \\\\mid \\\\vec{a}_N\\\\right)$, is a Bernoulli variable, and multiplication of a Bernoulli and a Gaussian is another (possibly skewed) distribution with a single mode (see [here](https://math.stackexchange.com/questions/3315169/what-is-the-distribution-of-the-product-of-a-bernoulli-0-1-a-normal-random-v), [here](https://stats.stackexchange.com/questions/27097/what-is-the-distribution-of-the-product-of-a-bernoulli-a-normal-random-variabl), and [here](https://www.math.wm.edu/~leemis/chart/UDR/PDFs/BernoulliP.pdf)). Bishop provides more details on why this is the case, and we can assume that $p(\\\\vec{a}_N|\\\\vec{t}_N)$ could be approximated by the Laplace method.\\n\\nThe approximation itself is a bit too cumbersome to go into detail. The idea is that one needs to first find the mode of $p(\\\\vec{a} |\\\\vec{t})$ by Taylor expanding the logarithm of $p(\\\\vec{a} | \\\\vec{t})$ and setting its derivative to zero. Second, the Hessian matrix helps to find the covariance matrix of the approximate distribution. This is because the inverse of the Hessian matrix of the log-likelihood function at the maximum likelihood estimate is asymptotically equal to the covariance matrix of the parameter estimates (see [Gaussian curvatures](https://en.wikipedia.org/wiki/Gaussian_curvature) and [Fisher information matrix](https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s)).\\n\\n\\n<style>\\n  .intermezzo22 {\\n    border: none;\\n    border-top: 2px solid #ccc;\\n    margin: 40px 0;\\n    width: 100%; /* or 100% if you want it to span the full width */\\n    text-align: center;\\n  }\\n  .intermezzo22::before {\\n    display: inline-block;\\n    position: relative;\\n    top: -14px;\\n    background: white;\\n    padding: 10px 10px;\\n    color: #666;\\n    font-size: 1.2em;\\n  }\\n</style>\\n<hr class=\"intermezzo22\">\\n\\n\\nSay, the Laplace approximation yields \\n\\n$$\\n\\\\begin{align} \\nq(\\\\vec{a})= N \\\\left(\\\\vec{a} \\\\mid \\\\hat{\\\\vec{a}}, {H}^{-1}\\\\right)\\n\\\\end{align}\\n$$\\n[//]: # (\\\\label{eq:gp_class_laplace_solution})\\n\\nwhere $\\\\hat{\\\\vec{a}}$ is the mean\\n(mode) of the approximated function and the covariance\\n$H = W + C^{\\\\top}$ with $W$ being a diagonal matrix with elements\\n$\\\\sigma(a_{i})(1 - \\\\sigma(a_i))$ while $a_n$ are the elements of\\n$\\\\vec{a}$ and $C$ being the prior\\'s covariance matrix. ', mimetype='text/plain', start_char_idx=6111, end_char_idx=15731, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7744367532583452)], metadata={'8f3762d5-2064-4b97-a08d-00d1d6debb6a': {'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}, '4a6746a1-1060-405b-a33d-aedcb0d860b5': {'file_path': '/Users/maksim.rostov/pdev/yaub/frontend/simple-yaub/public/posts/kernel_models/post.md', 'file_name': 'post.md', 'file_type': 'text/markdown', 'file_size': 16735, 'creation_date': '2025-01-26', 'last_modified_date': '2025-01-26'}}), is_error=False)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"Blogposts Accessor\",\n",
        "    description=\"RAG for posts\",\n",
        ")\n",
        "\n",
        "tool.call(\n",
        "    \"What references in the end does the author mention for Kernel methods post?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnsDEuS1wHh-"
      },
      "source": [
        "## Creating Toolspecs\n",
        "\n",
        "Let's create a `ToolSpec` from the `GmailToolSpec` from the LlamaHub and convert it to a list of tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Wkh5_GGywHh_",
        "outputId": "cc169a80-972b-4961-e3af-ac586a4840fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load_data load_data() -> List[llama_index.core.schema.Document]\n",
            "Load emails from the user's account.\n",
            "search_messages search_messages(query: str, max_results: Optional[int] = None)\n",
            "\n",
            "        Searches email messages given a query string and the maximum number\n",
            "        of results requested by the user\n",
            "           Returns: List of relevant message objects up to the maximum number of results.\n",
            "\n",
            "        Args:\n",
            "            query (str): The user's query\n",
            "            max_results (Optional[int]): The maximum number of search results\n",
            "            to return.\n",
            "\n",
            "        \n",
            "create_draft create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
            "\n",
            "        Create and insert a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            to (Optional[str]): The email addresses to send the message to\n",
            "            subject (Optional[str]): The subject for the event\n",
            "            message (Optional[str]): The message for the event\n",
            "\n",
            "        \n",
            "update_draft update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
            "\n",
            "        Update a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           This function is required to be passed a draft_id that is obtained when creating messages\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            to (Optional[str]): The email addresses to send the message to\n",
            "            subject (Optional[str]): The subject for the event\n",
            "            message (Optional[str]): The message for the event\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n",
            "get_draft get_draft(draft_id: str = None) -> str\n",
            "\n",
            "        Get a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n",
            "send_draft send_draft(draft_id: str = None) -> str\n",
            "\n",
            "        Sends a draft email.\n",
            "           Print the returned draft's message and id.\n",
            "           Returns: Draft object, including draft id and message meta data.\n",
            "\n",
            "        Args:\n",
            "            draft_id (str): the id of the draft to be updated\n",
            "\n",
            "        \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.tools.google import GmailToolSpec\n",
        "\n",
        "tool_spec = GmailToolSpec()\n",
        "tool_spec_list = tool_spec.to_tool_list()\n",
        "[print(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent with Websearching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your History lists the pages you've visited on Chrome in the last 90 days. It doesn't store: If you’re signed in to Chrome and sync your history, then your History also shows pages you’ve visited … Von Ihnen besuchte Websites werden in Ihrem Browserverlauf gespeichert. Sie können in Chrome Ihren Browserverlauf einsehen oder löschen und ähnliche Suchanfragen finden. Sie … Delete your activity automatically You can automatically delete some of the activity in your Google Account. On your computer, go to your Google Account. At the left, click Data & privacy. … Under \"History settings,\" click My Activity. To access your activity: Browse your activity, organized by day and time. To find specific activity, at the top, use the search bar and filters. Manage … Delete browsing data in Chrome You can delete your Chrome browsing history and other browsing data, like saved form entries, or just delete data from a specific date. \n",
            "\n",
            "{'snippet': 'May 22, 2025 · Beginning as a minor electorate in 1648 with scattered territories stretching from the Rhine to the Baltic, Prussia would transform itself into a formidable military state by 1815.', 'title': 'The Rise of Prussia: How a Small State Became a Military …', 'link': 'https://ancientwarhistory.com/the-rise-of-prussia-how-a-small-state-became-a-military-powerhouse-1648-1815/'}\n",
            "{'snippet': 'May 8, 2025 · This journey—from Baltic pagans to “kings in Prussia”—shows how a single state repeatedly reinvented itself and ultimately shifted the balance of power across the continent.', 'title': 'Prussia: The History That Changed Europe - The Coveners League', 'link': 'http://www.covenersleague.com/news-archives/item/643-prussia-the-history-that-changed-europe'}\n",
            "{'snippet': 'Feb 2, 2025 · This historical narrative highlights not only the challenges faced but also the profound impact of local cultures on Prussian identity, ultimately demonstrating how these interactions …', 'title': 'The Birth of Prussia: From the Teutonic Order to a European Power', 'link': 'https://www.berlin-rickshaw.com/post/the-birth-of-prussia-from-the-teutonic-order-to-a-european-power'}\n",
            "{'snippet': \"2 days ago · In this video I will be covering the History of Kingdom of Prussia of its rise and fall of it's power, the reforms and changes, it's culture, wars and milita...\", 'title': 'Airon The History Buff - Kingdom of Prussia - YouTube', 'link': 'https://www.youtube.com/watch?v=utJCdqcCTFk'}\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
        "\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "ddg_search_res = DuckDuckGoSearchResults(output_format=\"list\")\n",
        "\n",
        "res_s = ddg_search.invoke(\"History of Prussia\")\n",
        "res_r = ddg_search_res.invoke(\"History of Prussia\")\n",
        "\n",
        "print(res_s,\"\\n\")\n",
        "for res in res_r:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Prussia - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Prussia', 'body': '3 days ago · Prussia formed the German Empire when it united the German states in 1871. It was de facto dissolved by an emergency decree transferring powers of the Prussian government to …'}\n",
            "{'title': 'Prussia | History, Maps, Flag, & Definition | Britannica', 'href': 'https://www.britannica.com/place/Prussia', 'body': 'Jul 2, 2025 · Prussia, in European history, any of three historical areas of eastern and central Europe. It is most often associated with the kingdom ruled by the German Hohenzollern dynasty, …'}\n",
            "{'title': 'The Rise of Prussia: How a Small State Became a Military …', 'href': 'https://ancientwarhistory.com/the-rise-of-prussia-how-a-small-state-became-a-military-powerhouse-1648-1815/', 'body': 'May 22, 2025 · Beginning as a minor electorate in 1648 with scattered territories stretching from the Rhine to the Baltic, Prussia would transform itself into a formidable military state by 1815.'}\n",
            "{'title': 'Prussia: The History That Changed Europe - The Coveners League', 'href': 'http://www.covenersleague.com/news-archives/item/643-prussia-the-history-that-changed-europe', 'body': 'May 8, 2025 · This journey—from Baltic pagans to “kings in Prussia”—shows how a single state repeatedly reinvented itself and ultimately shifted the balance of power across the continent.'}\n",
            "{'title': 'The Birth of Prussia: From the Teutonic Order to a European Power', 'href': 'https://www.berlin-rickshaw.com/post/the-birth-of-prussia-from-the-teutonic-order-to-a-european-power', 'body': 'Feb 2, 2025 · This historical narrative highlights not only the challenges faced but also the profound impact of local cultures on Prussian identity, ultimately demonstrating how these interactions …'}\n",
            "{'title': 'What Was Prussia? - Mythbusting Berlin - Berlin Experiences', 'href': 'https://berlinexperiences.com/what-was-prussia-mythbusting-berlin/', 'body': 'Mar 19, 2025 · But how did this kingdom, with its roots in Baltic territories, achieve such prominence, and why does its complex history continue to evoke admiration, debate, and …'}\n",
            "{'title': 'History of the expansion of the Kingdom of Prussia - iNEWS - 資 …', 'href': 'https://inf.news/en/history/f3736f733dbf506f22f572d0f62dcbe7.html', 'body': '2 days ago · From the beginning of its establishment to its subsequent expansion, the Kingdom of Prussia has left a glorious page in European history. The following eight pictures show the history …'}\n",
            "{'title': 'Prussia, Austria, And Germany: Historical Boundaries And Inclusions', 'href': 'https://shunculture.com/article/does-germany-include-prussia-and-austria', 'body': 'Jan 4, 2025 · Explore the shifting historical boundaries of Prussia, Austria, and Germany. Understand the complex dynamics of territorial changes and cultural inclusions that shaped …'}\n",
            "{'title': 'History of Europe - Prussia, Enlightenment, Unification | Britannica', 'href': 'https://www.britannica.com/topic/history-of-Europe/Prussia', 'body': 'Jul 1, 2025 · Much was achieved: the restoration of Prussia and the establishment of an industrial base, in particular the exploitation of the new Silesian resources. Legal rights and freedom of …'}\n",
            "{'title': 'Frederick the Great - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Frederick_the_Great', 'body': '4 days ago · Prussia greatly increased its territories and became a major military power in Europe under his rule. He became known as Frederick the Great (German: Friedrich der Große) and was …'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/f6/6mkw37fs20q07p9x11q7d7k80000gq/T/ipykernel_59675/3941285.py:4: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  search_ggg = DDGS()\n"
          ]
        }
      ],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "search_ggg = DDGS()\n",
        "\n",
        "res_dgg = search_ggg.text('History of Prussia')\n",
        "for res in res_dgg:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Prussia\n",
            "https://en.wikipedia.org/wiki/Abolition_of_Prussia\n",
            "https://en.wikipedia.org/wiki/Flag_of_Prussia\n",
            "https://en.wikipedia.org/wiki/Prussia_(disambiguation)\n",
            "https://en.wikipedia.org/wiki/Duchy_of_Prussia\n",
            "https://en.wikipedia.org/wiki/Kingdom_of_Prussia\n",
            "https://en.wikipedia.org/wiki/Prussia_(region)\n",
            "https://en.wikipedia.org/wiki/Royal_Prussia\n",
            "https://en.wikipedia.org/wiki/Category:History_of_Prussia\n",
            "https://simple.wikipedia.org/wiki/Prussia\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/googlesearch/__init__.py:321: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  anchors = soup.findAll('a')\n"
          ]
        }
      ],
      "source": [
        "# https://www.geeksforgeeks.org/python/performing-google-search-using-python-code/ \n",
        "from googlesearch import search\n",
        "\n",
        "res_g = search(\"History of Prussia, no wikipedia\", stop=10)\n",
        "for res in res_g:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/googlesearch/__init__.py:321: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  anchors = soup.findAll('a')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.instagram.com/?hl=en\n",
            "https://www.instagram.com/\n",
            "https://en.m.wikipedia.org/wiki/File:Instagram_logo_2016.svg\n",
            "https://en.wikipedia.org/wiki/Instagram\n",
            "https://apps.apple.com/us/app/instagram/id389801252\n",
            "https://www.facebook.com/instagram/\n",
            "https://nl.wikipedia.org/wiki/Instagram\n",
            "https://www.sesarju.eu/sites/default/files/webform/sids_2025_poster/_sid_/23akk.pdf\n",
            "https://apps.apple.com/nl/app/instagram/id389801252\n",
            "http://www.medianest.be/wat-instagram\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/googlesearch/__init__.py:321: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  anchors = soup.findAll('a')\n"
          ]
        }
      ],
      "source": [
        "res_g = search(\"Instagram\", stop=10)\n",
        "for res in res_g:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "The history of Prussia dates back to its formation in 1648 as a minor electorate with scattered territories stretching from the Rhine to the Baltic. It gradually transformed itself into a formidable military power, eventually forming the German Empire when it united the German states in 1871. Historically associated with the German Hohenzollern dynasty, Prussia played a significant role in shaping Europe's balance of power by reinventing its identity through local cultures.\n",
            "\n",
            "You can find more detailed information on Wikipedia and Britannica about various aspects related to the history of Prussia, including its rise as a military powerhouse.\n",
            "The history of Prussia dates back to its formation in 1648 as a minor electorate with scattered\n",
            "territories stretching from the Rhine to the Baltic. It gradually transformed itself into a\n",
            "formidable military power, eventually forming the German Empire when it united the German states in\n",
            "1871. Historically associated with the German Hohenzollern dynasty, Prussia played a significant\n",
            "role in shaping Europe's balance of power by reinventing its identity through local cultures.  You\n",
            "can find more detailed information on Wikipedia and Britannica about various aspects related to the\n",
            "history of Prussia, including its rise as a military powerhouse.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "from llama_index.core.agent.workflow import ReActAgent, FunctionAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.workflow import Context\n",
        "import llama_index.core\n",
        "\n",
        "llama_index.core.set_global_handler(\"simple\")\n",
        "\n",
        "with  warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    def get_relevant_webpages(query: str) -> list:\n",
        "        \"\"\"\n",
        "        Gets the relevant webpages urls for a query.\n",
        "\n",
        "        Args:\n",
        "            query (str): what to search online on www.\n",
        "\n",
        "        Returns:\n",
        "            list: list of url links (websites) related to the query \n",
        "        \"\"\"\n",
        "        search_ggg = DDGS()\n",
        "        results = search_ggg.text(query)\n",
        "        return results\n",
        "\n",
        "    web_search_tool = FunctionTool.from_defaults(\n",
        "        fn=get_relevant_webpages,\n",
        "        name=\"get_relevant_webpages\",\n",
        "        description=\"Useful for getting a list of relevant webpages (url links) for a particular query. \" \\\n",
        "        \"Together with the web links the list also include a short information about the answer for the query.\"\n",
        "    )\n",
        "\n",
        "    search_agent = ReActAgent(\n",
        "        name = \"Web Searcher\", \n",
        "        description = \"Search the web give links to the relevant pages it found\",\n",
        "        system_prompt = \"\",\n",
        "        tools=[web_search_tool],\n",
        "        verbose=True,\n",
        "        llm=chat_model,\n",
        "    )\n",
        "\n",
        "\n",
        "    ctx = Context(search_agent)\n",
        "    answer = await search_agent.run(\"History of Prussia, search online with the web search tool\", ctx=ctx)\n",
        "    print(answer)\n",
        "    pprint(answer.response.blocks[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2025/07/19 12:37:40 WARNING mlflow.tracing.processor.mlflow_v2: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/inspect.py:592: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/mlflow/llama_index/tracer.py:290: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  if params_str := metadata.json(exclude_unset=True):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running step init_run\n",
            "Step init_run produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n",
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced no event\n",
            "Running step call_tool\n",
            "Step call_tool produced event ToolCallResult\n",
            "Running step aggregate_tool_results\n",
            "Step aggregate_tool_results produced event AgentInput\n",
            "Running step setup_agent\n",
            "Step setup_agent produced event AgentSetup\n",
            "Running step run_agent_step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/site-packages/mlflow/llama_index/tracer.py:290: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  if params_str := metadata.json(exclude_unset=True):\n",
            "2025/07/19 12:38:01 WARNING mlflow.tracing.export.async_export_queue: Failed to log trace to MLflow backend. Error: When an mlflow-artifacts URI was supplied, the tracking URI must be a valid http or https URI, but it was currently set to file:///Users/maksim.rostov/pdev/freestyling/agents/hf-course/unit2.2_llama_index/mlruns. Perhaps you forgot to set the tracking URI to the running MLflow server. To set the tracking URI, use either of the following methods:\n",
            "1. Set the MLFLOW_TRACKING_URI environment variable to the desired tracking URI. `export MLFLOW_TRACKING_URI=http://localhost:5000`\n",
            "2. Set the tracking URI programmatically by calling `mlflow.set_tracking_uri`. `mlflow.set_tracking_uri('http://localhost:5000')`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step run_agent_step produced event AgentOutput\n",
            "Running step parse_agent_output\n",
            "Step parse_agent_output produced event StopEvent\n",
            "Prussia was a Germanic kingdom and state that existed from the 1200s to the 1900s. Its importance peaked when it united the German states to form the German Empire in 1871 before being de facto dissolved by an emergency decree transferring powers of the Prussian government to the German Chancery. The territory included East Prussia, Brandenburg, and Saxony (including much of present-day Poland). Before its dissolution, there were provinces such as East Prussia and the margraves of Brandenburg who became highly dependent on estates representing counts, lords, knights, and towns.\n",
            "\n",
            "Prussia's coat of arms depicted a black eagle on a white background. The main flag featured this same symbol. \n",
            "\n",
            "Key events include the granting of Burzenland in Transylvania to the Teutonic Knights by King Andrew II of Hungary in 1211 as a fiefdom for the German military order.\n",
            "\n",
            "In the mid-16th century, the margraves of Brandenburg had become highly dependent on the estates representing counts, lords, knights, and towns. \n",
            "\n",
            "The influence of Prussia is still felt across German history, even though its name has vanished from maps after being dissolved into modern Germany.\n",
            "\n",
            "Thought: I can answer without using any more tools.\n",
            "Answer: The history of Prussia involved several key events including its origin as a kingdom under the Hohenzollern family in the 13th century. It became an independent duchy that played a significant role in shaping German politics and culture until it was dissolved into modern Germany after World War I.\n",
            "Prussia was a Germanic kingdom and state that existed from the 1200s to the 1900s. Its importance\n",
            "peaked when it united the German states to form the German Empire in 1871 before being de facto\n",
            "dissolved by an emergency decree transferring powers of the Prussian government to the German\n",
            "Chancery. The territory included East Prussia, Brandenburg, and Saxony (including much of present-\n",
            "day Poland). Before its dissolution, there were provinces such as East Prussia and the margraves of\n",
            "Brandenburg who became highly dependent on estates representing counts, lords, knights, and towns.\n",
            "Prussia's coat of arms depicted a black eagle on a white background. The main flag featured this\n",
            "same symbol.   Key events include the granting of Burzenland in Transylvania to the Teutonic Knights\n",
            "by King Andrew II of Hungary in 1211 as a fiefdom for the German military order.  In the mid-16th\n",
            "century, the margraves of Brandenburg had become highly dependent on the estates representing\n",
            "counts, lords, knights, and towns.   The influence of Prussia is still felt across German history,\n",
            "even though its name has vanished from maps after being dissolved into modern Germany.  Thought: I\n",
            "can answer without using any more tools. Answer: The history of Prussia involved several key events\n",
            "including its origin as a kingdom under the Hohenzollern family in the 13th century. It became an\n",
            "independent duchy that played a significant role in shaping German politics and culture until it was\n",
            "dissolved into modern Germany after World War I.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent.workflow import ReActAgent, FunctionAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.workflow import Context\n",
        "\n",
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment(experiment_name=\"Test\")\n",
        "mlflow.set_tracking_uri('http://localhost:5000')\n",
        "mlflow.llama_index.autolog()\n",
        "\n",
        "\n",
        "def get_relevant_webpages(query: str) -> list:\n",
        "    \"\"\"\n",
        "    Gets the relevant webpages urls for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): what to search online on www.\n",
        "\n",
        "    Returns:\n",
        "        list: list of url links (websites) related to the query \n",
        "    \"\"\"\n",
        "    search_ggg = DDGS()\n",
        "    results = search_ggg.text(query)\n",
        "    return results\n",
        "\n",
        "web_search_tool = FunctionTool.from_defaults(\n",
        "    fn=get_relevant_webpages,\n",
        "    name=\"get_relevant_webpages\",\n",
        "    description=\"Useful for getting a list of relevant webpages (url links) for a particular query. \" \\\n",
        "    \"Together with the web links the list also include a short information about the answer for the query.\"\n",
        ")\n",
        "\n",
        "search_agent = ReActAgent(\n",
        "    name = \"Web Searcher\", \n",
        "    description = \"Search the web give links to the relevant pages it found\",\n",
        "    system_prompt = \"\",\n",
        "    tools=[web_search_tool],\n",
        "    verbose=True,\n",
        "    llm=chat_model,\n",
        ")\n",
        "\n",
        "\n",
        "ctx = Context(search_agent)\n",
        "answer = await search_agent.run(\"History of Prussia, search online with the web search tool\", ctx=ctx)\n",
        "print(answer)\n",
        "pprint(answer.response.blocks[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<llama_index.core.callbacks.simple_llm_handler.SimpleLLMHandler at 0x3d50849b0>"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama_index.core.global_handler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=59675) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
            "  pid, fd = os.forkpty()\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/maksim.rostov/pdev/freestyling/agents/hf-course/.conda/bin/python\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
