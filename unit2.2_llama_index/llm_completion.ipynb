{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100538ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to work with custom models in Llama Index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f6c3c",
   "metadata": {},
   "source": [
    "### LLMer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata\n",
    ")\n",
    "\n",
    "import hf_course.llmer.grpc.chats_pb2 as chats_pb2\n",
    "import hf_course.llmer.grpc.chats_pb2_grpc as chats_pb2_grpc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d452ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomLLM(CustomLLM):\n",
    "    def __init__(self):\n",
    "        llmer_address = \"localhost:50051\"\n",
    "        channel = grpc.insecure_channel(llmer_address)  \n",
    "        client = chats_pb2_grpc.LLMChatServiceStub(channel=channel)\n",
    "        session_request = chats_pb2.StartSessionRequest(\n",
    "                mode=\"QuestionAnsweringChatBot\",\n",
    "                user=\"user123\",\n",
    "        )\n",
    "        session_id = client.StartSession(session_request)\n",
    "        session_id = session_id\n",
    "                    \n",
    "    \"\"\"Needs to implement these three\"\"\"\n",
    "    # @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            context_window=4096,  # where does this field matter\n",
    "            num_output=256,       # where does this field matter\n",
    "            model_name=\"my-test-model\",\n",
    "        )\n",
    "    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
    "        # FIXME: the base class does not allow me to define fields for some reason, find a way \n",
    "        llmer_address = \"localhost:50051\"\n",
    "        channel = grpc.insecure_channel(llmer_address)  \n",
    "        client = chats_pb2_grpc.LLMChatServiceStub(channel=channel)\n",
    "        session_request = chats_pb2.StartSessionRequest(\n",
    "                mode=\"QuestionAnsweringChatBot\",\n",
    "                user=\"user123\",\n",
    "        )\n",
    "        session_id_response = client.StartSession(session_request)\n",
    "        session_id = session_id_response.id\n",
    "        print(session_id)\n",
    "        message_question = chats_pb2.Question(\n",
    "            session_id=session_id,\n",
    "            system_prompt=\"somethings\",\n",
    "            question_prompt=prompt\n",
    "        )\n",
    "        answer = client.SendMessage(message_question)\n",
    "\n",
    "        return CompletionResponse(text=answer.text)\n",
    "\n",
    "    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
    "        # TODO: lets see if i can get away without streaming\n",
    "        # FIXME: the base class does not allow me to define fields for some reason, find a way \n",
    "        llmer_address = \"localhost:50051\"\n",
    "        channel = grpc.insecure_channel(llmer_address)  \n",
    "        client = chats_pb2_grpc.LLMChatServiceStub(channel=channel)\n",
    "        session_request = chats_pb2.StartSessionRequest(\n",
    "                mode=\"QuestionAnsweringChatBot\",\n",
    "                user=\"user123\",\n",
    "        )\n",
    "        session_id_response = client.StartSession(session_request)\n",
    "        session_id = session_id_response.id\n",
    "        print(session_id)\n",
    "        message_question = chats_pb2.Question(\n",
    "            session_id=session_id,\n",
    "            system_prompt=\"\",\n",
    "            question_prompt=prompt\n",
    "        )\n",
    "        answer = client.SendMessage(message_question)\n",
    "\n",
    "        return CompletionResponse(text=answer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843d7d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyCustomLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm_example = \u001b[43mMyCustomLLM\u001b[49m()\n\u001b[32m      3\u001b[39m resp = llm_example.complete(\u001b[33m\"\u001b[39m\u001b[33mI am not that tall, how can i seeem to me be more tall\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m resp\n",
      "\u001b[31mNameError\u001b[39m: name 'MyCustomLLM' is not defined"
     ]
    }
   ],
   "source": [
    "llm_example = MyCustomLLM()\n",
    "\n",
    "resp = llm_example.complete(\"I am not that tall, how can i seeem to me be more tall\")\n",
    "resp\n",
    "print(resp.raw[\"messages\"]) # nothing in raw "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a333fcc",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a97641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, als een AI kan ik geen geheugen hebben van vorige interacties buiten de huidige sessie. Kan ik u helpen met iets specifisch of heeft u bepaalde informatie vergeten die ik nodig zou hebben?\n",
      "Sorry, als een AI kan ik geen geheugen hebben van vorige interacties buiten de huidige sessie. Kan ik u helpen met iets specifisch of heeft u bepaalde informatie vergeten die ik nodig zou hebben?\n",
      "Sorry, als een AI kan ik geen geheugen hebben van vorige interacties buiten de huidige sessie. Kan ik u helpen met iets specifisch of heeft u bepaalde informatie vergeten die ik nodig zou hebben?\n"
     ]
    }
   ],
   "source": [
    "model = Ollama(model=\"qwen2:7b\")\n",
    "model.complete(\"Yo maatje, hoe kan ik beter leven?\")\n",
    "resp = model.complete(\"wat had ik je net gevraagd?\")\n",
    "print(resp.text)\n",
    "print(resp.raw[\"message\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Aanbeveling\": \"\\n1. Regelmatige Fysieke Oefening: Probeer elke dag minstens 30 minute aan fysieke activiteit door te voeren. Dit kan een wandeltocht, fietsrit of zelfs gewichten zijn.\\n2. Gesonde Voeding: Eten van een gevarieerd en evenwichtig eten met veel groente en fruit is cruciaal voor je gezondheid. Vervang zo veel mogelijk ongezonde zoutjes en suikers door voedingswaarden die je lijf nodig heeft.\\n3. Overvulling van het Geestelijke: Leer meditereren of yoga om je geest te kalmeren. Lezen, schrijven en andere creatieve activiteiten kunnen ook helpen om je gedachten op een rustiger koers te krijgen.\\n4. Genoeg Slaap: Probeer elke nacht ongeveer 7 tot 9 uur te slapen. Je lichaam en geest zijn belangrijker dan de meeste van ons realiseren, dus zorg ervoor dat ze in top toestand zijn.\\n5. Gesonder Relaties: Wees oprecht naar jezelf en andere mensen. Probeer om positieve en ondersteunende relaties te bouwen. Als jij beter bent voor jezelf en anderen, voel je je waarschijnlijk ook beter.\\n6. Stressbeheersing: Zorg ervoor dat je een strategie hebt om stress aan te pakken. Dit kan het lezen van een boek zijn, het maken van kunst of zelfs het oplossen van een raadsel. Het belangrijkste is dat je vindt wat werkt voor jou.\\n7. Positieve Attitude: Probeer altijd positief te blijven. Hoewel dit niet betekent dat je elke moeilijkheid onmiddellijk kunt negeren, het betekent wel dat je de toekomst op een optimistische manier kunt benaderen.\"}\n"
     ]
    }
   ],
   "source": [
    "# Try to structure your outputs a bit more, seems to be working poorly with qwen2\n",
    "model_json = Ollama(model=\"qwen2:7b\", json_mode=True)\n",
    "resp_json = model_json.complete(\"Yo maatje, hoe kan ik beter leven?\") \n",
    "print(str(resp_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "999c76c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"World War II\",\"place\":\"Global, spanning Europe, Africa, Asia and the Pacific regions\",\"time_period\":\"1939-1945\",\"country\":\"Involved countries include Germany, Japan, Italy, Britain, France, Russia, USA among others\"}\n",
      "{\"name\":\"The American Revolutionary War\",\"place\":\"Colonies of British America and the British Empire\",\"time_period\":\"1775-1783\",\"country\":\"United States of America\"}\n"
     ]
    }
   ],
   "source": [
    "# Stucture even more\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "class HistoricFact(BaseModel):\n",
    "    \"\"\"A historic event.\"\"\"\n",
    "    name: str\n",
    "    place: str\n",
    "    time_period: str\n",
    "    country: str\n",
    "\n",
    "model_struct = Ollama(\n",
    "    model=\"qwen2:7b\",\n",
    ")\n",
    "model_struct_call = model_struct.as_structured_llm(HistoricFact)\n",
    "\n",
    "response = model_struct_call.chat([ChatMessage(role=\"user\", content=\"Name a famous historic event\")])\n",
    "print(response.message.content)\n",
    "\n",
    "response = model_struct_call.complete(\"Name a famous historic event\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb56b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het  kw amen  de  Pool se ,  D uit se  en  O ost -E u rop ese  culture n  sam env allen  in  Pr us se ,  wat  veel  vari atie  bied t  in  term en  van  vo eds el .  Hier  zijn  en ke le  klass ie ke  gere chten  die  in  Pr us se  worden  ge get en :\n",
      "\n",
      " 1 .  Big os :  Dit  is  een  tradition eel  Pool se  gro ent eg ro ente b ou ill iet  wa aron der  k arn ar be ids ,  pap rika 's ,  kn ol r atten ,  o lij ven ,  ger uit en  var k ens v le es  en  w ort els  worden  ing ede kt .\n",
      "\n",
      " 2 .  Pier ogi :  Deze  kleine  v le es -  of  gro ent ep ak ket jes  zijn  een  gew a arde erd  gere cht  in  veel  land en  van  O ost -E u ropa ,  wa aron der  Pol en .\n",
      "\n",
      " 3 .  K omp ot :  Dit  is  een  tradition eel  Pool se  drank je  g emaakt  van  ged ik ke  fruit bro od jes  zoals  appel ,  appel  en /of  ab ri ko zen .\n",
      "\n",
      " 4 .  D ö ner k ab ob :  Dit  is  een  Turk se  ke uze  die  in  veel  land en  van  Europa  te  vinden  is ,  wa aron der  Pr us se .\n",
      "\n",
      " 5 .  K arp aska  Dad lic ia :  Een  spec ia al  gere cht  van  bro od ge bro d  dat  gev uld  wordt  met  gro ent en  en  worst jes .\n",
      "\n",
      " 6 .  Pier ż on ka  ( P ols chen ):  Dit  is  een  tradition eel  P ools  best a ans recht  voor  de  fe est d agen ,  ge vo erd  met  e ieren ,  ui 's ,  pap rika 's ,  kn ol r atten  en  v le es .\n",
      "\n",
      " 7 .  K ules h :  Een  so ort  gro ent eb ro od g at je  dat  in  veel  land en  van  O ost -E u ropa  wordt  ge get en ,  vo oral  in  de  winter .\n",
      "    \n",
      " 8 .  P og ac za :  Deze  tradition ele  D uit se  bro od re us  is  va ak  op ger uild  voor  cro iss ants  of  dan ar ren  tijd ens  het  ont bij t .  \n",
      "\n",
      " 9 .  Ż ure k :  Een  typ isch  Pool se  gro ent ek omp ot  met  worst ,  gek ook te  e ieren  en  een  so ort  bro od geb ak .\n",
      "\n",
      " 1 0 .  S aus  met  v le es :  Dit  is  een  tradition eel  gere cht  van  ges mo orde  worst jes  in  een  sa us  g emaakt  van  ger uit en  v le es ,  ui 's  en  bon en .\n",
      "\n",
      " H oud  er  re kening  mee  dat  deze  gere chten  vari ë ren  af h ank elijk  van  de  reg io  binnen  Pr us se .  Het  is  belang rijk  om  te  wet en  dat  de  Pr uis ische  culture n  veel  zijn  ge ï mp or rete erd  uit  andere  land en ,  wa ard oor  de  vo eds el gew o ont en  ook  vers cheiden heid  hebben  gek reg en .  "
     ]
    }
   ],
   "source": [
    "response = model.stream_complete(\"wat eet je over Prussia\")\n",
    "for r in response:\n",
    "    print(r.delta, end=\" \")\n",
    "model.complete(\"wat eet je over Prussia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efd2914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'llama_index.core.base.llms.types.ChatMessage'>\n",
      "Ahoy there, matey! I be Captain彩霞掠海 (Cai Xia Luehai), the most dashing and daring captain in all the seas. Call me Captain Sunsetsails or just plain old Cai for short. What brings you to my deck today?\n",
      "<class 'ollama._types.Message'>\n",
      "Ahoy there, matey! I be Captain彩霞掠海 (Cai Xia Luehai), the most dashing and daring captain in all the seas. Call me Captain Sunsetsails or just plain old Cai for short. What brings you to my deck today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = model.chat(messages)\n",
    "\n",
    "print(type(resp.message))\n",
    "print(resp.message.blocks[0].text)\n",
    "\n",
    "print(type(resp.raw[\"message\"]))\n",
    "print(resp.raw[\"message\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a88ce",
   "metadata": {},
   "source": [
    "## Ollama Vision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5473a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image shows a sign with the text \"Ollam Vision\". The background of the sign is white, and the text is in black. The sign includes a URL or website address at the bottom. Below the main sign, there's another smaller banner with the text \"Welcome to the era of open-source multimodal models.\" The image is taken from an angle where the top of the sign is visible but not fully captured in the frame. The overall style of the image is promotional or informative, suggesting a connection to the tech industry or software development. \n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(\n",
    "\tmodel=\"llava\",\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': 'Describe this image:',\n",
    "\t\t\t'images': ['/Users/maksim.rostov/Desktop/test5.png']\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "401c52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant:  The image shows a sign or banner that says \"Ollam Vision,\" which appears to be an invitation or advertisement for some kind of event, gathering, or organization related to open-source multimodal models. It is welcoming visitors and is located in an area called \"The Era of Open Source.\" \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llava\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            TextBlock(text=\"What does the image say\"),\n",
    "            ImageBlock(path=\"/Users/maksim.rostov/Desktop/test5.png\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
