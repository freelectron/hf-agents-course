{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama  \n",
        "\n",
        "chat_model = Ollama(model=\"qwen2:7b\", context_window=80000, request_timeout=300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'Hello! How can I assist you today?',\n",
              " 'additional_kwargs': {'tool_calls': None, 'thinking': None},\n",
              " 'raw': {'model': 'qwen2:7b',\n",
              "  'created_at': '2025-08-04T21:51:42.680495Z',\n",
              "  'done': True,\n",
              "  'done_reason': 'stop',\n",
              "  'total_duration': 981048958,\n",
              "  'load_duration': 29762875,\n",
              "  'prompt_eval_count': 20,\n",
              "  'prompt_eval_duration': 730119875,\n",
              "  'eval_count': 10,\n",
              "  'eval_duration': 220454708,\n",
              "  'message': Message(role='assistant', content='Hello! How can I assist you today?', thinking=None, images=None, tool_calls=None),\n",
              "  'usage': {'prompt_tokens': 20, 'completion_tokens': 10, 'total_tokens': 30}},\n",
              " 'logprobs': None,\n",
              " 'delta': None}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = chat_model.complete(\"Hey\")\n",
        "res.raw[\"message\"][\"content\"]\n",
        "res.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnsDEuS1wHh-"
      },
      "source": [
        "## Researcher: An Agent Workflow\n",
        "\n",
        "Let's create an agent workflow that would: \n",
        "defintions  \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_with_beautifulsoup(raw_html: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract clean text from raw HTML using BeautifulSoup.\n",
        "    \n",
        "    Args:\n",
        "        raw_html (str): Raw HTML content\n",
        "        \n",
        "    Returns:\n",
        "        str: Extracted clean text\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
        "    \n",
        "    # Remove script and style elements\n",
        "    for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
        "        script.decompose()\n",
        "    \n",
        "    # Get text and clean it up\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    # Clean up whitespace\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "    \n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Optional, Annotated\n",
        "from datetime import datetime\n",
        "import urllib\n",
        "\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# import mlflow\n",
        "# mlflow.set_experiment(experiment_name=datetime.now().isoformat())\n",
        "# mlflow.set_tracking_uri('http://localhost:5000')\n",
        "# mlflow.llama_index.autolog()\n",
        "\n",
        "MAX_NUMBER_URL_SOURCES = 1 \n",
        "\n",
        "class SearchState(TypedDict):\n",
        "    # Query to research\n",
        "    query: str \n",
        "\n",
        "    # Keep the interactions for ReAct (Reason - Act - Observe - Repeat) style agent \n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "    # Section of the blogpost\n",
        "    sections: list[str]\n",
        "\n",
        "    # Current section that is being written \n",
        "    section: str\n",
        "\n",
        "    # List of relevant urls webpages (resources/references) to the query \n",
        "    web_urls: Optional[list[str]] \n",
        "\n",
        "    # Current url that is being dowloaded processed \n",
        "    url: Optional[str] \n",
        "\n",
        "    # Retrieved text from from the respective urls websources \n",
        "    text_sources: Optional[list[str]]  \n",
        "\n",
        "    # Final output blogpost\n",
        "    post: str\n",
        "\n",
        "def create_plan(state: SearchState):\n",
        "    \"\"\"\n",
        "    Create an exectution plan for an agent. Must define what subsections a blogpost will have. \n",
        "    \"\"\"\n",
        "    query = state[\"query\"]\n",
        "    msgs = state[\"messages\"]\n",
        "    prompt = f\"Based on the previous conversation: {msgs}; Create a list of subsection that a blogpost on this {query} topic must have. Just output a list of subsections so that it can be parsed in python format, [section_name_1, section_name_2,.. etc], do not out put new line charcters, nothing else, just plain list.\"\n",
        "    res = chat_model.complete(prompt)\n",
        "\n",
        "    state[\"messages\"] += [AIMessage(content=res.raw[\"message\"][\"content\"])]\n",
        "    state[\"sections\"] = res.raw[\"message\"][\"content\"].strip()[1:-1].split(\",\")\n",
        "\n",
        "    print(\"create_plan: \", state[\"sections\"])\n",
        "\n",
        "    return state\n",
        "\n",
        "def check_plan(state: SearchState):\n",
        "    human_feedback = input(\"Please provide your input, if any: \")\n",
        "    if len(human_feedback) > 0: \n",
        "        state[\"messages\"] += [HumanMessage(content=human_feedback)]\n",
        "        return \"iterate\"\n",
        "    else:\n",
        "        return \"proceed\"\n",
        "\n",
        "def start_section(state: SearchState):\n",
        "    print(\"Starting to process a section..\")\n",
        "    return state \n",
        "\n",
        "def check_start_section(state: SearchState):\n",
        "    section = state[\"sections\"][0]\n",
        "    if len(state[\"sections\"])>0: \n",
        "        state[\"sections\"] = state[\"sections\"][1:]\n",
        "        state[\"section\"] = f\"You are writting a section about {section} that is a part of the blogpost on {state['query']}\" \n",
        "        return \"proceed\"\n",
        "    else:\n",
        "        print(\"No sectons to process..\")\n",
        "        return \"stop\"\n",
        "\n",
        "def get_relevant_webpages(state: SearchState):\n",
        "    \"\"\"\n",
        "    Gets the url of the most relevant webpages for the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): what to search online on www.\n",
        "\n",
        "    Returns:\n",
        "        str: url link.\n",
        "    \"\"\"\n",
        "    from ddgs import DDGS\n",
        "\n",
        "    search_ggg = DDGS()\n",
        "    results = search_ggg.text(query=state[\"section\"], max_results=MAX_NUMBER_URL_SOURCES )\n",
        "    state[\"web_urls\"] += [res[\"href\"] for res in results]\n",
        "\n",
        "    print(\"get_relevant_webpages: \", state[\"web_urls\"])\n",
        "    \n",
        "    return state\n",
        "\n",
        "def download_webpages(state: SearchState) -> str:\n",
        "    \"\"\"\n",
        "    Load the raw webpage of the url. Store it in the context.\n",
        "    \n",
        "    Args:\n",
        "        url (str): www url of the page.\n",
        "    \n",
        "    Returns: \n",
        "        str: html string of the text on the webpage.\n",
        "    \"\"\"\n",
        "\n",
        "    for url in state[\"web_urls\"][-MAX_NUMBER_URL_SOURCES:]:\n",
        "        try: \n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                html_text = response.read()\n",
        "                state[\"text_sources\"].append(extract_text_with_beautifulsoup(html_text))\n",
        "        except urllib.error.URLError as e:\n",
        "            print(\"Error getting the page: \", e)\n",
        "        except Exception as e:\n",
        "            print(\"Something happened: \", e)\n",
        "    \n",
        "    return state\n",
        "\n",
        "def generate_blogpost_section(state:SearchState) -> str:\n",
        "    \"\"\"\n",
        "    Generate a blogpost section in the markdown format based on the raw of a resource. \n",
        "    \"\"\"\n",
        "    # TODO: how to use all the resources? \n",
        "    sources = state[\"text_sources\"][-MAX_NUMBER_URL_SOURCES:]\n",
        "    task = f\"Based on the this information {sources}, generate a extensive blogpost section about the topic {state['section']} in the markdown format.\"\n",
        "    state[\"post\"] += chat_model.complete(task)\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Create the graph\n",
        "researcher_graph = StateGraph(SearchState)\n",
        "\n",
        "# Add nodes\n",
        "researcher_graph.add_node(\"create_plan\", create_plan)\n",
        "researcher_graph.add_node(\"start_section\", start_section)\n",
        "researcher_graph.add_node(\"get_relevant_webpages\", get_relevant_webpages)\n",
        "researcher_graph.add_node(\"download_webpages\", download_webpages)\n",
        "researcher_graph.add_node(\"generate_blogpost_section\", generate_blogpost_section)\n",
        "\n",
        "researcher_graph.add_edge(START, \"create_plan\")\n",
        "researcher_graph.add_conditional_edges(\n",
        "    \"create_plan\",\n",
        "    check_plan,\n",
        "    {\n",
        "        \"proceed\": \"start_section\",\n",
        "        \"iterate\": \"create_plan\",\n",
        "    }\n",
        ")\n",
        "researcher_graph.add_conditional_edges(\n",
        "    \"start_section\",\n",
        "    check_start_section,\n",
        "    {\n",
        "        \"proceed\": \"get_relevant_webpages\",\n",
        "        \"stop\": END,\n",
        "    }\n",
        ")\n",
        "researcher_graph.add_edge(\"get_relevant_webpages\", \"download_webpages\")\n",
        "researcher_graph.add_edge(\"download_webpages\", \"generate_blogpost_section\")\n",
        "researcher_graph.add_edge(\"generate_blogpost_section\", END)\n",
        "dag = researcher_graph.compile()\n",
        "display(Image(dag.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_state = SearchState(query=\"History of Prussia\", text_sources=list(), web_urls=list(), post=\"\")\n",
        "res = dag.invoke(init_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pprint\n",
        "# # Set width to control line length (e.g., 80 characters)\n",
        "# pp = pprint.PrettyPrinter(width=80, depth=4)\n",
        "# pp.pprint(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res[\"post\"].text)\n",
        "with open(f\"post_v2_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.md\", \"w\") as f:\n",
        "    f.write(res[\"post\"].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
