{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama  \n",
        "\n",
        "chat_model = Ollama(model=\"qwen2:7b\", context_window=80000, request_timeout=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnsDEuS1wHh-"
      },
      "source": [
        "## Researcher: An Agent Workflow\n",
        "\n",
        "Let's create an agent workflow that would: \n",
        "defintions  \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_with_beautifulsoup(raw_html: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract clean text from raw HTML using BeautifulSoup.\n",
        "    \n",
        "    Args:\n",
        "        raw_html (str): Raw HTML content\n",
        "        \n",
        "    Returns:\n",
        "        str: Extracted clean text\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
        "    \n",
        "    # Remove script and style elements\n",
        "    for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
        "        script.decompose()\n",
        "    \n",
        "    # Get text and clean it up\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    # Clean up whitespace\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "    \n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import urllib\n",
        "\n",
        "# import mlflow\n",
        "# mlflow.set_experiment(experiment_name=datetime.now().isoformat())\n",
        "# mlflow.set_tracking_uri('http://localhost:5000')\n",
        "# mlflow.llama_index.autolog()\n",
        "\n",
        "\n",
        "class SearchState(TypedDict):\n",
        "    # Query to research\n",
        "    query: str \n",
        "\n",
        "    # List of relevant urls webpages (resources/references) to the query \n",
        "    web_urls: Optional[list[str]] \n",
        "\n",
        "    # Current url that is being dowloaded processed \n",
        "    url: Optional[str] \n",
        "\n",
        "    # Retrieved text from from respective urls websources \n",
        "    text_sources: Optional[list[str]]  \n",
        "\n",
        "    # Final output blogpost\n",
        "    post: str\n",
        "\n",
        "\n",
        "def get_relevant_webpages(state: SearchState):\n",
        "    \"\"\"\n",
        "    Gets the url of the most relevant webpages for the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): what to search online on www.\n",
        "\n",
        "    Returns:\n",
        "        str: url link.\n",
        "    \"\"\"\n",
        "    from ddgs import DDGS\n",
        "\n",
        "    search_ggg = DDGS()\n",
        "    results = search_ggg.text(query=state[\"query\"], max_results=2)\n",
        "    state[\"web_urls\"] = [res[\"href\"] for res in results]\n",
        "    \n",
        "    return state\n",
        "\n",
        "def check_urls(state: SearchState):\n",
        "    \"\"\"There could be complex comparison logic here, but we just implement this in `route_move_to_download`.\"\"\"\n",
        "    return state\n",
        "    \n",
        "def route_move_to_download(state: SearchState): \n",
        "    if len(state[\"web_urls\"])>0:\n",
        "        return \"proceed\"\n",
        "    else:\n",
        "        return \"stop\"\n",
        "\n",
        "def download_webpages(state: SearchState) -> str:\n",
        "    \"\"\"\n",
        "    Load the raw webpage of the url. Store it in the context.\n",
        "    \n",
        "    Args:\n",
        "        url (str): www url of the page.\n",
        "    \n",
        "    Returns: \n",
        "        str: html string of the text on the webpage.\n",
        "    \"\"\"\n",
        "\n",
        "    for url in state[\"web_urls\"]:\n",
        "        try: \n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                html_text = response.read()\n",
        "                state[\"text_sources\"].append(extract_text_with_beautifulsoup(html_text))\n",
        "        except urllib.error.URLError as e:\n",
        "            print(\"Error getting the page: \", e)\n",
        "        except Exception as e:\n",
        "            print(\"Something happened: \", e)\n",
        "    \n",
        "    return state\n",
        "\n",
        "def generate_blogpost(state:SearchState) -> str:\n",
        "    \"\"\"\n",
        "    Generate a blogpost in a markdown format based on the raw of a resource. \n",
        "    \"\"\"\n",
        "    # TODO: how to use all the resources? \n",
        "    sources = state[\"text_sources\"][0]\n",
        "    task = f\"Based on the this information {sources}, generate a blogpost about the topic {state['query']} in the markdown format.\"\n",
        "    state[\"post\"] = chat_model.complete(task)\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Create the graph\n",
        "researcher_graph = StateGraph(SearchState)\n",
        "\n",
        "# Add nodes\n",
        "researcher_graph.add_node(\"get_relevant_webpages\", get_relevant_webpages)\n",
        "researcher_graph.add_node(\"check_urls\", check_urls)\n",
        "researcher_graph.add_node(\"download_webpages\", download_webpages)\n",
        "researcher_graph.add_node(\"generate_blogpost\", generate_blogpost)\n",
        "\n",
        "researcher_graph.add_edge(START, \"get_relevant_webpages\")\n",
        "researcher_graph.add_edge(\"get_relevant_webpages\", \"check_urls\")\n",
        "researcher_graph.add_conditional_edges(\n",
        "    \"check_urls\",\n",
        "    route_move_to_download,\n",
        "    {\n",
        "        \"proceed\": \"download_webpages\",\n",
        "        \"stop\": END\n",
        "    }\n",
        ")\n",
        "researcher_graph.add_edge(\"download_webpages\", \"generate_blogpost\")\n",
        "researcher_graph.add_edge(\"generate_blogpost\", END)\n",
        "\n",
        "dag = researcher_graph.compile()\n",
        "display(Image(dag.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_state = SearchState(query=\"History of Prussia\", text_sources=list())\n",
        "res = dag.invoke(init_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pprint\n",
        "# # Set width to control line length (e.g., 80 characters)\n",
        "# pp = pprint.PrettyPrinter(width=80, depth=4)\n",
        "# pp.pprint(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res[\"post\"].text)\n",
        "with open(f\"post_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.md\", \"w\") as f:\n",
        "    f.write(res[\"post\"].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
